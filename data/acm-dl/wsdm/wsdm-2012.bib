@inproceedings{10.1145/2124295.2124297,
author = {Varian, Hal R.},
title = {Nowcasting the Macroeconomy with Search Engine Data},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124297},
doi = {10.1145/2124295.2124297},
abstract = {It is now possible to acquire real time information on economic variables of interest from various commercial sources. I illustrate how one can use Google Trends data to measure the state of the macroeconomy in various sectors, and discuss some of the ramifications for research and policy.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {1–2},
numpages = {2},
keywords = {forecasting},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124299,
author = {van Zwol, Roelof and Garcia Pueyo, Lluis},
title = {Spatially-Aware Indexing for Image Object Retrieval},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124299},
doi = {10.1145/2124295.2124299},
abstract = {The success of image object retrieval systems relies on the visual bag-of-words paradigm, which allows image retrieval systems to adopt a retrieval strategy analogous to text retrieval. In this paper we propose two spatially-aware retrieval strategies for image object retrieval that replaces the vector space model. The advantage of the proposed spatially-aware indexing and retrieval strategies are threefold: (1) It allows for the deployment of small visual vocabularies, (2) the number of images evaluated at retrieval time is significantly reduced, and (3) it eliminates the need for a post-retrieval phase, which is normally used to test the spatial composition of the visual words in the retrieved images.The first spatially-aware retrieval strategy explores the direct neighbourhood of two local features for common visual words to determine the similarity of the region surrounding the local features. The second strategy embeds the spatial composition of its neighbourhood directly in the index using edge signatures. Both strategies rely on the coherence of the neighbourhood of points in different images containing similar objects. The comparison of the spatially-aware retrieval strategies against the vector space baseline shows a significant improvement in terms of early precision, and at the same time significantly reduce the number of candidates to be considered at retrieval time.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {3–12},
numpages = {10},
keywords = {spatially-aware indexing, quantization error, image retrieval},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124300,
author = {Zhang, Yuan Cao and S\'{e}aghdha, Diarmuid \'{O} and Quercia, Daniele and Jambor, Tamas},
title = {Auralist: Introducing Serendipity into Music Recommendation},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124300},
doi = {10.1145/2124295.2124300},
abstract = {Recommendation systems exist to help users discover content in a large body of items. An ideal recommendation system should mimic the actions of a trusted friend or expert, producing a personalised collection of recommendations that balance between the desired goals of accuracy, diversity, novelty and serendipity. We introduce the Auralist recommendation framework, a system that - in contrast to previous work - attempts to balance and improve all four factors simultaneously. Using a collection of novel algorithms inspired by principles of "serendipitous discovery", we demonstrate a method of successfully injecting serendipity, novelty and diversity into recommendations whilst limiting the impact on accuracy. We evaluate Auralist quantitatively over a broad set of metrics and, with a user study on music recommendation, show that Auralist's emphasis on serendipity indeed improves user satisfaction.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {13–22},
numpages = {10},
keywords = {recommender systems, novelty, collaborative filtering, accuracy, metrics, diversification, serendipity},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124301,
author = {Cheng, Hanqiang and Liang, Yu-Li and Xing, Xinyu and Liu, Xue and Han, Richard and Lv, Qin and Mishra, Shivakant},
title = {Efficient Misbehaving User Detection in Online Video Chat Services},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124301},
doi = {10.1145/2124295.2124301},
abstract = {Online video chat services, such as Chatroulette, Omegle, and vChatter are becoming increasingly popular and have attracted millions of users. One critical problem encountered in such applications is the presence of misbehaving users ("flashers") and obscene content. Automatically filtering out obscene content from these systems in an efficient manner poses a difficult challenge. This paper presents a novel Fine-Grained Cascaded (FGC) classification solution that significantly speeds up the compute-intensive process of classifying misbehaving users by dividing image feature extraction into multiple stages and filtering out easily classified images in earlier stages, thus saving unnecessary computation costs of feature extraction in later stages. Our work is further enhanced by integrating new webcam-related contextual information (illumination and color) into the classification process, and a 2-stage soft margin SVM algorithm for combining multiple features. Evaluation results using real-world data set obtained from Chatroulette show that the proposed FGC based classification solution significantly outperforms state-of-the-art techniques.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {23–32},
numpages = {10},
keywords = {online video chat, misbehaving user},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124302,
author = {Zhang, Haipeng and Korayem, Mohammed and You, Erkang and Crandall, David J.},
title = {Beyond Co-Occurrence: Discovering and Visualizing Tag Relationships from Geo-Spatial and Temporal Similarities},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124302},
doi = {10.1145/2124295.2124302},
abstract = {Studying relationships between keyword tags on social sharing websites has become a popular topic of research, both to improve tag suggestion systems and to discover connections between the concepts that the tags represent. Existing approaches have largely relied on tag co-occurrences. In this paper, we show how to find connections between tags by comparing their distributions over time and space, discovering tags with similar geographic and temporal patterns of use. Geo-spatial, temporal and geo-temporal distributions of tags are extracted and represented as vectors which can then be compared and clustered. Using a dataset of tens of millions of geo-tagged Flickr photos, we show that we can cluster Flickr photo tags based on their geographic and temporal patterns, and we evaluate the results both qualitatively and quantitatively using a panel of human judges. We also develop visualizations of temporal and geographic tag distributions, and show that they help humans recognize semantic relationships between tags. This approach to finding and visualizing similar tags is potentially useful for exploring any data having geographic and temporal annotations.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {33–42},
numpages = {10},
keywords = {tag semantics and visualization, geo-spatial and temporal clustering, flickr},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124303,
author = {Dalvi, Nilesh and Kumar, Ravi and Pang, Bo},
title = {Object Matching in Tweets with Spatial Models},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124303},
doi = {10.1145/2124295.2124303},
abstract = {Despite their 140-character limitation, tweets embody a lot of valuable information, especially temporal and spatial. In this paper we study the geographic aspects of tweets, for a given object domain. We propose a user-level model for spatial encoding in tweets that goes beyond the explicit geo-coding or place name mentions; this model can be used to match objects to tweets. We illustrate our model and methodology using restaurants as the objects, and show a significant improvement in performance over using standard language models. En route, we obtain a method to geolocate users who tweet about geolocated objects; this may be of independent interest.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {43–52},
numpages = {10},
keywords = {language model, spatial model, tweets, object matching},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124305,
author = {Papadakis, George and Ioannou, Ekaterini and Nieder\'{e}e, Claudia and Palpanas, Themis and Nejdl, Wolfgang},
title = {Beyond 100 Million Entities: Large-Scale Blocking-Based Resolution for Heterogeneous Data},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124305},
doi = {10.1145/2124295.2124305},
abstract = {A prerequisite for leveraging the vast amount of data available on the Web is Entity Resolution, i.e., the process of identifying and linking data that describe the same real-world objects. To make this inherently quadratic process applicable to large data sets, blocking is typically employed: entities (records) are grouped into clusters - the blocks - of matching candidates and only entities of the same block are compared. However, novel blocking techniques are required for dealing with the noisy, heterogeneous, semi-structured, user-generateddata in the Web, as traditional blocking techniques are inapplicable due to their reliance on schema information. The introduction of redundancy, improves the robustness of blocking methods but comes at the price of additional computational cost.In this paper, we present methods for enhancing the efficiency of redundancy-bearing blocking methods, such as our attribute-agnostic blocking approach. We introduce novel blocking schemes that build blocks based on a variety of evidences, including entity identifiers and relationships between entities; they significantly reduce the required number of comparisons, while maintaining blocking effectiveness at very high levels. We also introduce two theoretical measures that provide a reliable estimation of the performance of a blocking method, without requiring the analytical processing of its blocks. Based on these measures, we develop two techniques for improving the performance of blocking: combining individual, complementary blocking schemes, and purging blocks until given criteria are satisfied. We test our methods through an extensive experimental evaluation, using a voluminous data set with 182 million heterogeneous entities. The outcomes of our study show the applicability and the high performance of our approach.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {53–62},
numpages = {10},
keywords = {attribute-agnostic blocking, data cleaning, entity resolution},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124306,
author = {Fang, Yi and Si, Luo and Somasundaram, Naveen and Yu, Zhengtao},
title = {Mining Contrastive Opinions on Political Texts Using Cross-Perspective Topic Model},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124306},
doi = {10.1145/2124295.2124306},
abstract = {This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {63–72},
numpages = {10},
keywords = {opinion mining, opinion retrieval, topic modeling, contrastive opinions},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124307,
author = {Talukdar, Partha Pratim and Wijaya, Derry and Mitchell, Tom},
title = {Coupled Temporal Scoping of Relational Facts},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124307},
doi = {10.1145/2124295.2124307},
abstract = {Recent research has made significant advances in automatically constructing knowledge bases by extracting relational facts (e.g., Bill Clinton-presidentOf-US) from large text corpora. Temporally scoping such relational facts in the knowledge base (i.e., determining that Bill Clinton-presidentOf-US is true only during the period 1993 - 2001) is an important, but relatively unexplored problem. In this paper, we propose a joint inference framework for this task, which leverages fact-specific temporal constraints, and weak supervision in the form of a few labeled examples. Our proposed framework, CoTS (Coupled Temporal Scoping), exploits temporal containment, alignment, succession, and mutual exclusion constraints among facts from within and across relations. Our contribution is multi-fold. Firstly, while most previous research has focused on micro-reading approaches for temporal scoping, we pose it in a macro-reading fashion, as a change detection in a time series of facts' features computed from a large number of documents. Secondly, to the best of our knowledge, there is no other work that has used joint inference for temporal scoping. We show that joint inference is effective compared to doing temporal scoping of individual facts independently. We conduct our experiments on large scale open-domain publicly available time-stamped datasets, such as English Gigaword Corpus and Google Books Ngrams, demonstrating CoTS's effectiveness.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {73–82},
numpages = {10},
keywords = {joint inference, knowledge base, temporal scoping},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124308,
author = {Dasgupta, Anirban and Gurevich, Maxim and Zhang, Liang and Tseng, Belle and Thomas, Achint O.},
title = {Overcoming Browser Cookie Churn with Clustering},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124308},
doi = {10.1145/2124295.2124308},
abstract = {Many large Internet websites are accessed by users anonymously, without requiring registration or logging-in. However, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. Cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user (web browser). However, users may reset cookies, making such association short-lived and noisy. In this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. Such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems.We present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. We address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. We observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. The scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrained K-means.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {83–92},
numpages = {10},
keywords = {distributed computing, bayes factor, clustering algorithms, similarity measure, browser cookie churn},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124309,
author = {Tang, Jiliang and Gao, Huiji and Liu, Huan},
title = {MTrust: Discerning Multi-Faceted Trust in a Connected World},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124309},
doi = {10.1145/2124295.2124309},
abstract = {Traditionally, research about trust assumes a single type of trust between users. However, trust, as a social concept, inherently has many facets indicating multiple and heterogeneous trust relationships between users. Due to the presence of a large trust network for an online user, it is necessary to discern multi-faceted trust as there are naturally experts of different types. Our study in product review sites reveals that people place trust differently to different people. Since the widely used adjacency matrix cannot capture multi-faceted trust relationships between users, we propose a novel approach by incorporating these relationships into traditional rating prediction algorithms to reliably estimate their strengths. Our work results in interesting findings such as heterogeneous pairs of reciprocal links. Experimental results on real-world data from Epinions and Ciao show that our work of discerning multi-faceted trust can be applied to improve the performance of tasks such as rating prediction, facet-sensitive ranking, and status theory.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {93–102},
numpages = {10},
keywords = {trust network, multi-dimension tie strength, heterogeneous trust, multi-faceted trust},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124310,
author = {Najork, Marc and Fetterly, Dennis and Halverson, Alan and Kenthapadi, Krishnaram and Gollapudi, Sreenivas},
title = {Of Hammers and Nails: An Empirical Comparison of Three Paradigms for Processing Large Graphs},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124310},
doi = {10.1145/2124295.2124310},
abstract = {Many phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (e.g. PageRank) becomes challenging.This paper presents an empirical study of computations on such large graphs in three well-studied platform models, viz., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs viz., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {103–112},
numpages = {10},
keywords = {graph algorithms, graph servers, databases, very large graphs, data-parallel computing},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124311,
author = {Long, Bo and Chang, Yi and Dong, Anlei and He, Jianzhang},
title = {Pairwise Cross-Domain Factor Model for Heterogeneous Transfer Ranking},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124311},
doi = {10.1145/2124295.2124311},
abstract = {Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation systems. Traditional ranking mainly focuses on one type of data source, and effective modeling relies on a sufficiently large number of labeled examples, which require expensive and time-consuming labeling process. However, in many real-world applications, ranking over multiple related heterogeneous domains becomes a common situation, where in some domains we may have a relatively large amount of training data while in some other domains we can only collect very little. Theretofore, how to leverage labeled information from related heterogeneous domain to improve ranking in a target domain has become a problem of great interests. In this paper, we propose a novel probabilistic model, pairwise cross-domain factor model, to address this problem. The proposed model learns latent factors(features) for multi-domain data in partially-overlapped heterogeneous feature spaces. It is capable of learning homogeneous feature correlation, heterogeneous feature correlation, and pairwise preference correlation for cross-domain knowledge transfer. We also derive two PCDF variations to address two important special cases. Under the PCDF model, we derive a stochastic gradient based algorithm, which facilitates distributed optimization and is flexible to adopt different loss functions and regularization functions to accommodate different data distributions. The extensive experiments on real world data sets demonstrate the effectiveness of the proposed model and algorithm.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {113–122},
numpages = {10},
keywords = {heterogeneous transfer ranking, ranking, stochastic gradient descent, pairwise cross-domain factor model, source domain, homogeneous transfer ranking, target domain},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124312,
author = {Ahmed, Amr and Aly, Moahmed and Gonzalez, Joseph and Narayanamurthy, Shravan and Smola, Alexander J.},
title = {Scalable Inference in Latent Variable Models},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124312},
doi = {10.1145/2124295.2124312},
abstract = {Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets.In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {123–132},
numpages = {10},
keywords = {graphical models, large-scale systems, inference, latent models},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124313,
author = {Rendle, Steffen},
title = {Learning Recommender Systems with Adaptive Regularization},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124313},
doi = {10.1145/2124295.2124313},
abstract = {Many factorization models like matrix or tensor factorization have been proposed for the important application of recommender systems. The success of such factorization models depends largely on the choice of good values for the regularization parameters. Without a careful selection they result in poor prediction quality as they either underfit or overfit the data. Regularization values are typically determined by an expensive search that requires learning the model parameters several times: once for each tuple of candidate values for the regularization parameters. In this paper, we present a new method that adapts the regularization automatically while training the model parameters. To achieve this, we optimize simultaneously for two criteria: (1) as usual the model parameters for the regularized objective and (2) the regularization of future parameter updates for the best predictive quality on a validation set. We develop this for the generic model class of Factorization Machines which subsumes a wide variety of factorization models. We show empirically, that the advantages of our adaptive regularization method compared to expensive hyperparameter search do not come to the price of worse predictive quality. In total with our method, learning regularization parameters is as easy as learning model parameters and thus there is no need for any time-consuming search of regularization values because they are found on-the-fly. This makes our method highly attractive for practical use.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {133–142},
numpages = {10},
keywords = {regularization, tensor factorization, matrix factorization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124314,
author = {Balakrishnan, Suhrid and Chopra, Sumit},
title = {Collaborative Ranking},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124314},
doi = {10.1145/2124295.2124314},
abstract = {Typical recommender systems use the root mean squared error (RMSE) between the predicted and actual ratings as the evaluation metric. We argue that RMSE is not an optimal choice for this task, especially when we will only recommend a few (top) items to any user. Instead, we propose using a ranking metric, namely normalized discounted cumulative gain (NDCG), as a better evaluation metric for this task. Borrowing ideas from the learning to rank community for web search, we propose novel models which approximately optimize NDCG for the recommendation task. Our models are essentially variations on matrix factorization models where we also additionally learn the features associated with the users and the items for the ranking task. Experimental results on a number of standard collaborative filtering data sets validate our claims. The results also show the accuracy and efficiency of our models and the benefits of learning features for ranking.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {143–152},
numpages = {10},
keywords = {ndcg, collaborative ranking, learning to rank, recommender systems, rmse},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124315,
author = {De Francisci Morales, Gianmarco and Gionis, Aristides and Lucchese, Claudio},
title = {From Chatter to Headlines: Harnessing the Real-Time Web for Personalized News Recommendation},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124315},
doi = {10.1145/2124295.2124315},
abstract = {We propose a new methodology for recommending interesting news to users by exploiting the information in their twitter persona. We model relevance between users and news articles using a mix of signals drawn from the news stream and from twitter: the profile of the social neighborhood of the users, the content of their own tweet stream, and topic popularity in the news and in the whole twitter-land.We validate our approach on a real-world dataset of approximately 40k articles coming from Yahoo! News and one month of crawled twitter data. We train our model using a learning-to-rank approach and support-vector machines. The train and test set are drawn from Yahoo! toolbar log data. We heuristically identify 3214 users of twitter in the log and use their clicks on news articles to train our system.Our methodology is able to predict with good accuracy the news articles clicked by the users and rank them higher than other news articles. The results show that the combination of various signals from real-time Web and micro-blogging platforms can be a useful resource to understand user behavior.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {153–162},
numpages = {10},
keywords = {personalization, news recommendation, micro-blogging applications, recommendation systems, real-time web},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124316,
author = {Moghaddam, Samaneh and Jamali, Mohsen and Ester, Martin},
title = {ETF: Extended Tensor Factorization Model for Personalizing Prediction of Review Helpfulness},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124316},
doi = {10.1145/2124295.2124316},
abstract = {Online reviews are valuable sources of information for a variety of decision-making processes such as purchasing products. As the number of online reviews is growing rapidly, it becomes increasingly difficult for users to identify those that are helpful. This has motivated research into the problem of identifying high quality and helpful reviews automatically. The current methods assume that the helpfulness of a review is independent from the readers of that review. However, we argue that the quality of a review may not be the same for different users. For example, a professional and an amateur photographer may rate the helpfulness of a review very differently. In this paper, we introduce the problem of predicting a personalized review quality for recommendation of helpful reviews. To address this problem, we propose a series of increasingly sophisticated probabilistic graphical models, based on Matrix Factorization and Tensor Factorization. We evaluate the proposed models using a database of 1.5 million reviews and more than 13 million quality ratings obtained from Epinions.com. The experiments demonstrate that the proposed latent factor models outperform the state-of-the art approaches using textual and social features. Finally, our experiments confirm that the helpfulness of a review is indeed not the same for all users and that there are some latent factors that affect a user's evaluation of the review quality.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {163–172},
numpages = {10},
keywords = {tensor factorization, personalized review quality prediction, review recommendation, matrix factorization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124317,
author = {Krohn-Grimberghe, Artus and Drumond, Lucas and Freudenthaler, Christoph and Schmidt-Thieme, Lars},
title = {Multi-Relational Matrix Factorization Using Bayesian Personalized Ranking for Social Network Data},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124317},
doi = {10.1145/2124295.2124317},
abstract = {A key element of the social networks on the internet such as Facebook and Flickr is that they encourage users to create connections between themselves, other users and objects.One important task that has been approached in the literature that deals with such data is to use social graphs to predict user behavior (e.g. joining a group of interest). More specifically, we study the cold-start problem, where users only participate in some relations, which we will call social relations, but not in the relation on which the predictions are made, which we will refer to as target relations.We propose a formalization of the problem and a principled approach to it based on multi-relational factorization techniques. Furthermore, we derive a principled feature extraction scheme from the social data to extract predictors for a classifier on the target relation. Experiments conducted on real world datasets show that our approach outperforms current methods.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {173–182},
numpages = {10},
keywords = {item prediction, ranking, matrix factorization, cold-start, recommender systems, multi-relational learning, joint factorization, item recommendation, social network},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124318,
author = {Kant, Ravi and Sengamedu, Srinivasan H. and Kumar, Krishnan S.},
title = {Comment Spam Detection by Sequence Mining},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124318},
doi = {10.1145/2124295.2124318},
abstract = {Comments are supported by several web sites to increase user participation. Users can usually comment on a variety of media types - photos, videos, news articles, blogs, etc. Comment spam is one of the biggest challenges facing this feature. The traditional approach to combat spam is to train classifiers using various machine learning techniques. Since the commonly used classifiers work on the entire comment text, it is easy to mislead them by embedding spam content in good content.In this paper, we make several contributions towards comment spam detection. (1) We propose a new framework for spam detection that is immune to embed attacks. We characterize spam by a set of frequently occurring sequential patterns. (2) We introduce a variant (called min-closed) of the frequent closed sequence mining problem that succinctly captures all the frequently occurring patterns. We prove as well as experimentally show that the set of min-closed sequences is an order of magnitude smaller than the set of closed sequences and yet has exactly the same coverage. (3) We describe MCPRISM, extension of the recently published PRISM algorithm that effectively mines min-closed sequences, using prime encoding. In the process, we solve the open problem of using the prime-encoding technique to speed up traditional closed sequence mining. (4) We finally need to whittle down the set of frequent subsequences to a small set without sacrificing coverage. This problem is NP-Hard but we show that the coverage function is submodular and hence the greedy heuristic gives a fast algorithm that is close to optimal. We then describe the experiments that were carried out on a large real world comment data and the publicly available Gazelle dataset. (1) We show that nearly 80% of spam on real world data can be effectively captured by the mined sequences at very low false positive rates. (2) The sequences mined are highly discriminative. (3) On Gazelle data, the proposed algorithmic enhancements are faster by at least by a factor and by an order of magnitude on the larger comment dataset.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {183–192},
numpages = {10},
keywords = {frequent subsequence mining, comments spam detection},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124319,
author = {Amiri, Hadi and Chua, Tat-Seng},
title = {Mining Slang and Urban Opinion Words and Phrases from CQA Services: An Optimization Approach},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124319},
doi = {10.1145/2124295.2124319},
abstract = {Current opinion lexicons contain most of the common opinion words, but they miss slang and so-called urban opinion words and phrases (e.g. delish, cozy, yummy, nerdy, and yuck). These subjectivity clues are frequently used in community questions and are useful for opinion question analysis. This paper introduces a principled approach to constructing an opinion lexicon for community-based question answering (cQA) services. We formulate the opinion lexicon induction as a semi-supervised learning task in the graph context. Our method makes use of existing opinion words to extract new opinion entities (slang and urban words/phrases) from community questions. It then models the opinion entities in a graph context to learn the polarity of the new opinion entities based on the graph connectivity information. In contrast to previous approaches, our method not only learns such polarities from the labeled data but also from the unlabeled data and is more feasible in the web context where the dictionary-based relations (such as synonym, antonym, or hyponym) between most words are not available for constructing a high quality graph. The experiments show that our approach is effective both in terms of the quality of the discovered new opinion entities as well as its ability in inferring their polarity. Furthermore, since the value of opinion lexicons lies in their usefulness in applications, we show the utility of the constructed lexicon in the sentiment classification task.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {193–202},
numpages = {10},
keywords = {slang, opinion mining, sentiment orientation, opinion lexicon, sentiment analysis, urban word},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124320,
author = {Tsur, Oren and Rappoport, Ari},
title = {What's in a Hashtag? Content Based Prediction of the Spread of Ideas in Microblogging Communities},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124320},
doi = {10.1145/2124295.2124320},
abstract = {Current social media research mainly focuses on temporal trends of the information flow and on the topology of the social graph that facilitates the propagation of information. In this paper we study the effect of the content of the idea on the information propagation. We present an efficient hybrid approach based on a linear regression for predicting the spread of an idea in a given time frame. We show that a combination of content features with temporal and topological features minimizes prediction error.Our algorithm is evaluated on Twitter hashtags extracted from a dataset of more than 400 million tweets. We analyze the contribution and the limitations of the various feature types to the spread of information, demonstrating that content aspects can be used as strong predictors thus should not be disregarded. We also study the dependencies between global features such as graph topology and content features.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {643–652},
numpages = {10},
keywords = {information diffusion, twitter, microblogging, hashtags, social media},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124322,
author = {Huang, Jeff and Lin, Thomas and White, Ryen W.},
title = {No Search Result Left behind: Branching Behavior with Browser Tabs},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124322},
doi = {10.1145/2124295.2124322},
abstract = {Today's Web browsers allow users to open links in new windows or tabs. This action, which we call 'branching', is sometimes performed on search results when the user plans to eventually visit multiple results. We detect branching behavior on a large commercial search engine with a client-side script on the results page. Two-fifths of all users spawned new tabs on search results in the timeframe of our study; branching usage varied with different query types and vertical. Both branching and backtracking are viable methods for visiting multiple search results. To understand user search strategies, we treat multiple result clicks following a query as ordered events to understand user search strategies. Users branching in a query are more likely to click search results from top to bottom, while users who backtrack are less likely to do so; this is especially true for queries involving more than two clicks. These findings inform an experiment in which we take a popular click model and modify it to account for the differing user behavior when branching. By understanding that users continue examining search results before viewing a branched result, we can improve the click model for branching queries.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {203–212},
numpages = {10},
keywords = {click models, examining search results, browser tabs},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124323,
author = {Kim, Jin Young and Collins-Thompson, Kevyn and Bennett, Paul N. and Dumais, Susan T.},
title = {Characterizing Web Content, User Interests, and Search Behavior by Reading Level and Topic},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124323},
doi = {10.1145/2124295.2124323},
abstract = {A user's expertise or ability to understand a document on a given topic is an important aspect of that document's relevance. However, this aspect has not been well-explored in information retrieval systems, especially those at Web scale where the great diversity of content, users, and tasks presents an especially challenging search problem. To help improve our modeling and understanding of this diversity, we apply automatic text classifiers, based on reading difficulty and topic prediction, to estimate a novel type of profile for important entities in Web search -- users, websites, and queries. These profiles capture topic and reading level distributions, which we then use in conjunction with search log data to characterize and compare different entities.We find that reading level and topic distributions provide an important new representation of Web content and user interests, and that using both together is more effective than using either one separately. In particular we find that: 1) the reading level of Web content and the diversity of visitors to a website can vary greatly by topic; 2) the degree to which a user's profile matches with a site's profile is closely correlated with the user's preference of the website in search results, and 3) site or URL profiles can be used to predict 'expertness' whether a given site or URL is oriented toward expert vs. non-expert users. Our findings provide strong evidence in favor of jointly incorporating reading level and topic distribution metadata into a variety of critical tasks in Web information systems.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {213–222},
numpages = {10},
keywords = {reading level prediction, topic prediction, web search, domain expertise, log analysis},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124324,
author = {Scaiella, Ugo and Ferragina, Paolo and Marino, Andrea and Ciaramita, Massimiliano},
title = {Topical Clustering of Search Results},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124324},
doi = {10.1145/2124295.2124324},
abstract = {Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them.In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph.We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {223–232},
numpages = {10},
keywords = {search result clustering, topical annotation, user study, spectral clustering},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124325,
author = {Tan, Chenhao and Gabrilovich, Evgeniy and Pang, Bo},
title = {To Each His Own: Personalized Content Selection Based on Text Comprehensibility},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124325},
doi = {10.1145/2124295.2124325},
abstract = {Imagine a physician and a patient doing a search on antibiotic resistance. Or a chess amateur and a grandmaster conducting a search on Alekhine's Defence. Although the topic is the same, arguably the two users in each case will satisfy their information needs with very different texts. Yet today search engines mostly adopt the one-size-fits-all solution, where personalization is restricted to topical preference. We found that users do not uniformly prefer simple texts, and that the text comprehensibility level should match the user's level of preparedness. Consequently, we propose to model the comprehensibility of texts as well as the users' reading proficiency in order to better explain how different users choose content for further exploration. We also model topic-specific reading proficiency, which allows us to better explain why a physician might choose to read sophisticated medical articles yet simple descriptions of SLR cameras. We explore different ways to build user profiles, and use collaborative filtering techniques to overcome data sparsity. We conducted experiments on large-scale datasets from a major Web search engine and a community question answering forum. Our findings confirm that explicitly modeling text comprehensibility can significantly improve content ranking (search results or answers, respectively).},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {233–242},
numpages = {10},
keywords = {personalization, re-ranking, user modeling, text comprehensibility},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124327,
author = {Dalvi, Bhavana Bharat and Cohen, William W. and Callan, Jamie},
title = {WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124327},
doi = {10.1145/2124295.2124327},
abstract = {We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {243–252},
numpages = {10},
keywords = {clustering, hyponymy relation acquisition, web mining},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124328,
author = {Kanani, Pallika H. and McCallum, Andrew K.},
title = {Selecting Actions for Resource-Bounded Information Extraction Using Reinforcement Learning},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124328},
doi = {10.1145/2124295.2124328},
abstract = {Given a database with missing or uncertain content, our goal is to correct and fill the database by extracting specific information from a large corpus such as the Web, and to do so under resource limitations. We formulate the information gathering task as a series of choices among alternative, resource-consuming actions and use reinforcement learning to select the best action at each time step. We use temporal difference q-learning method to train the function that selects these actions, and compare it to an online, error-driven algorithm called SampleRank. We present a system that finds information such as email, job title and department affiliation for the faculty at our university, and show that the learning-based approach accomplishes this task efficiently under a limited action budget. Our evaluations show that we can obtain 92.4% of the final F1, by only using 14.3% of all possible actions.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {253–262},
numpages = {10},
keywords = {active information acquisition, web mining, resource-bounded information extraction, reinforcement learning},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124329,
author = {Panigrahi, Debmalya and Das Sarma, Atish and Aggarwal, Gagan and Tomkins, Andrew},
title = {Online Selection of Diverse Results},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124329},
doi = {10.1145/2124295.2124329},
abstract = {The phenomenal growth in the volume of easily accessible information via various web-based services has made it essential for service providers to provide users with personalized representative summaries of such information. Further, online commercial services including social networking and micro-blogging websites, e-commerce portals, leisure and entertainment websites, etc. recommend interesting content to users that is simultaneously diverse on many different axes such as topic, geographic specificity, etc. The key algorithmic question in all these applications is the generation of a succinct, representative, and relevant summary from a large stream of data coming from a variety of sources. In this paper, we formally model this optimization problem, identify its key structural characteristics, and use these observations to design an extremely scalable and efficient algorithm. We analyze the algorithm using theoretical techniques to show that it always produces a nearly optimal solution. In addition, we perform large-scale experiments on both real-world and synthetically generated datasets, which confirm that our algorithm performs even better than its analytical guarantees in practice, and also outperforms other candidate algorithms for the problem by a wide margin.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {263–272},
numpages = {10},
keywords = {result diversity, online algorithm},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124330,
author = {Andersen, Reid and Gleich, David F. and Mirrokni, Vahab},
title = {Overlapping Clusters for Distributed Computation},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124330},
doi = {10.1145/2124295.2124330},
abstract = {Most graph decomposition procedures seek to partition a graph into disjoint sets of vertices. Motivated by applications of clustering in distributed computation, we describe a graph decomposition algorithm for the paradigm where the partitions intersect. This algorithm covers the vertex set with a collection of overlapping clusters. Each vertex in the graph is well-contained within some cluster in the collection. We then describe a framework for distributed computation across a collection of overlapping clusters and describe how this framework can be used in various algorithms based on the graph diffusion process. In particular, we focus on two illustrative examples: (i) the simulation of a randomly walking particle and (ii) the solution of a linear system, e.g. PageRank. Our simulation results for these two cases show a significant reduction in swapping between clusters in a random walk, a significant decrease in communication volume during a linear system solve in a geometric mesh, and some ability to reduce the communication volume during a linear system solve in an information network.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {273–282},
numpages = {10},
keywords = {covering, pagerank, diffusion, leave-time, conductance},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124332,
author = {Papadimitriou, Panagiotis and Garcia-Molina, Hector},
title = {Sponsored Search Auctions with Conflict Constraints},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124332},
doi = {10.1145/2124295.2124332},
abstract = {In sponsored search auctions advertisers compete for ad slots in the search engine results page, by bidding on keywords of interest. To improve advertiser expressiveness, we augment the bidding process with conflict constraints. With such constraints, advertisers can condition their bids on the non-appearance of certain undesired ads on the results page. We study the complexity of the allocation problem in these augmented SSA and we introduce an algorithm that can efficiently allocate the ad slots to advertisers. We evaluate the algorithm run time in simulated conflict scenarios and we study the implications of the conflict constraints on search engine revenue. Our results show that the allocation problem can be solved within few tens of milliseconds and that the adoption of conflict constraints can potentially increase search engine revenue.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {283–292},
numpages = {10},
keywords = {sponsored search, branch-and-bound, conflicts, auction design},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124333,
author = {Rosales, R\'{o}mer and Cheng, Haibin and Manavoglu, Eren},
title = {Post-Click Conversion Modeling and Analysis for Non-Guaranteed Delivery Display Advertising},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124333},
doi = {10.1145/2124295.2124333},
abstract = {In on-line search and display advertising, the click-trough rate (CTR) has been traditionally a key measure of ad/campaign effectiveness. More recently, the market has gained interest in more direct measures of profitability, one early alternative is the conversion rate (CVR). CVRs measure the proportion of certain users who take a predefined, desirable action, such as a purchase, registration, download, etc.; as compared to simply page browsing. We provide a detailed analysis of conversion rates in the context of non-guaranteed delivery targeted advertising. In particular we focus on the post-click conversion (PCC) problem or the analysis of conversions after a user click on a referring ad. The key elements we study are the probability of a conversion given a click in a user/page context, P(conversion | click, context). We provide various fundamental properties of this process based on contextual information, formalize the problem of predicting PCC, and propose an approach for measuring attribute relevance when the underlying attribute distribution is non-stationary. We provide experimental analyses based on logged events from a large-scale advertising platform.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {293–302},
numpages = {10},
keywords = {conversion rate, display advertising, non-guaranteed delivery, conversion modeling, post-click conversion},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124334,
author = {Xu, Danqing and Liu, Yiqun and Zhang, Min and Ma, Shaoping and Ru, Liyun},
title = {Incorporating Revisiting Behaviors into Click Models},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124334},
doi = {10.1145/2124295.2124334},
abstract = {Click-through behaviors are treated as invaluable sources of user feedback and they have been leveraged in several commercial search engines in recent years. However, estimating unbiased relevance is always a challenging task because of position bias. To solve this problem, many researchers have proposed a variety of assumptions to model click-through behaviors. Most of these models share a common examination hypothesis, which is that users examine search results from the top to the bottom. Nevertheless, this model cannot draw a complete picture of information-seeking behaviors. Many eye-tracking studies find that user interactions are not sequential but contain revisiting patterns. If a user clicks on a higher ranked document after having clicked on a lower-ranked one, we call this scenario a revisiting pattern, and we believe that the revisiting patterns are important signals regarding a user's click preferences. This paper incorporates revisiting behaviors into click models and introduces a novel click model named Temporal Hidden Click Model (THCM). This model dynamically models users' click behaviors with a temporal order. In our experiment, we collect over 115 million query sessions from a widely-used commercial search engine and then conduct a comparative analysis between our model and several state-of-the-art click models. The experimental results show that the THCM model achieves a significant improvement in the Normalized Discounted Cumulative Gain (NDCG), the click perplexity and click distributions metrics.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {303–312},
numpages = {10},
keywords = {temporal hidden click model, document relevance, click-through behavior, revisit},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124335,
author = {Chen, Weizhu and Wang, Dong and Zhang, Yuchen and Chen, Zheng and Singla, Adish and Yang, Qiang},
title = {A Noise-Aware Click Model for Web Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124335},
doi = {10.1145/2124295.2124335},
abstract = {Recent advances in click model have established it as an attractive approach to infer document relevance. Most of these advances consider the user click/skip behavior as binary events but neglect the context in which a click happens. We show that real click behavior in industrial search engines is often noisy and not always a good indication of relevance. For a considerable percentage of clicks, users select what turn out to be irrelevant documents and these clicks should not be directly used as evidence for relevance inference. Thus in this paper, we put forward an observation that the relevance indication degree of a click is not a constant, but can be differentiated by user preferences and the context in which the user makes her click decision. In particular, to interpret the click behavior discriminatingly, we propose a Noise-aware Click Model (NCM) by characterizing the noise degree of a click, which indicates the quality of the click for inferring relevance. Specifically, the lower the click noise is, the more important the click is in its role for relevance inference. To verify the necessity of explicitly accounting for the uninformative noise in a user click, we conducted experiments on a billion-scale dataset. Extensive experimental results demonstrate that as compared with two state-of-the-art click models in Web Search, NCM can better interpret user click behavior and achieve significant improvements in terms of both perplexity and NDCG.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {313–322},
numpages = {10},
keywords = {click model, click noise, log analysis},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124336,
author = {Shen, Si and Hu, Botao and Chen, Weizhu and Yang, Qiang},
title = {Personalized Click Model through Collaborative Filtering},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124336},
doi = {10.1145/2124295.2124336},
abstract = {Click modeling aims to interpret the users' search click data in order to predict their clicking behavior. Existing models can well characterize the position bias of documents and snippets in relation to users' mainstream click behavior. Yet, current advances depict users' search actions only in a general setting by implicitly assuming that all users act in the same way, regardless of the fact that anyone, motivated with some individual interest, is more likely to click on a link than others. It is in light of this that we put forward a novel personalized click model to describe the user-oriented click preferences, which applies and extends matrix / tensor factorization from the view of collaborative filtering to connect users, queries and documents together. Our model serves as a generalized personalization framework that can be incorporated to the previously proposed click models and, in many cases, to their future extensions. Despite the sparsity of search click data, our personalized model demonstrates its advantage over the best click models previously discussed in the Web-search literature, supported by our large-scale experiments on a real dataset. A delightful bonus is the model's ability to gain insights into queries and documents through latent feature vectors, and hence to handle rare and even new query-document pairs much better than previous click models.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {323–332},
numpages = {10},
keywords = {personalization, click model, user behavior, click log analysis, search engine},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124337,
author = {Ahmed, Amr and Teo, Choon Hui and Vishwanathan, S.V.N. and Smola, Alex},
title = {Fair and Balanced: Learning to Present News Stories},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124337},
doi = {10.1145/2124295.2124337},
abstract = {Relevance, diversity and personalization are key issues when presenting content which is apt to pique a user's interest. This is particularly true when presenting an engaging set of news stories. In this paper we propose an efficient algorithm for selecting a small subset of relevant articles from a streaming news corpus. It offers three key pieces of improvement over past work: 1) It is based on a detailed model of a user's viewing behavior which does not require explicit feedback. 2) We use the notion of submodularity to estimate the propensity of interacting with content. This improves over the classical context independent relevance ranking algorithms. Unlike existing methods, we learn the submodular function from the data. 3) We present an efficient online algorithm which can be adapted for personalization, story adaptation, and factorization models. Experiments show that our system yields a significant improvement over a retrieval system deployed in production.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {333–342},
numpages = {10},
keywords = {graphical models, online learning, submodularity, personalization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124338,
author = {Wang, Chen and Bi, Keping and Hu, Yunhua and Li, Hang and Cao, Guihong},
title = {Extracting Search-Focused Key n-Grams for Relevance Ranking in Web Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124338},
doi = {10.1145/2124295.2124338},
abstract = {In web search, relevance ranking of popular pages is relatively easy, because of the inclusion of strong signals such as anchor text and search log data. In contrast, with less popular pages, relevance ranking becomes very challenging due to a lack of information. In this paper the former is referred to as head pages, and the latter tail pages. We address the challenge by learning a model that can extract search-focused key n-grams from web pages, and using the key n-grams for searches of the pages, particularly, the tail pages. To the best of our knowledge, this problem has not been previously studied. Our approach has four characteristics. First, key n-grams are search-focused in the sense that they are defined as those which can compose "good queries" for searching the page. Second, key n-grams are learned in a relative sense using learning to rank techniques. Third, key n-grams are learned using search log data, such that the characteristics of key n-grams in the search log data, particularly in the heads; can be applied to the other data, particularly to the tails. Fourth, the extracted key n-grams are used as features of the relevance ranking model also trained with learning to rank techniques. Experiments validate the effectiveness of the proposed approach with large-scale web search datasets. The results show that our approach can significantly improve relevance ranking performance on both heads and tails; and particularly tails, compared with baseline approaches. Characteristics of our approach have also been fully investigated through comprehensive experiments.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {343–352},
numpages = {10},
keywords = {search relevance, learning to rank, tail page, ranking, key n-gram extraction},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124339,
author = {Song, Yang and Zhou, Dengyong and He, Li-wei},
title = {Query Suggestion by Constructing Term-Transition Graphs},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124339},
doi = {10.1145/2124295.2124339},
abstract = {Query suggestion is an interactive approach for search engines to better understand users information need. In this paper, we propose a novel query suggestion framework which leverages user re-query feedbacks from search engine logs. Specifically, we mined user query reformulation activities where the user only modifies part of the query by (1) adding terms after the query, (2) deleting terms within the query, or (3) modifying terms to new terms. We build a term-transition graph based on the mined data. Two models are proposed which address topic-level and term-level query suggestions, respectively. In the first topic-based unsupervised Pagerank model, we perform random walk on each of the topic-based term-transition graph and calculate the Pagerank for each term within a topic. Given a new query, we suggest relevant queries based on its topic distribution and term-transition probability within each topic. Our second model resembles the supervised learning-to-rank (LTR) framework, in which term modifications are treated as documents so that each query reformulation is treated as a training instance. A rich set of features are constructed for each (query, document) pair from Pagerank, Wikipedia, N-gram, ODP and so on. This supervised model is capable of suggesting new queries on a term level which addresses the limitation of previous methods. Experiments are conducted on a large data set from a commercial search engine. By comparing the with state-of-the-art query suggestion methods [4, 2], our proposals exhibit significant performance increase for all categories of queries.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {353–362},
numpages = {10},
keywords = {pagerank, learning-to-rank, query suggestion},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124340,
author = {Mass, Yosi and Sagiv, Yehoshua},
title = {Language Models for Keyword Search over Data Graphs},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124340},
doi = {10.1145/2124295.2124340},
abstract = {In keyword search over data graphs, an answer is a non-redundant subtree that includes the given keywords. This paper focuses on improving the effectiveness of that type of search. A novel approach that combines language models with structural relevance is described. The proposed approach consists of three steps. First, language models are used to assign dynamic, query-dependent weights to the graph. Those weights complement static weights that are pre-assigned to the graph. Second, an existing algorithm returns candidate answers based on their weights. Third, the candidate answers are re-ranked by creating a language model for each one. The effectiveness of the proposed approach is verified on a benchmark of three datasets: IMDB, Wikipedia and Mondial. The proposed approach outperforms all existing systems on the three datasets, which is a testament to its robustness. It is also shown that the effectiveness can be further improved by augmenting keyword queries with very basic knowledge about the structure.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {363–372},
numpages = {10},
keywords = {semantic weights, data graphs, ranking, language models},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124341,
author = {Buscher, Georg and White, Ryen W. and Dumais, Susan and Huang, Jeff},
title = {Large-Scale Analysis of Individual and Task Differences in Search Result Page Examination Strategies},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124341},
doi = {10.1145/2124295.2124341},
abstract = {Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {373–382},
numpages = {10},
keywords = {rich interaction logging, individual differences, task differences},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124342,
author = {Cheung, Jackie Chi Kit and Li, Xiao},
title = {Sequence Clustering and Labeling for Unsupervised Query Intent Discovery},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124342},
doi = {10.1145/2124295.2124342},
abstract = {One popular form of semantic search observed in several modern search engines is to recognize query patterns that trigger instant answers or domain-specific search, producing semantically enriched search results. This often requires understanding the query intent in addition to the meaning of the query terms in order to access structured data sources. A major challenge in intent understanding is to construct a domain-dependent schema and to annotate search queries based on such a schema, a process that to date has required much manual annotation effort. We present an unsupervised method for clustering queries with similar intent and for producing a pattern consisting of a sequence of semantic concepts and/or lexical items for each intent. Furthermore, we leverage the discovered intent patterns to automatically annotate a large number of queries beyond those used in clustering. We evaluated our method on 10 selected domains, discovering over 1400 intent patterns and automatically annotating 125K (and potentially many more) queries. We found that over 90% of patterns and 80% of instance annotations tested are judged to be correct by a majority of annotators.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {383–392},
numpages = {10},
keywords = {query intent discovery, semantic search, clustering},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124343,
author = {Pavlu, Virgil and Rajput, Shahzad and Golbus, Peter B. and Aslam, Javed A.},
title = {IR System Evaluation Using Nugget-Based Test Collections},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124343},
doi = {10.1145/2124295.2124343},
abstract = {The development of information retrieval systems such as search engines relies on good test collections, including assessments of retrieved content. The widely employed Cranfield paradigm dictates that the information relevant to a topic be encoded at the level of documents, therefore requiring effectively complete document relevance assessments. As this is no longer practical for modern corpora, numerous problems arise, including scalability, reusability, and applicability. We propose a new method for relevance assessment based on relevant information, not relevant documents. Once the relevant 'nuggets' are collected, our matching method can assess any document for relevance with high accuracy, and so any retrieved list of documents can be assessed for performance. In this paper we analyze the performance of the matching function by looking at specific cases and by comparing with other methods. We then show how these inferred relevance assessments can be used to perform IR system evaluation, and we discuss in particular reusability and scalability. Our main contribution is a methodology for producing test collections that are highly accurate, more complete, scalable, reusable, and can be generated with similar amounts of effort as existing methods, with great potential for future applications.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {393–402},
numpages = {10},
keywords = {test collection, nuggets},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124344,
author = {Kotov, Alexander and Zhai, ChengXiang},
title = {Tapping into Knowledge Base for Concept Feedback: Leveraging Conceptnet to Improve Search Results for Difficult Queries},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124344},
doi = {10.1145/2124295.2124344},
abstract = {Query expansion is an important and commonly used technique for improving Web search results. Existing methods for query expansion have mostly relied on global or local analysis of document collection, click-through data, or simple ontologies such as WordNet. In this paper, we present the results of a systematic study of the methods leveraging the ConceptNet knowledge base, an emerging new Web resource, for query expansion. Specifically, we focus on the methods leveraging ConceptNet to improve the search results for poorly performing (or difficult) queries. Unlike other lexico-semantic resources, such as WordNet and Wikipedia, which have been extensively studied in the past, ConceptNet features a graph-based representation model of commonsense knowledge, in which the terms are conceptually related through rich relational ontology. Such representation structure enables complex, multi-step inferences between the concepts, which can be applied to query expansion. We first demonstrate through simulation experiments that expanding queries with the related concepts from ConceptNet has great potential for improving the search results for difficult queries. We then propose and study several supervised and unsupervised methods for selecting the concepts from ConceptNet for automatic query expansion. The experimental results on multiple data sets indicate that the proposed methods can effectively leverage ConceptNet to improve the retrieval performance of difficult queries both when used in isolation as well as in combination with pseudo-relevance feedback.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {403–412},
numpages = {10},
keywords = {conceptnet, query analysis, knowledge bases, query expansion},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124345,
author = {Ieong, Samuel and Mishra, Nina and Sadikov, Eldar and Zhang, Li},
title = {Domain Bias in Web Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124345},
doi = {10.1145/2124295.2124345},
abstract = {This paper uncovers a new phenomenon in web search that we call domain bias --- a user's propensity to believe that a page is more relevant just because it comes from a particular domain. We provide evidence of the existence of domain bias in click activity as well as in human judgments via a comprehensive collection of experiments. We begin by studying the difference between domains that a search engine surfaces and that users click. Surprisingly, we find that despite changes in the overall distribution of surfaced domains, there has not been a comparable shift in the distribution of clicked domains. Users seem to have learned the landscape of the internet and their click behavior has thus become more predictable over time. Next, we run a blind domain test, akin to a Pepsi/Coke taste test, to determine whether domains can shift a user's opinion of which page is more relevant. We find that domains can actually flip a user's preference about 25% of the time. Finally, we demonstrate the existence of systematic domain preferences, even after factoring out confounding issues such as position bias and relevance, two factors that have been used extensively in past work to explain user behavior. The existence of domain bias has numerous consequences including, for example, the importance of discounting click activity from reputable domains.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {413–422},
numpages = {10},
keywords = {domain bias, user behavior, web search},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124346,
author = {Shan, Dongdong and Ding, Shuai and He, Jing and Yan, Hongfei and Li, Xiaoming},
title = {Optimized Top-k Processing with Global Page Scores on Block-Max Indexes},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124346},
doi = {10.1145/2124295.2124346},
abstract = {Large web search engines are facing formidable performance challenges because they have to process thousands of queries per second on tens of billions of documents, within interactive response time. Among many others, Top-k query processing (also called early termination or dynamic pruning) is an important class of optimization techniques that can improve the search efficiency and achieve faster query processing by avoiding the scoring of documents that are unlikely to be in the top results. One recent technique is using Block-Max index. In the Block-Max index, the posting lists are organized as blocks and the maximum score for each block is stored to improve the query efficiency. Although query processing speedup is achieved with Block-Max index, the ranking function for the Top-k results is the term-based approach. It is well known that documents' static scores are also important for a good ranking function. In this paper, we show that the performance of the state-of-the-art algorithms with the Block-Max index is degraded when the static score is added in the ranking function. Then we study efficient techniques for Top-k query processing in the case where a page's static score is given, such as PageRank, in addition to the term-based approach. In particular, we propose a set of new algorithms based on the WAND and MaxScore with Block-Max index using local score, which outperform the existing ones. Then we propose new techniques to estimate a better score upper bound for each block. We also study the search efficiency on different index structures where the document identifiers are assigned by URL sorting or by static document scores. Experiments on TREC GOV2 and ClueWeb09B show that considerable performance gains are achieved.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {423–432},
numpages = {10},
keywords = {block-max inverted index, top-k query processing, early termination},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124348,
author = {Sontag, David and Collins-Thompson, Kevyn and Bennett, Paul N. and White, Ryen W. and Dumais, Susan and Billerbeck, Bodo},
title = {Probabilistic Models for Personalizing Web Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124348},
doi = {10.1145/2124295.2124348},
abstract = {We present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {433–442},
numpages = {10},
keywords = {machine learning, probabilistic models, re-ranking, personalization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124349,
author = {Bendersky, Michael and Metzler, Donald and Croft, W. Bruce},
title = {Effective Query Formulation with Multiple Information Sources},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124349},
doi = {10.1145/2124295.2124349},
abstract = {Most standard information retrieval models use a single source of information (e.g., the retrieval corpus) for query formulation tasks such as term and phrase weighting and query expansion. In contrast, in this paper, we present a unified framework that automatically optimizes the combination of information sources used for effective query formulation. The proposed framework produces fully weighted and expanded queries that are both more effective and more compact than those produced by the current state-of-the-art query expansion and weighting methods. We conduct an empirical evaluation of our framework for both newswire and web corpora. In all cases, our combination of multiple information sources for query formulation is found to be more effective than using any single source. The proposed query formulations are especially advantageous for large scale web corpora, where they also reduce the number of terms required for effective query expansion, and improve the diversity of the retrieved results.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {443–452},
numpages = {10},
keywords = {query formulation, query expansion, external sources},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124350,
author = {Kang, Changsung and Wang, Xuanhui and Chang, Yi and Tseng, Belle},
title = {Learning to Rank with Multi-Aspect Relevance for Vertical Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124350},
doi = {10.1145/2124295.2124350},
abstract = {Many vertical search tasks such as local search focus on specific domains. The meaning of relevance in these verticals is domain-specific and usually consists of multiple well-defined aspects (e.g., text matching and distance in local search). Thus the overall relevance between a query and a document is a tradeoff between multiple relevance aspects. Such a tradeoff can vary for different types of queries or in different contexts. In this paper, we explore these vertical-specific aspects in the learning to rank setting. We propose a novel formulation in which the relevance between a query and a document is assessed with respect to each aspect, forming the multi-aspect relevance. In order to compute a ranking function, we study two types of learning-based approaches to estimate the tradeoff between these relevance aspects: a label aggregation method and a model aggregation method. Since there are only a few aspects, a minimal amount of training data is needed to learn the tradeoff. We conduct both offline and online test experiments on a local search engine and the experimental results show that our proposed multi-aspect relevance formulation is very promising. The two types of aggregation methods perform more effectively than a set of baseline methods including a conventional learning to rank method.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {453–462},
numpages = {10},
keywords = {aggregation, web search, multi-aspect relevance},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124351,
author = {Chen, Danqi and Chen, Weizhu and Wang, Haixun and Chen, Zheng and Yang, Qiang},
title = {Beyond Ten Blue Links: Enabling User Click Modeling in Federated Web Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124351},
doi = {10.1145/2124295.2124351},
abstract = {Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {463–472},
numpages = {10},
keywords = {click model, log analysis, federated search},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124353,
author = {Liu, Yandong and Pandey, Sandeep and Agarwal, Deepak and Josifovski, Vanja},
title = {Finding the Right Consumer: Optimizing for Conversion in Display Advertising Campaigns},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124353},
doi = {10.1145/2124295.2124353},
abstract = {The ultimate goal of advertisers are conversions representing desired user actions on the advertisers' websites in the form of purchases and product information request. In this paper we address the problem of finding the right audience for display campaigns by finding the users that are most likely to convert. This challenging problem is at the heart of display campaign optimization and has to deal with several issues such as very small percentage of converters in the general population, high-dimensional representation of the user profiles, large churning rate of users and advertisers. To overcome these difficulties, in our approach we use two sources of information: a seed set of users that have converted for a campaign in the past; and a description of the campaign based on the advertiser's website. We explore the importance of the information provided by each of these two sources in a principled manner and then combine them to propose models for predicting converters. In particular, we show how seed set can be used to capture the campaign-specific targeting constraints, while the campaign metadata allows to share targeting knowledge across campaigns. We give methods for learning these models and perform experiments on real-world advertising campaigns. Our findings show that the seed set and the campaign metadata are complimentary to each other and both sources provide valuable information for conversion optimization.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {473–482},
numpages = {10},
keywords = {advertising, modeling, conversions},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124354,
author = {Agarwal, Deepak and Gurevich, Maxim},
title = {Fast Top-k Retrieval for Model Based Recommendation},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124354},
doi = {10.1145/2124295.2124354},
abstract = {A crucial task in many recommender problems like computational advertising, content optimization, and others is to retrieve a small set of items by scoring a large item inventory through some elaborate statistical/machine-learned model. This is challenging since the retrieval has to be fast (few milliseconds) to load the page quickly. Fast retrieval is well studied in the information retrieval (IR) literature, especially in the context of document retrieval for queries. When queries and documents have sparse representation and relevance is measured through cosine similarity (or some variant thereof), one could build highly efficient retrieval algorithms that scale gracefully to increasing item inventory. The key components exploited by such algorithms is sparse query-document representation and the special form of the relevance function. Many machine-learned models used in modern recommender problems do not satisfy these properties and since brute force evaluation is not an option with large item inventory, heuristics that filter out some items are often employed to reduce model computations at runtime.In this paper, we take a two-stage approach where the first stage retrieves top-K items using our approximate procedures and the second stage selects the desired top-k using brute force model evaluation on the K retrieved items. The main idea of our approach is to reduce the first stage to a standard IR problem, where each item is represented by a sparse feature vector (a.k.a. the vector-space representation) and the query-item relevance score is given by vector dot product. The sparse item representation is learnt to closely approximate the original machine-learned score by using retrospective data. Such a reduction allows leveraging extensive work in IR that resulted in highly efficient retrieval systems. Our approach is model-agnostic, relying only on data generated from the machine-learned model. We obtain significant improvements in the computational cost vs. accuracy tradeoff compared to several baselines in our empirical evaluation on both synthetic models and on a click-through (CTR) model used in online advertising.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {483–492},
numpages = {10},
keywords = {inverted index, lasso, top-k retrieval, machine learning, support vector machines},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124355,
author = {Xiong, Chenyan and Wang, Taifeng and Ding, Wenkui and Shen, Yidong and Liu, Tie-Yan},
title = {Relational Click Prediction for Sponsored Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124355},
doi = {10.1145/2124295.2124355},
abstract = {This paper is concerned with the prediction of clicking an ad in sponsored search. The accurate prediction of user's click on an ad plays an important role in sponsored search, because it is widely used in both ranking and pricing of the ads. Previous work on click prediction usually takes a single ad as input, and ignores its relationship to the other ads shown in the same page. This independence assumption here, however, might not be valid in the real scenario. In this paper, we first perform an analysis on this issue by looking at the click-through rates (CTR) of the same ad, in the same position and for the same query, but surrounded by different ads. We found that in most cases the CTR varies largely, which suggests that the relationship between ads is really an important factor in predicting click probability. Furthermore, our investigation shows that the more similar the surrounding ads are to an ad, the lower the CTR of the ad is. Based on this observation, we design a continuous conditional random fields (CRF) based model for click prediction, which considers both the features of an ad and its similarity to the surrounding ads. We show that the model can be effectively learned using maximum likelihood estimation, and can also be efficiently inferred due to its closed form solution. Our experimental results on the click-through log from a commercial search engine show that the proposed model can predict clicks more accurately than previous independent models. To our best knowledge this is the first work that predicts ad clicks by considering the relationship between ads.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {493–502},
numpages = {10},
keywords = {continuous crf, sponsored search, online advertising, relational click prediction},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124356,
author = {Liu, Yang and Chen, Roy and Chen, Yan and Mei, Qiaozhu and Salib, Suzy},
title = {"I Loan Because...": Understanding Motivations for pro-Social Lending},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124356},
doi = {10.1145/2124295.2124356},
abstract = {As a new paradigm of online communities, microfinance sites such as Kiva.org have attracted much public attention. To understand lender motivations on Kiva, we classify the lenders' self-stated motivations into ten categories with human coders and machine learning based classifiers. We employ text classifiers using lexical features, along with social features based on lender activity information on Kiva, to predict the categories of lender motivation statements. Although the task appears to be much more challenging than traditional topic-based categorization, our classifiers can achieve high precision in most categories. Using the results of this classification along with Kiva teams information, we predict lending activity from lender motivation and team affiliations. Finally, we make design recommendations regarding Kiva practices which might increase pro-social lending.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {503–512},
numpages = {10},
keywords = {lending motivation, text classification, microfinance, kiva, pro-social lending},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124358,
author = {Ruiz, Eduardo J. and Hristidis, Vagelis and Castillo, Carlos and Gionis, Aristides and Jaimes, Alejandro},
title = {Correlating Financial Time Series with Micro-Blogging Activity},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124358},
doi = {10.1145/2124295.2124358},
abstract = {We study the problem of correlating micro-blogging activity with stock-market events, defined as changes in the price and traded volume of stocks. Specifically, we collect messages related to a number of companies, and we search for correlations between stock-market events for those companies and features extracted from the micro-blogging messages. The features we extract can be categorized in two groups. Features in the first group measure the overall activity in the micro-blogging platform, such as number of posts, number of re-posts, and so on. Features in the second group measure properties of an induced interaction graph, for instance, the number of connected components, statistics on the degree distribution, and other graph-based properties.We present detailed experimental results measuring the correlation of the stock market events with these features, using Twitter as a data source. Our results show that the most correlated features are the number of connected components and the number of nodes of the interaction graph. The correlation is stronger with the traded volume than with the price of the stock. However, by using a simulator we show that even relatively small correlations between price and micro-blogging features can be exploited to drive a stock trading strategy that outperforms other baseline strategies.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {513–522},
numpages = {10},
keywords = {social networks, micro-blogging, financial time series},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124359,
author = {Awadallah, Rawia and Ramanath, Maya and Weikum, Gerhard},
title = {Harmony and Dissonance: Organizing the People's Voices on Political Controversies},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124359},
doi = {10.1145/2124295.2124359},
abstract = {The wikileaks documents about the death of Osama Bin Laden and the debates about the economic crisis in Greece and other European countries are some of the controversial topics being played on the news everyday. Each of these topics has many different aspects, and there is no absolute, simple truth in answering questions such as: should the EU guarantee the financial stability of each member country, or should the countries themselves be solely responsible? To understand the landscape of opinions, it would be helpful to know which politician or other stakeholder takes which position - support or opposition - on these aspects of controversial topics.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {523–532},
numpages = {10},
keywords = {web information extraction, political opinion mining},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124360,
author = {Becker, Hila and Iter, Dan and Naaman, Mor and Gravano, Luis},
title = {Identifying Content for Planned Events across Social Media Sites},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124360},
doi = {10.1145/2124295.2124360},
abstract = {User-contributed Web data contains rich and diverse information about a variety of events in the physical world, such as shows, festivals, conferences and more. This information ranges from known event features (e.g., title, time, location) posted on event aggregation platforms (e.g., Last.fm events, EventBrite, Facebook events) to discussions and reactions related to events shared on different social media sites (e.g., Twitter, YouTube, Flickr). In this paper, we focus on the challenge of automatically identifying user-contributed content for events that are planned and, therefore, known in advance, across different social media sites. We mine event aggregation platforms to extract event features, which are often noisy or missing. We use these features to develop query formulation strategies for retrieving content associated with an event on different social media sites. Further, we explore ways in which event content identified on one social media site can be used to retrieve additional relevant event content on other social media sites. We apply our strategies to a large set of user-contributed events, and analyze their effectiveness in retrieving relevant event content from Twitter, YouTube, and Flickr.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {533–542},
numpages = {10},
keywords = {event identification, cross-site document retrieval, social media},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124361,
author = {Byers, John W. and Mitzenmacher, Michael and Zervas, Georgios},
title = {Daily Deals: Prediction, Social Diffusion, and Reputational Ramifications},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124361},
doi = {10.1145/2124295.2124361},
abstract = {Daily deal sites have become the latest Internet sensation, providing discounted offers to customers for restaurants, ticketed events, services, and other items. We begin by undertaking a study of the economics of daily deals on the web, based on a dataset we compiled by monitoring Groupon and LivingSocial sales in 20 large cities over several months. We use this dataset to characterize deal purchases; glean insights about operational strategies of these firms; and evaluate customers' sensitivity to factors such as price, deal scheduling, and limited inventory. We then marry our daily deals dataset with additional datasets we compiled from Facebook and Yelp users to study the interplay between social networks and daily deal sites. First, by studying user activity on Facebook while a deal is running, we provide evidence that daily deal sites benefit from significant word-of-mouth effects during sales events, consistent with results predicted by cascade models. Second, we consider the effects of daily deals on the longer-term reputation of merchants, based on their Yelp reviews before and after they run a daily deal. Our analysis shows that while the number of reviews increases significantly due to daily deals, average rating scores from reviewers who mention daily deals are 10% lower than scores of their peers on average.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {543–552},
numpages = {10},
keywords = {reputation, daily deals, social diffusion},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124363,
author = {Qi, Guo-Jun and Aggarwal, Charu C. and Huang, Thomas S.},
title = {On Clustering Heterogeneous Social Media Objects with Outlier Links},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124363},
doi = {10.1145/2124295.2124363},
abstract = {The clustering of social media objects provides intrinsic understanding of the similarity relationships between documents, images, and their contextual sources. Both content and link structure provide important cues for an effective clustering algorithm of the underlying objects. While link information provides useful hints for improving the clustering process, it also contains a significant amount of noisy information. Therefore, a robust clustering algorithm is required to reduce the impact of noisy links. In order to address the aforementioned problems, we propose heterogeneous random fields to model the structure and content of social media networks. We design a probability measure on the social media networks which output a configuration of clusters that are consistent with both content and link structure. Furthermore, noisy links can also be detected, and their impact on the clustering algorithm can be significantly reduced. We conduct experiments on a real social media network and show the advantage of the method over other state-of-the-art algorithms.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {553–562},
numpages = {10},
keywords = {robust clustering, social media networks, noisy links},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124364,
author = {Meij, Edgar and Weerkamp, Wouter and de Rijke, Maarten},
title = {Adding Semantics to Microblog Posts},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124364},
doi = {10.1145/2124295.2124364},
abstract = {Microblogs have become an important source of information for the purpose of marketing, intelligence, and reputation management. Streams of microblogs are of great value because of their direct and real-time nature. Determining what an individual microblog post is about, however, can be non-trivial because of creative language usage, the highly contextualized and informal nature of microblog posts, and the limited length of this form of communication. We propose a solution to the problem of determining what a microblog post is about through semantic linking: we add semantics to posts by automatically identifying concepts that are semantically related to it and generating links to the corresponding Wikipedia articles. The identified concepts can subsequently be used for, e.g., social media mining, thereby reducing the need for manual inspection and selection. Using a purpose-built test collection of tweets, we show that recently proposed approaches for semantic linking do not perform well, mainly due to the idiosyncratic nature of microblog posts. We propose a novel method based on machine learning with a set of innovative features and show that it is able to achieve significant improvements over all other methods, especially in terms of precision.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {563–572},
numpages = {10},
keywords = {wikipedia, semantic linking, microblogs},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124365,
author = {Huang, Junming and Cheng, Xue-Qi and Shen, Hua-Wei and Zhou, Tao and Jin, Xiaolong},
title = {Exploring Social Influence via Posterior Effect of Word-of-Mouth Recommendations},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124365},
doi = {10.1145/2124295.2124365},
abstract = {Word-of-mouth has proven an effective strategy for promoting products through social relations. Particularly, existing studies have convincingly demonstrated that word-of-mouth recommendations can boost users' prior expectation and hence encourage them to adopt a certain innovation, such as buying a book or watching a movie. However, less attention has been paid to studying the posterior effect of word-of-mouth recommendations, i.e., whether or not word-of-mouth recommendations can influence users' posterior evaluation on the products or services recommended to them, the answer to which is critical to estimating user satisfaction when proposing a word-of-mouth marketing strategy. In order to fill this gap, in this paper we empirically study the above issue and verify that word-of-mouth recommendations are strongly associated with users' posterior evaluation. Through elaborately designed statistical hypothesis tests we prove the causality that word-of-mouth recommendations directly prompt the posterior evaluation of receivers. Finally, we propose a method for investigating users' social influence, namely, their ability to affect followers' posterior evaluation via word-of-mouth recommendations, by examining the number of their followers and their sensitivity of discovering good items. The experimental results on real datasets show that our method can successfully identify 78% influential friends with strong social influence.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {573–582},
numpages = {10},
keywords = {social influence, word-of-mouth recommendation, posterior evaluation},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124366,
author = {Xu, Xueke and Tan, Songbo and Liu, Yue and Cheng, Xueqi and Lin, Zheng and Guo, Jiafeng},
title = {Find Me Opinion Sources in Blogosphere: A Unified Framework for Opinionated Blog Feed Retrieval},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124366},
doi = {10.1145/2124295.2124366},
abstract = {This paper aims to find blog feeds having a principal inclination towards making opinionated comments on the given topic, so that we can subscribe to them to track influential and interesting opinions in the blogosphere. One major challenge is assigning topic-related opinion scores to blog feeds, which is embodied in two aspects. Firstly, we should identify whether the blog feed has a principal on-topic opinionated inclination. This inclination should be collectively revealed by all posts of the feed. We should fully consider evidences from all the posts of the feed to identify salient information among many posts of the feed. Secondly, we should capture topic-related opinions in the blog feed while ignoring irrelevant opinions.In this paper, we propose a unified framework for opinionated blog feed retrieval, which combines topic relevance and opinion scores with a generative model. Furthermore, we propose a language modeling approach to estimating opinion scores that is seamlessly integrated into the framework, where two language models, Topic-specific Opinion Model (TOM) and Topic-biased Feed Model (TFM), work collectively to reflect whether the blog feed shows a principal on-topic opinionated inclination. To estimate TFM, we propose a topic-biased random walk to exploit both content and structural information to capture topic-biased salient information in the feed. As for TOM estimation, we propose to use a generative mixture model with prior guidance to effectively capture topic-specific opinion expressing language usage. The conducted experiments in the context of the TREC 2009-2010 Blog Track show the effectiveness of our proposed approaches.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {583–592},
numpages = {10},
keywords = {opinionated blog feed retrieval, mixture model, topic-biased random walk, topic-related opinionatedness},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124367,
author = {Das Sarma, Anish and Gollapudi, Sreenivas and Panigrahy, Rina and Zhang, Li},
title = {Understanding Cyclic Trends in Social Choices},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124367},
doi = {10.1145/2124295.2124367},
abstract = {Motivated by trends in popularity of products, we present a formal model for studying trends in our choice of products in terms of three parameters: (1) their innate utility; (2) individual boredom associated with repeated usage of an item; and (3) social influences associated with the preferences from other people. Different from previous work, in this paper we introduce boredom to explain the cyclic pattern in individual and social choices. We formally model boredom and show that a rational individual would make cyclic choices when considering the boredom factor. Furthermore, we extend the model to social choices by showing that a society that votes for a particular style or product can be viewed as a single individual cycling through different choices.We adopt a natural model of utility an individual derives from using an item, i.e., the utility of an item gets discounted by its repeated use and increases when the item is not used. We address the problem of optimally choosing items for usage, so as to maximize overall user satisfaction over a period of time. First we show that the simple greedy heuristic of always choosing the item with the maximum current composite utility can be arbitrarily worse than the optimal. Second, we prove that even with just a single individual, determining the optimal strategy for choosing items is NP-hard. Third, we show that a simple modification to the greedy algorithm is a provably close approximation to the optimal strategy. Finally, we present an experimental study over real-world data collected from query logs to compare our algorithms.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {593–602},
numpages = {10},
keywords = {cyclic trends, social choice, fashion trends},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124368,
author = {Bhagat, Smriti and Goyal, Amit and Lakshmanan, Laks V.S.},
title = {Maximizing Product Adoption in Social Networks},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124368},
doi = {10.1145/2124295.2124368},
abstract = {One of the key objectives of viral marketing is to identify a small set of users in a social network, who when convinced to adopt a product will influence others in the network leading to a large number of adoptions in an expected sense. The seminal work of Kempe et al. [13] approaches this as the problem of influence maximization. This and other previous papers tacitly assume that a user who is influenced (or, informed) about a product necessarily adopts the product and encourages her friends to adopt it. However, an influenced user may not adopt the product herself, and yet form an opinion based on the experiences of her friends, and share this opinion with others. Furthermore, a user who adopts the product may not like the product and hence not encourage her friends to adopt it to the same extent as another user who adopted and liked the product. This is independent of the extent to which those friends are influenced by her. Previous works do not account for these phenomena.We argue that it is important to distinguish product adoption from influence. We propose a model that factors in a user's experience (or projected experience) with a product. We adapt the classical Linear Threshold (LT) propagation model by defining an objective function that explicitly captures product adoption, as opposed to influence. We show that under our model, adoption maximization is NP-hard and the objective function is monotone and submodular, thus admitting an approximation algorithm. We perform experiments on three real popular social networks and show that our model is able to distinguish between influence and adoption, and predict product adoption much more accurately than approaches based on the classical LT model.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {603–612},
numpages = {10},
keywords = {product adoption, viral marketing, influence},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124369,
author = {Weber, Ingmar and Ukkonen, Antti and Gionis, Aris},
title = {Answers, Not Links: Extracting Tips from Yahoo! Answers to Address How-to Web Queries},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124369},
doi = {10.1145/2124295.2124369},
abstract = {We investigate the problem of mining "tips" from Yahoo! Answers and displaying those tips in response to related web queries. Here, a "tip" is a short, concrete and self-contained bit of non-obvious advice such as "To zest a lime if you don't have a zester : use a cheese grater."First, we estimate the volume of web queries with "how-to" intent, which could be potentially addressed by a tip. Second, we analyze how to detect such queries automatically without solely relying on literal "how to *" patterns. Third, we describe how to derive potential tips automatically from Yahoo! Answers, and we develop machine-learning techniques to remove low-quality tips. Finally, we discuss how to match web queries with "how-to" intent to tips. We evaluate both the quality of these direct displays as well as the size of the query volume that can be addressed by serving tips.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {613–622},
numpages = {10},
keywords = {direct display, how-to queries, tips, web search},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124370,
author = {Yin, Peifeng and Luo, Ping and Wang, Min and Lee, Wang-Chien},
title = {A Straw Shows Which Way the Wind Blows: Ranking Potentially Popular Items from Early Votes},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124370},
doi = {10.1145/2124295.2124370},
abstract = {Prediction of popular items in online content sharing systems has recently attracted a lot of attention due to the tremendous need of users and its commercial values. Different from previous works that make prediction by fitting a popularity growth model, we tackle this problem by exploiting the latent conforming and maverick personalities of those who vote to assess the quality of on-line items. We argue that the former personality prompts a user to cast her vote conforming to the majority of the service community while on the contrary the later personality makes her vote different from the community. We thus propose a Conformer-Maverick (CM) model to simulate the voting process and use it to rank top-k potentially popular items based on the early votes they received. Through an extensive experimental evaluation, we validate our ideas and find that our proposed CM model achieves better performance than baseline solutions, especially for smaller k.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {623–632},
numpages = {10},
keywords = {generative model, popular ranking, conformer, maverick},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124371,
author = {Kucuktunc, Onur and Cambazoglu, B. Barla and Weber, Ingmar and Ferhatosmanoglu, Hakan},
title = {A Large-Scale Sentiment Analysis for Yahoo! Answers},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124371},
doi = {10.1145/2124295.2124371},
abstract = {Sentiment extraction from online web documents has recently been an active research topic due to its potential use in commercial applications. By sentiment analysis, we refer to the problem of assigning a quantitative positive/negative mood to a short bit of text. Most studies in this area are limited to the identification of sentiments and do not investigate the interplay between sentiments and other factors. In this work, we use a sentiment extraction tool to investigate the influence of factors such as gender, age, education level, the topic at hand, or even the time of the day on sentiments in the context of a large online question answering site. We start our analysis by looking at direct correlations, e.g., we observe more positive sentiments on weekends, very neutral ones in the Science &amp; Mathematics topic, a trend for younger people to express stronger sentiments, or people in military bases to ask the most neutral questions. We then extend this basic analysis by investigating how properties of the (asker, answerer) pair affect the sentiment present in the answer. Among other things, we observe a dependence on the pairing of some inferred attributes estimated by a user's ZIP code. We also show that the best answers differ in their sentiments from other answers, e.g., in the Business &amp; Finance topic, best answers tend to have a more neutral sentiment than other answers. Finally, we report results for the task of predicting the attitude that a question will provoke in answers. We believe that understanding factors influencing the mood of users is not only interesting from a sociological point of view, but also has applications in advertising, recommendation, and search.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {633–642},
numpages = {10},
keywords = {prediction, sentiment analysis, attitude, collaborative question answering, sentimentality},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124372,
author = {Vasconcelos, Marisa Affonso and Ricci, Saulo and Almeida, Jussara and Benevenuto, Fabr\'{\i}cio and Almeida, Virg\'{\i}lio},
title = {Tips, Dones and Todos: Uncovering User Profiles in Foursquare},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124372},
doi = {10.1145/2124295.2124372},
abstract = {Online Location Based Social Networks (LBSNs), which combine social network features with geographic information sharing, are becoming increasingly popular. One such application is Foursquare, which doubled its user population in less than six months. Among other features, Foursquare allows users to leave tips (i.e., reviews or recommendations) at specific venues as well as to give feedback on previously posted tips by adding them to their to-do lists or marking them as done. In this paper, we analyze how Foursquare users exploit these three features - tips, dones and to-dos - uncovering different behavior profiles. Our study reveals the existence of very active and influential users, some of which are famous businesses and brands, that seem engaged in posting tips at a large variety of venues while also receiving a great amount of user feedback on them. We also provide evidence of spamming, showing the existence of users that post tips whose contents are unrelated to the nature or domain of the venue where the tips were left.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {653–662},
numpages = {10},
keywords = {location based social networks, spamming, user behavior characterization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124373,
author = {Sun, Yizhou and Han, Jiawei and Aggarwal, Charu C. and Chawla, Nitesh V.},
title = {When Will It Happen? Relationship Prediction in Heterogeneous Information Networks},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124373},
doi = {10.1145/2124295.2124373},
abstract = {Link prediction, i.e., predicting links or interactions between objects in a network, is an important task in network analysis. Although the problem has attracted much attention recently, there are several challenges that have not been addressed so far. First, most existing studies focus only on link prediction in homogeneous networks, where all objects and links belong to the same type. However, in the real world, heterogeneous networks that consist of multi-typed objects and relationships are ubiquitous. Second, most current studies only concern the problem of whether a link will appear in the future but seldom pay attention to the problem of when it will happen. In this paper, we address both issues and study the problem of predicting when a certain relationship will happen in the scenario of heterogeneous networks. First, we extend the link prediction problem to the relationship prediction problem, by systematically defining both the target relation and the topological features, using a meta path-based approach. Then, we directly model the distribution of relationship building time with the use of the extracted topological features. The experiments on citation relationship prediction between authors on the DBLP network demonstrate the effectiveness of our methodology.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {663–672},
numpages = {10},
keywords = {relationship prediction, heterogeneous information networks},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124374,
author = {Kairam, Sanjay Ram and Wang, Dan J. and Leskovec, Jure},
title = {The Life and Death of Online Groups: Predicting Group Growth and Longevity},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124374},
doi = {10.1145/2124295.2124374},
abstract = {We pose a fundamental question in understanding how to identify and design successful communities: What factors predict whether a community will grow and survive in the long term? Social scientists have addressed this question extensively by analyzing offline groups which endeavor to attract new members, such as social movements, finding that new individuals are influenced strongly by their ties to members of the group. As a result, prior work on the growth of communities has treated growth primarily as a diffusion processes, leading to findings about group evolution which can be difficult to explain. The proliferation of online social networks and communities, however, has created new opportunities to study, at a large scale and with very fine resolution, the mechanisms which lead to the formation, growth, and demise of online groups.In this paper, we analyze data from several thousand online social networks built on the Ning platform with the goal of understanding the factors contributing to the growth and longevity of groups within these networks. Specifically, we investigate the role that two types of growth (growth through diffusion and growth by other means) play during a group's formative stages from the perspectives of both the individual member and the group. Applying these insights to a population of groups of different ages and sizes, we build a model to classify groups which will grow rapidly over the short-term and long-term. Our model achieves over 79% accuracy in predicting group growth over the following two months and over 78% accuracy in predictions over the following two years. We utilize a similar approach to predict which groups will die within a year. The results of our combined analysis provide insight into how both early non-diffusion growth and a complex set of network constraints appear to contribute to the initial and continued growth and success of groups within social networks. Finally we discuss implications of this work for the design, maintenance, and analysis of online communities.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {673–682},
numpages = {10},
keywords = {group formation, social networks, online communities, information diffusion},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124375,
author = {Lee, Chia-Jung and Croft, W. Bruce and Kim, Jin Young},
title = {Evaluating Search in Personal Social Media Collections},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124375},
doi = {10.1145/2124295.2124375},
abstract = {The prevalence of social media applications is generating potentially large personal archives of posts, tweets, and other communications. The existence of these archives creates a need for search tools, which can be seen as an extension of current desktop search services. Little is currently known about the best search techniques for personal archives of social data, because of the difficulty of creating test collections. In this paper, we describe how test collections for personal social data can be created by using games to collect queries. We then compare a range of retrieval models that exploit the semi-structured nature of social data. Our results show that a mixture of language models with field distribution estimation can be effective for this type of data, with certain fields, such as the name of the poster, being particularly important. We also analyze the properties of the queries that were generated by users with two versions of the games.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {683–692},
numpages = {10},
keywords = {desktop search, twitter, personal social media collections, facebook, search evaluation, semi-structured document},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124376,
author = {Saha, Ankan and Sindhwani, Vikas},
title = {Learning Evolving and Emerging Topics in Social Media: A Dynamic Nmf Approach with Temporal Regularization},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124376},
doi = {10.1145/2124295.2124376},
abstract = {As massive repositories of real-time human commentary, social media platforms have arguably evolved far beyond passive facilitation of online social interactions. Rapid analysis of information content in online social media streams (news articles, blogs,tweets etc.) is the need of the hour as it allows business and government bodies to understand public opinion about products and policies. In most of these settings, data points appear as a stream of high dimensional feature vectors. Guided by real-world industrial deployment scenarios, we revisit the problem of online learning of topics from streaming social media content. On one hand, the topics need to be dynamically adapted to the statistics of incoming datapoints, and on the other hand, early detection of rising new trends is important in many applications. We propose an online nonnegative matrix factorizations framework to capture the evolution and emergence of themes in unstructured text under a novel temporal regularization framework. We develop scalable optimization algorithms for our framework, propose a new set of evaluation metrics, and report promising empirical results on traditional TDT tasks as well as streaming Twitter data. Our system is able to rapidly capture emerging themes, track existing topics over time while maintaining temporal consistency and continuity in user views, and can be explicitly configured to bound the amount of information being presented to the user.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {693–702},
numpages = {10},
keywords = {topic models, dictionary learning, nmf, time series analysis},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124378,
author = {Anderson, Ashton and Huttenlocher, Daniel and Kleinberg, Jon and Leskovec, Jure},
title = {Effects of User Similarity in Social Media},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124378},
doi = {10.1145/2124295.2124378},
abstract = {There are many settings in which users of a social media application provide evaluations of one another. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. Earlier work has studied how the relative status between two users - that is, their comparative levels of status in the group - affects the types of evaluations that one user gives to another.Here we study how similarity in the characteristics of two users can affect the evaluation one user provides of another. We analyze this issue under a range of natural similarity measures, showing how the interaction of similarity and status can produce strong effects. Among other consequences, we find that evaluations are less status-driven when users are more similar to each other; and we use effects based on similarity to provide a plausible mechanism for a complex phenomenon observed in studies of user evaluation, that evaluations are particularly low among users of roughly equal status.Our work has natural applications to the prediction of evaluation outcomes based on user characteristics, and the use of similarity information makes possible a novel application that we introduce here - to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {703–712},
numpages = {10},
keywords = {user similarity, user-to-user evaluations, ballot-blind prediction, status},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124379,
author = {Panigrahy, Rina and Najork, Marc and Xie, Yinglian},
title = {How User Behavior is Related to Social Affinity},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124379},
doi = {10.1145/2124295.2124379},
abstract = {Previous research has suggested that people who are in the same social circle exhibit similar behaviors and tastes. The rise of social networks gives us insights into the social circles of web users, and recommendation services (including search engines, advertisement engines, and collaborative filtering engines) provide a motivation to adapt recommendations to the interests of the audience. An important primitive for supporting these applications is the ability to quantify how connected two users are in a social network. The shortest-path distance between a pair of users is an obvious candidate measure. This paper introduces a new measure of "affinity" in social networks that takes into account not only the distance between two users, but also the number of edge-disjoint paths between them, i.e. the "robustness" of their connection. Our measure is based on a sketch-based approach, and affinity queries can be answered extremely efficiently (at the expense of a one-time offline sketch computation). We compare this affinity measure against the "approximate shortest-path distance", a sketch-based distance measure with similar efficiency characteristics. Our empirical study is based on a Hotmail email exchange graph combined with demographic information and Bing query history, and a Twitter mention-graph together with the text of the underlying tweets. We found that users who are close to each other - either in terms of distance or affinity - have a higher similarity in terms of demographics, queries, and tweets.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {713–722},
numpages = {10},
keywords = {affinity, distance, social networks, sketching, influence},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124380,
author = {Sadilek, Adam and Kautz, Henry and Bigham, Jeffrey P.},
title = {Finding Your Friends and Following Them to Where You Are},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124380},
doi = {10.1145/2124295.2124380},
abstract = {Location plays an essential role in our lives, bridging our online and offline worlds. This paper explores the interplay between people's location, interactions, and their social ties within a large real-world dataset. We present and evaluate Flap, a system that solves two intimately related tasks: link and location prediction in online social networks. For link prediction, Flap infers social ties by considering patterns in friendship formation, the content of people's messages, and user location. We show that while each component is a weak predictor of friendship alone, combining them results in a strong model, accurately identifying the majority of friendships. For location prediction, Flap implements a scalable probabilistic model of human mobility, where we treat users with known GPS positions as noisy sensors of the location of their friends. We explore supervised and unsupervised learning scenarios, and focus on the efficiency of both learning and inference. We evaluate Flap on a large sample of highly active users from two distinct geographical areas and show that it (1) reconstructs the entire friendship graph with high accuracy even when no edges are given; and (2) infers people's fine-grained location, even when they keep their data private and we can only access the location of their friends. Our models significantly outperform current comparable approaches to either task.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {723–732},
numpages = {10},
keywords = {visualization, graphical models, social networks, machine learning, location modeling, link prediction},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124381,
author = {Singer, Yaron},
title = {How to Win Friends and Influence People, Truthfully: Influence Maximization Mechanisms for Social Networks},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124381},
doi = {10.1145/2124295.2124381},
abstract = {Throughout the past decade there has been extensive research on algorithmic and data mining techniques for solving the problem of influence maximization in social networks: if one can incentivize a subset of individuals to become early adopters of a new technology, which subset should be selected so that the word-of-mouth effect in the social network is maximized? Despite the progress in modeling and techniques, the incomplete information aspect of the problem has been largely overlooked. While data can often provide the network structure and influence patterns may be observable, the inherent cost individuals have to become early adopters is difficult to extract.In this paper we introduce mechanisms that elicit individuals' costs while providing desirable approximation guarantees in some of the most well-studied models of social network influence. We follow the mechanism design framework which advocates for allocation and payment schemes that incentivize individuals to report their true information. We also performed experiments using the Mechanical Turk platform and social network data to provide evidence of the framework's effectiveness in practice.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {733–742},
numpages = {10},
keywords = {influence maximization, mechanism design, social networks, viral marketing},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124382,
author = {Tang, Jie and Lou, Tiancheng and Kleinberg, Jon},
title = {Inferring Social Ties across Heterogenous Networks},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124382},
doi = {10.1145/2124295.2124382},
abstract = {It is well known that different types of social ties have essentially different influence on people. However, users in online social networks rarely categorize their contacts into "family", "colleagues", or "classmates". While a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of inferring social ties over multiple heterogeneous networks. In this work, we develop a framework for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of inferring the type of social relationships in a target network by borrowing knowledge from a different source network. Our empirical study on five different genres of networks validates the effectiveness of the proposed framework. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, the proposed framework is able to obtain an F1-score of 90% (8-28% improvements over alternative methods) for inferring manager-subordinate relationships in an enterprise email network.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {743–752},
numpages = {10},
keywords = {social influence analysis, predictive model, inferring social ties, social network},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124384,
author = {Mason, Hilary},
title = {The Secret Life of Social Links},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124384},
doi = {10.1145/2124295.2124384},
abstract = {The social web is a messy place! At bitly, we see hundreds of millions of shares and clicks per day--clicks that contain all sorts of wonderful content from lolcats to spacecraft launches. I'll discuss our philosophy, tools, and techniques for looking at the data, and new research opportunities that weren't possible before.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {753–754},
numpages = {2},
keywords = {social content, social web},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124386,
author = {Chiarandini, Luca},
title = {Exploration and Discovery of User-Generated Content in Large Information Spaces},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124386},
doi = {10.1145/2124295.2124386},
abstract = {The accumulation of large collections of social media data poses new challenges for the design of exploratory experiences, such as when a user browses through a collection to discover content (e.g. exploring photo collections, network of friends, etc). Cardinality and characteristics of the set, together with volatility of the information, resulting from fast and continuous creation, deletion and updating of entries, trigger novel research questions. In this context, we plan to investigate and contribute to the data analysis, and user interface design of exploratory experiences. The proposed approach is an iterative process where analysis and design phases are performed in cycles. The long-term vision is to understand the underlying reasoning in order to be able to automatically replicate it.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {755–756},
numpages = {2},
keywords = {human computer interaction, social media, data mining},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124387,
author = {Dave, Kushal S.},
title = {Computational Advertising: Leveraging User Interaction &amp; Contextual Factors for Improved Ad Relevance &amp; Targeting},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124387},
doi = {10.1145/2124295.2124387},
abstract = {Computational advertising refers to finding the most relevant ads matching a particular context on the web. The core problem attacked in computational advertising CA is of the match making between the ads and the context. My research work aims at leveraging various user interaction, ad and advertiser related information and contextual information for improving the relevance, ranking and targeting of ads. The research work focuses on the identification of various factors that contribute in retrieving and ranking the most relevant set of ads that match best with the context. Specifically, information associated with the user, publisher and advertiser is leveraged for this purpose.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {757–758},
numpages = {2},
keywords = {viral marketing, sponsored search, computational advertising, contextual advertising},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124388,
author = {Thompson, Brian},
title = {The Early Bird Gets the Buzz: Detecting Anomalies and Emerging Trends in Information Networks},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124388},
doi = {10.1145/2124295.2124388},
abstract = {In this work we propose a novel approach to anomaly detection in streaming communication data. We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time Data Streams) model. We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior. Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity. Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to information retrieval in data streams and information networks.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {759–760},
numpages = {2},
keywords = {streaming time series models, anomaly detection, graph algorithms, renewal theory, time-evolving graphs, information networks, communication networks},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124389,
author = {Santos-Neto, Elizeu},
title = {Characterizing and Harnessing Peer-Production of Information in Social Tagging Systems},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124389},
doi = {10.1145/2124295.2124389},
abstract = {Assessing the value of individual users' contributions in peer-production systems is paramount to the design of mechanisms that support collaboration and improve users' experience. For instance, to incentivize contributions, file sharing systems based on the BitTorrent protocol equate value with volume of contributed content and use a prioritization mechanism to reward users who contribute more. This approach and similar techniques used in resource sharing systems rely on the fact that the physical resources shared among users are easily quantifiable.In contrast, information-sharing systems, like social tagging systems, lack the notion of a physical resource unit (e.g., content size, bandwidth) that facilitates the task of evaluating user contributions. For this reason, the issue of estimating the value of user contributions in information sharing systems remains largely unexplored. This paper outlines a research project to tackle the problem of assessing the value of contributions in social tagging systems.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {761–762},
numpages = {2},
keywords = {peer-production, social, systems, tagging, information value},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124391,
author = {Agichtein, Eugene and Gabrilovich, Evgeniy},
title = {Mining, Searching and Exploiting Collaboratively Generated Content on the Web},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124391},
doi = {10.1145/2124295.2124391},
abstract = {Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content.The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation.However, the quality and comprehensiveness of collaboratively created content vary significantly, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial.The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {763–764},
numpages = {2},
keywords = {collaboratively-generated content},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124392,
author = {Shah, Chirag},
title = {Collaborative Information Seeking: Understanding Users, Systems, and Content},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124392},
doi = {10.1145/2124295.2124392},
abstract = {The course will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. Traditionally, IR is considered an individual pursuit, and not surprisingly, the majority of tools, techniques, and models developed for addressing information need, retrieval, and usage have focused on single users. The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past. This course will introduce such works to the students, with an emphasis on understanding models and systems that support collaborative search or browsing. In addition, the course will provide samples of data collected through several experiments to demonstrate various mining and analysis techniques. Specifically, the course will (1) outline the research and latest developments in the field of collaborative IR, (2) list the challenges for designing and evaluating collaborative IR systems, and (3) show how traditional single user IR models and systems could be mapped to those for CIS. This will be achieved through introduction to appropriate literature, algorithms and interfaces that facilitate CIS, and methodologies for studying and evaluating them. Thus, the course will offer a balance between theoretical and practical elements of CIS.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {765–766},
numpages = {2},
keywords = {collaborative information seeking},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124393,
author = {Li, Hang and Xu, Jun},
title = {Machine Learning for Query-Document Matching in Search},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124393},
doi = {10.1145/2124295.2124393},
abstract = {In web search, relevance is one of the most important factors to meet users' satisfaction, and the success of a web search engine heavily depends on its performance on relevance. It has been observed that many hard cases in search relevance are due to term mismatch between query and documnt (e.g., query 'ny times' does not match well with document only containing 'new york times'), and thus it is not exaggerated to say that dealing with mismatch between query and document is one of the most critical research problems in web search. Recently researchers have spent significant effort to address the grand challenge. The major approach is to conduct more query and document understanding, and perform better matching between enriched query and document representations. With the availability of large amount of log data and advanced machine learning techniques, this becomes more feasible and significant progress has been made recently.In this tutorial, we will give a systematic and detailed presentation on newly developed machine learning technologies for query document matching in search. We will focus on the fundamental problems, as well as the novel solutions for query document matching at word form level, word sense level, topic level, and structure level. We will talk about novel technologies about query spelling error correction [3, 13], query rewriting [1, 4, 6, 7], query classification [2], topic modeling of documents [5, 9], query document matching [8, 10, 11, 12], and query document-title translation. The ideas and solutions introduced in this tutorial may motivate industrial practitioners to turn the research fruits into product reality. The summary of the state-of-the-art methods and the discussions on the technical issues in this tutorial may stimulate academic researchers to find new research directions and solutions.Matching between query and document is not limited to search, and similar problems can be observed at online advertisement, recommendation system, and other applications, as matching between objects from two spaces. The technologies we introduce can be generalized into more general machine learning techniques, which we call learning to match.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {767–768},
numpages = {2},
keywords = {query-document matching, web search, machine learning},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124395,
author = {Macdonald, Craig and Wang, Jun and Clarke, Charles},
title = {2nd International Workshop on Diversity in Document Retrieval (DDR 2012)},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124395},
doi = {10.1145/2124295.2124395},
abstract = {When an ambiguous query is received, a sensible approach is for the information retrieval (IR) system to diversify the results retrieved for this query, in the hope that at least one of the interpretations of the query intent will satisfy the user. Diversity is an increasingly important topic, of interest to both academic researchers (such as participants in the TREC Web and Blog track diversity tasks, or the NTCIR INTENT task), as well as to search engines professionals. In the 2nd edition of the Diversity in Document Retrieval workshop (DDR 2012), we solicited submissions both on approaches and models for diversity, the evaluation of diverse search results, and on applications of diverse search results. This workshop builds upon a successful 1st edition of DDR which was held at ECIR 2011 in Dublin, Ireland.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {769–770},
numpages = {2},
keywords = {information retrieval, diversity, search},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124396,
author = {Serdyukov, Pavel and Craswell, Nick and Dupret, Georges},
title = {WSCD 2012: Workshop on Web Search Click Data 2012},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124396},
doi = {10.1145/2124295.2124396},
abstract = {WSCD2012 is the second workshop on Web Search Click Data, following WSCD2009. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This workshop comes with a new click dataset based on click logs and an accompanying challenge to predict the relevance of documents based on clicks.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {771–772},
numpages = {2},
keywords = {click data, web search},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

