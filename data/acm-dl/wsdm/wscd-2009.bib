@inproceedings{10.1145/1507509.1507510,
author = {Brenes, David J. and Gayo-Avello, Daniel and P\'{e}rez-Gonz\'{a}lez, Kilian},
title = {Survey and Evaluation of Query Intent Detection Methods},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507510},
doi = {10.1145/1507509.1507510},
abstract = {User interactions with search engines reveal three main underlying intents, namely navigational, informational, and transactional. By providing more accurate results depending on such query intents the performance of search engines can be greatly improved. Therefore, query classification has been an active research topic for the last years. However, while query topic classification has deserved a specific bakeoff, no evaluation campaign has been devoted to the study of automatic query intent detection. In this paper some of the available query intent detection techniques are reviewed, an evaluation framework is proposed, and it is used to compare those methods in order to shed light on their relative performance and drawbacks. As it will be shown, manually prepared gold-standard files are much needed, and traditional pooling is not the most feasible evaluation method. In addition to this, future lines of work in both query intent detection and its evaluation are proposed.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {1–7},
numpages = {7},
keywords = {MSN query log, web search behavior, evaluation, query intent detection, click-through data},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507511,
author = {Bendersky, Michael and Croft, W. Bruce},
title = {Analysis of Long Queries in a Large Scale Search Log},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507511},
doi = {10.1145/1507509.1507511},
abstract = {We propose to use the search log to study long queries, in order to understand the types of information needs that are behind them, and to design techniques to improve search effectiveness when they are used. Long queries arise in many different applications, such as CQA (community-based question answering) and literature search, and they have been studied to some extent using TREC data. They are also, however, quite common in web search, as can be seen by looking at the distribution of query lengths in a large scale search log.In this paper we analyze the long queries in the search log with the aim of identifying the characteristics of the most commonly occurring types of queries, and the issues involved with using them effectively in a search engine. In addition, we propose a simple yet effective method for evaluating the performance of the queries in the search log using a combination of the click data in the search log with the existing TREC corpora.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {8–14},
numpages = {7},
keywords = {click data, web search, long queries},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507512,
author = {Duskin, Omer and Feitelson, Dror G.},
title = {Distinguishing Humans from Robots in Web Search Logs: Preliminary Results Using Query Rates and Intervals},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507512},
doi = {10.1145/1507509.1507512},
abstract = {The workload on web search engines is actually multiclass, being derived from the activities of both human users and automated robots. It is important to distinguish between these two classes in order to reliably characterize human web search behavior, and to study the effect of robot activity. We suggest an approach based on a multi-dimensional characterization of search sessions, and take first steps towards implementing it by studying the interaction between the query submittal rate and the minimal interval of time between different queries.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {15–19},
numpages = {5},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507513,
author = {Kim, Kye-Hyeon and Choi, Seungjin},
title = {Incremental Learning to Rank with Partially-Labeled Data},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507513},
doi = {10.1145/1507509.1507513},
abstract = {In this paper we present a semi-supervised learning method for a problem of learning to rank where we exploit Markov random walks and graph regularization in order to incorporate not only "labeled" web pages but also plenty of "un-labeled" web pages (click logs of which are not given) into learning a ranking function. In order to cope with scalability which existing semi-supervised learning methods suffer from, we develop a scalable and incremental method for semi-supervised learning to rank. In the graph regularization framework, we first determine features which well reflects data manifold and then make use of them to train a linear ranking function. We introduce a matrix-fee technique where we compute the eigenvectors of a huge similarity matrix without constructing the matrix itself. Then we present an incremental algorithm to learn a linear ranking function using features determined by projecting data onto the eigenvectors of the similarity matrix, which can be applied to a task of web-scale ranking. We evaluate our method on Live Search query log, showing that search performance is much improved when Live Search yields unsatisfactory search results.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {20–27},
numpages = {8},
keywords = {semi-supervised learning, learning to rank, information retrieval, web search, incremental learning, click-through data},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507514,
author = {Smith, G. and Brailsford, T. and Donner, C. and Hooijmaijers, D. and Truran, M. and Goulding, J. and Ashman, H.},
title = {Generating Unambiguous URL Clusters from Web Search},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507514},
doi = {10.1145/1507509.1507514},
abstract = {This paper reports on the generation of unambiguous clusters of URLs from clickthrough data from the MSN search query log excerpt (the RFP 2006 dataset). Selections (clickthroughs) by a single user from a single query can be assumed to have some mutual semantic relevance, and the URLs coselected in this way can be aggregated to form single-sense clusters. When the graphs for a single term separate into distinct clusters, the semantics of the distinct clusters can be interpreted as disambiguated aggregations of URLs. This principle had been tested on smaller and more constrained datasets previously, and this paper reports on findings from applying a method based on the principle to the RFP 2006 dataset.This paper evaluates the proposed coselection method for generating single-sense clusters against two other methods, with varying parameters. The evaluation is done both with a human evaluation to determine the quality of the clusters generated by the different methods, and by a simple "edit distance" analysis to determine the content difference of the methods.The main questions addressed are i) whether it is feasible to generate single-sense / sense-coherent clusters, and ii) whether, in a closed world, it would be feasible to discover ambiguous terms. The experimentation showed that sense-coherent clusters were found and further indicated that ambiguous terms could be detected from observing small overlap between large clusters.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {28–34},
numpages = {7},
keywords = {disambiguation, web search, clickthrough},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507515,
author = {Bar-Ilan, Judit and Zhu, Zheng and Levene, Mark},
title = {Topic-Specific Analysis of Search Queries},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507515},
doi = {10.1145/1507509.1507515},
abstract = {The analysis of search engine logs is important in order to understand how users interact with a search engine. Conventional analysis of search engine log data looks at various metrics such as query and session length aggregated over the full data set. Here we segment the data according to a top-level ontology of web search and compute the metrics on a topic by topic basis. Our results show that although for a given metric, such as query length, the statistics of most classes are similar to the aggregate statistic; there are usually some outlier classes which exhibit deviant behavior.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {35–42},
numpages = {8},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507516,
author = {Sydow, Marcin and Bonchi, Francesco and Castillo, Carlos and Donato, Debora},
title = {Optimising Topical Query Decomposition},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507516},
doi = {10.1145/1507509.1507516},
abstract = {Topical query decomposition (TQD) is a paradigm recently introduced in [1], which, given a query, returns to the user a set of queries that cover the answer set of the original query. The TQD problem was studied as a variant of the set-cover problem and solved by means of a greedy algorithm.This paper aims to strengthen the original formulation by introducing a new global objective function, and thus formalising the problem as a combinatorial optimisation one. Such a reformulation defines a common framework allowing a formal evaluation and comparison of different approaches to TQD. We apply simulated annealing, a sub-optimal meta-heuristic, to the problem of topical query decomposition and we show, through a large experimentation on a data sample extracted from an actual query log, that such meta-heuristic achieves better results than the greedy algorithm.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {43–47},
numpages = {5},
keywords = {query recommendation, objective function, query logs, query decomposition, simulated annealing},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507517,
author = {Baraglia, Ranieri and Cacheda, Fidel and Carneiro, Victor and Formoso, Vreixo and Perego, Raffaele and Silvestri, Fabrizio},
title = {Search Shortcuts Using Click-through Data},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507517},
doi = {10.1145/1507509.1507517},
abstract = {Major Web Search Engines take as a common practice to provide Suggestions to users in order to enhance their search experience. Such suggestions have normally the form of queries that are, to some extent, "similar" to the queries already submitted by the same or related users. The final aim of query suggestions is typically to help users to satisfy their information needs more quickly. In this paper we face this problem from a somewhat different perspective, and we propose a new query suggestion model based on Search Shortcuts, that consist in finding and proposing to the user "Successful" queries that allowed, in the past, several users to satisfy their information needs. This model differs from traditional query suggestion approaches, and allows the evaluation to be performed effectively by exploiting actual user sessions from the Microsoft 2006 RFP dataset. We evaluate several algorithms applied to this problem, both traditional Collaborative Filtering techniques and ad-hoc solutions, and report on preliminary results achieved.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {48–55},
numpages = {8},
keywords = {model, search shortcut, evaluation},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507518,
author = {Boldi, Paolo and Bonchi, Francesco and Castillo, Carlos and Donato, Debora and Vigna, Sebastiano},
title = {Query Suggestions Using Query-Flow Graphs},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507518},
doi = {10.1145/1507509.1507518},
abstract = {The query-flow graph [Boldi et al., CIKM 2008] is an aggregated representation of the latent querying behavior contained in a query log. Intuitively, in the query-flow graph a directed edge from query qi to query qj means that the two queries are likely to be part of the same search mission. Any path over the query-flow graph may be seen as a possible search task, whose likelihood is given by the strength of the edges along the path. An edge (qi, qj) is also labelled with some information: e.g., the probability that user moves from qi to qj, or the type of the transition, for instance, the fact that qj is a specialization of qi.In this paper we propose, and experimentally study, query recommendations based on short random walks on the query-flow graph. Our experiments show that these methods can match in precision, and often improve, recommendations based on query-click graphs, without using users' clicks. Our experiments also show that it is important to consider transition-type labels on edges for having good quality recommendations.Finally, one feature that we had in mind while devising our methods was that of providing diverse sets of recommendations: the experimentation that we conducted provides encouraging results in this sense.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {56–63},
numpages = {8},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507519,
author = {van der Heijden, Maarten and Hinne, Max and Kraaij, Wessel and Verberne, Suzan and van der Weide, Theo},
title = {Using Query Logs and Click Data to Create Improved Document Descriptions},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507519},
doi = {10.1145/1507509.1507519},
abstract = {Logfiles of search engines are a promising resource for data mining, since they provide raw data associated to users and web documents. In this paper we focus on the latter aspect and explore how the information in logfiles could be used to improve document descriptions. A pilot experiment demonstrated that document descriptors extracted from the queries that are associated with documents by clicks provide useful semantic information about documents in addition to document descriptors extracted from the full text of the web pages.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {64–67},
numpages = {4},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507520,
author = {Strohmaier, Markus and Kr\"{o}ll, Mark and K\"{o}rner, Christian},
title = {Intentional Query Suggestion: Making User Goals More Explicit during Search},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507520},
doi = {10.1145/1507509.1507520},
abstract = {The degree to which users' make their search intent explicit can be assumed to represent an upper bound on the level of service that search engines can provide. In a departure from traditional query expansion mechanisms, we introduce Intentional Query Suggestion as a novel idea that is attempting to make users' intent more explicit during search. In this paper, we present a prototypical algorithm for Intentional Query Suggestion and we discuss corresponding data from comparative experiments with traditional query suggestion mechanisms. Our preliminary results indicate that intentional query suggestions 1) diversify search result sets (i.e. it reduces result set overlap) and 2) have the potential to yield higher click-through rates than traditional query suggestions.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {68–74},
numpages = {7},
keywords = {query suggestion, user intent},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507521,
author = {Macdonald, Craig and Ounis, Iadh},
title = {Usefulness of Quality Click-through Data for Training},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507521},
doi = {10.1145/1507509.1507521},
abstract = {Modern Information Retrieval (IR) systems often employ document weighting models with many parameters that require to be appropriately set for effective retrieval performance. To obtain these parameter settings, quality training is usually required, where assessors have manually labelled the relevance of retrieved items for many queries. In this work, we examine the usefulness of high-quality click-through data for training an IR system, on searching the .gov vertical domain of the Web. We find that, compared to training using relevance judgements created using human assessors, the click-through trained settings are as good and occasionally better.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {75–79},
numpages = {5},
keywords = {queries &amp; click-through, training, web},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507522,
author = {Kamps, Jaap and Koolen, Marijn and Trotman, Andrew},
title = {Comparative Analysis of Clicks and Judgments for IR Evaluation},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507522},
doi = {10.1145/1507509.1507522},
abstract = {Queries and click-through data taken from search engine transaction logs is an attractive alternative to traditional test collections, due to its volume and the direct relation to end-user querying. The overall aim of this paper is to answer the question: How does click-through data differ from explicit human relevance judgments in information retrieval evaluation? We compare a traditional test collection with manual judgments to transaction log based test collections---by using queries as topics and subsequent clicks as pseudo-relevance judgments for the clicked results.Specifically, we investigate the following two research questions: Firstly, are there significant differences between clicks and relevance judgments. Earlier research suggests that although clicks and explicit judgments show reasonable agreement, clicks are different from static absolute relevance judgments. Secondly, are there significant differences between system ranking based on clicks and based on relevance judgments? This is an open question, but earlier research suggests that comparative evaluation in terms of system ranking is remarkably robust.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {80–87},
numpages = {8},
keywords = {web information retrieval, Wikipedia, transaction log analysis},
location = {Barcelona, Spain},
series = {WSCD '09}
}

@inproceedings{10.1145/1507509.1507523,
author = {Guo, Fan and Li, Lei and Faloutsos, Christos},
title = {Tailoring Click Models to User Goals},
year = {2009},
isbn = {9781605584348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507509.1507523},
doi = {10.1145/1507509.1507523},
abstract = {Click models provide a principled way of understanding user interaction with web search results in a query session and a statistical tool for leveraging search engine click logs to analyze and improve user experience. An important component in all existing click models is the user behavior assumption -- how users scan, examine and click web documents listed in the result page. Usually the average user behavior pattern is summarized in a small set of global parameters. Can we fit multiple models with different user behavior parameters on a click data set? A previous study showed that the mixture modeling approach did not lead to better performance despite extra computational cost.In this paper, we present how to tailor click models to user goals in web search through query term classification. We demonstrate that better predicative power could be achieved by fitting two click models for navigational queries and informational queries respectively, as evidenced by the likelihood and perplexity evaluation results on a subset of the MSN 2006 RFP data which consists of 121,179 distinct query terms and over 2.8 million query sessions. We also propose search relevance score (SRS) as a flexible evaluation metric of search engine performance. This metric can be derived as summary statistics under any click model, and is applicable to a single query session, a particular query term and the search engine overall.},
booktitle = {Proceedings of the 2009 Workshop on Web Search Click Data},
pages = {88–92},
numpages = {5},
keywords = {click model, user behavior, web search},
location = {Barcelona, Spain},
series = {WSCD '09}
}

