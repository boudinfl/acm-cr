@inproceedings{10.1145/1341531.1341535,
author = {Pandey, Sandeep and Olston, Christopher},
title = {Crawl Ordering by Search Impact},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341535},
doi = {10.1145/1341531.1341535},
abstract = {We study how to prioritize the fetching of new pages under the objective of maximizing the quality of search results. In particular, our objective is to fetch new pages that have the most impact, where the impact of a page is equal to the number of times the page appears in the top K search results for queries, for some constant K, e.g., K = 10. Since the impact of a page depends on its relevance score for queries, which in turn depends on the page content, the main difficulty lies in estimating the impact of the page before actually fetching it. Hence, impact must be estimated based on the limited information that is available prior to fetching page content, e.g., the URL string, number of in-links, referring anchortextWe formally characterize this problem and study its hardness. We leverage our formalism to design a new impact-driven crawling policy, and demonstrate its effectiveness using real world data. Our technique ensures that the crawler acquires content relevant to "tail topics" that are obscure but of interest to some users, rather than just redundantly accumulating content on popular topics.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {3–14},
numpages = {12},
keywords = {web crawling, crawl ordering, impact-driven crawling},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341537,
author = {Chierichetti, Flavio and Lattanzi, Silvio and Mari, Federico and Panconesi, Alessandro},
title = {On Placing Skips Optimally in Expectation},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341537},
doi = {10.1145/1341531.1341537},
abstract = {We study the problem of optimal skip placement in an inverted list. Assuming the query distribution to be known in advance, we formally prove that an optimal skip placement can be computed quite efficiently. Our best algorithm runs in time O (n log n), n being the length of the list.The placement is optimal in the sense that it minimizes the expected time to process a query. Our theoretical results are matched by experiments with a real corpus, showing that substantial savings can be obtained with respect to the traditional skip placement strategy, that of placing consecutive skips, each spanning √n many locations.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {15–24},
numpages = {10},
keywords = {skips, probabilistic analysis, inverted index},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341538,
author = {Goyal, Navin and Lifshits, Yury and Sch\"{u}tze, Hinrich},
title = {Disorder Inequality: A Combinatorial Approach to Nearest Neighbor Search},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341538},
doi = {10.1145/1341531.1341538},
abstract = {We say that an algorithm for nearest neighbor search is combinatorial if only direct comparisons between two pairwise similarity values are allowed. Combinatorial algorithms for nearest neighbor search have two important advantages: (1) they do not map similarity values to artificial distance values and do not use the triangle inequality for the latter, and (2) they work for arbitrarily complicated data representations and similarity functions.In this paper we introduce a special property of the similarity function on a set S that leads to efficient combinatorial algorithms for S. The disorder constant D(S) of a set S is defined to ensure the following inequality: if x is the a'th most similar object to z and y is the b'th most similar object to z, then x is among the D(S) (a + b) most similar objects to y.Assuming that disorder is small we present the first two known combinatorial algorithms for nearest neighbors whose query time has logarithmic dependence on the size of S. The first one, called Ranwalk, is a randomized zero-error algorithm that always returns the exact nearest neighbor. It uses space quadratic in the input size in preprocessing, but is very efficient in query processing. The second algorithm, called Arwalk, uses near-linear space. It uses random choices in preprocessing, but the query processing is essentially deterministic. For an arbitrary query q, there is only a small probability that the chosen data structure does not support qFinally, we show that for the Reuters corpus average disorder is indeed quite small and that Ranwalk efficiently computes the nearest neighbor in most cases.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {25–32},
numpages = {8},
keywords = {random walk, disorder dimension, disorder inequality, randomized algorithms, disorder constant, similarity search, nearest neighbor search, proximity search},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341539,
author = {Ben-Yitzhak, Ori and Golbandi, Nadav and Har'El, Nadav and Lempel, Ronny and Neumann, Andreas and Ofek-Koifman, Shila and Sheinwald, Dafna and Shekita, Eugene and Sznajder, Benjamin and Yogev, Sivan},
title = {Beyond Basic Faceted Search},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341539},
doi = {10.1145/1341531.1341539},
abstract = {This paper extends traditional faceted search to support richer information discovery tasks over more complex data models. Our first extension adds exible, dynamic business intelligence aggregations to the faceted application, enabling users to gain insight into their data that is far richer than just knowing the quantities of documents belonging to each facet. We see this capability as a step toward bringing OLAP capabilities, traditionally supported by databases over relational data, to the domain of free-text queries over metadata-rich content. Our second extension shows how one can efficiently extend a faceted search engine to support correlated facets - a more complex information model in which the values associated with a document across multiple facets are not independent. We show that by reducing the problem to a recently solved tree-indexing scenario, data with correlated facets can be efficiently indexed and retrieved},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {33–44},
numpages = {12},
keywords = {business intelligence, search engines, multifaceted search},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341540,
author = {Mei, Qiaozhu and Church, Kenneth},
title = {Entropy of Search Logs: How Hard is Search? With Personalization? With Backoff?},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341540},
doi = {10.1145/1341531.1341540},
abstract = {How many pages are there on the Web? 5B? 20B? More? Less? Big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. Language modeling techniques are applied to MSN's search logs to estimate entropy. The perplexity is surprisingly small: millions, not billions.Entropy is a powerful tool for sizing challenges and opportunities. How hard is search? How hard are query suggestion mechanisms like auto-complete? How much does personalization help? All these difficult questions can be answered by estimation of entropy from search logs.What is the potential opportunity for personalization? In this paper, we propose a new way to personalize search, personalization with backoff. If we have relevant data for a particular user, we should use it. But if we don't, back off to larger and larger classes of similar users. As a proof of concept, we use the first few bytes of the IP address to define classes. The coefficients of each backoff class are estimated with an EM algorithm. Ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {45–54},
numpages = {10},
keywords = {personalization with backoff, search log, entropy, demographics, search difficulty},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341542,
author = {Elsas, Jonathan L. and Carvalho, Vitor R. and Carbonell, Jaime G.},
title = {Fast Learning of Document Ranking Functions with the Committee Perceptron},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341542},
doi = {10.1145/1341531.1341542},
abstract = {This paper presents a new variant of the perceptron algorithm using selective committee averaging (or voting). We apply this agorithm to the problem of learning ranking functions for document retrieval, known as the "Learning to Rank" problem. Most previous algorithms proposed to address this problem focus on minimizing the number of misranked document pairs in the training set. The committee perceptron algorithm improves upon existing solutions by biasing the final solution towards maximizing an arbitrary rank-based performance metrics. This method performs comparably or better than two state-of-the-art rank learning algorithms, and also provides significant training time improvements over those methods, showing over a 45-fold reduction in training time compared to ranking SVM},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {55–64},
numpages = {10},
keywords = {perceptron algorithm, learning to rank, document retrieval},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341543,
author = {Meiss, Mark R. and Menczer, Filippo and Fortunato, Santo and Flammini, Alessandro and Vespignani, Alessandro},
title = {Ranking Web Sites with Real User Traffic},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341543},
doi = {10.1145/1341531.1341543},
abstract = {We analyze the traffic-weighted Web host graph obtained from a large sample of real Web users over about seven months. A number of interesting structural properties are revealed by this complex dynamic network, some in line with the well-studied boolean link host graph and others pointing to important differences. We find that while search is directly involved in a surprisingly small fraction of user clicks, it leads to a much larger fraction of all sites visited. The temporal traffic patterns display strong regularities, with a large portion of future requests being statistically predictable by past ones. Given the importance of topological measures such as PageRank in modeling user navigation, as well as their role in ranking sites for Web search, we use the traffic data to validate the PageRank random surfing model. The ranking obtained by the actual frequency with which a site is visited by users differs significantly from that approximated by the uniform surfing/teleportation behavior modeled by PageRank, especially for the most important sites. To interpret this finding, we consider each of the fundamental assumptions underlying PageRank and show how each is violated by actual user behavior},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {65–76},
numpages = {12},
keywords = {pagerank, web traffic, navigation, teleportation, weighted host graph, search, ranking},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341544,
author = {Taylor, Michael and Guiver, John and Robertson, Stephen and Minka, Tom},
title = {SoftRank: Optimizing Non-Smooth Rank Metrics},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341544},
doi = {10.1145/1341531.1341544},
abstract = {We address the problem of learning large complex ranking functions. Most IR applications use evaluation metrics that depend only upon the ranks of documents. However, most ranking functions generate document scores, which are sorted to produce a ranking. Hence IR metrics are innately non-smooth with respect to the scores, due to the sort. Unfortunately, many machine learning algorithms require the gradient of a training objective in order to perform the optimization of the model parameters,and because IR metrics are non-smooth,we need to find a smooth proxy objective that can be used for training. We present a new family of training objectives that are derived from the rank distributions of documents, induced by smoothed scores. We call this approach SoftRank. We focus on a smoothed approximation to Normalized Discounted Cumulative Gain (NDCG), called SoftNDCG and we compare it with three other training objectives in the recent literature. We present two main results. First, SoftRank yields a very good way of optimizing NDCG. Second, we show that it is possible to achieve state of the art test set NDCG results by optimizing a soft NDCG objective on the training set with a different discount function},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {77–86},
numpages = {10},
keywords = {ranking, metrics, learning, gradient descent, optimization},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341545,
author = {Craswell, Nick and Zoeter, Onno and Taylor, Michael and Ramsey, Bill},
title = {An Experimental Comparison of Click Position-Bias Models},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341545},
doi = {10.1145/1341531.1341545},
abstract = {Search engine click logs provide an invaluable source of relevance information, but this information is biased. A key source of bias is presentation order: the probability of click is influenced by a document's position in the results page. This paper focuses on explaining that bias, modelling how probability of click depends on position. We propose four simple hypotheses about how position bias might arise. We carry out a large data-gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. We then explore which of the four hypotheses best explains the real-world position effects, and compare these to a simple logistic regression model. The data are not well explained by simple position models, where some users click indiscriminately on rank 1 or there is a simple decay of attention over ranks. A 'cascade' model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {87–94},
numpages = {8},
keywords = {web search models, user behavior, click data},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341547,
author = {Buehrer, Gregory and Chellapilla, Kumar},
title = {A Scalable Pattern Mining Approach to Web Graph Compression with Communities},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341547},
doi = {10.1145/1341531.1341547},
abstract = {A link server is a system designed to support efficient implementations of graph computations on the web graph. In this work, we present a compression scheme for the web graph specifically designed to accommodate community queries and other random access algorithms on link servers. We use a frequent pattern mining approach to extract meaningful connectivity formations. Our Virtual Node Miner achieves graph compression without sacrificing random access by generating virtual nodes from frequent itemsets in vertex adjacency lists. The mining phase guarantees scalability by bounding the pattern mining complexity to O(E log E). We facilitate global mining, relaxing the requirement for the graph to be sorted by URL, enabling discovery for both inter-domain as well as intra-domain patterns. As a consequence, the approach allows incremental graph updates. Further, it not only facilitates but can also expedite graph computations such as PageRank and local random walks by implementing them directly on the compressed graph. We demonstrate the effectiveness of the proposed approach on several publicly available large web graph data sets. Experimental results indicate that the proposed algorithm achieves a 10- to 15-fold compression on most real word web graph data sets},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {95–106},
numpages = {12},
keywords = {log-linear mining, webgraph compression, link analysis},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341548,
author = {Huang, Jian and Zhuang, Ziming and Li, Jia and Giles, C. Lee},
title = {Collaboration over Time: Characterizing and Modeling Network Evolution},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341548},
doi = {10.1145/1341531.1341548},
abstract = {A formal type of scientific and academic collaboration is coauthorship which can be represented by a coauthorship network. Coauthorship networks are among some of the largest social networks and offer us the opportunity to study the mechanisms underlying large-scale real world networks. We construct such a network for the Computer Science field covering research collaborations from 1980 to 2005, based on a large dataset of 451,305 papers authored by 283,174 distinct researchers. By mining this network, we first present a comprehensive study of the network statistical properties for a longitudinal network at the overall network level as well as for the intermediate community level. Major observations are that the database community is the best connected while the AI community is the most assortative, and that the Computer Science field as a whole shows a collaboration pattern more similar to Mathematics than to Biology. Moreover, the small world phenomenon and the scale-free degree distribution accompany the growth of the network. To study the individual collaborations, we propose a novel stochastic model, Stochastic Poisson model with Optimization Tree (Spot)to efficiently predict any increment of collaboration based on the local neighborhood structure. Spot models the non-stationary Poisson process by maximizing the log-likelihood with a tree structure. Empirical results show that Spot outperforms Support Vector Regression by better fitting collaboration records and predicting the rate of collaboration},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {107–116},
numpages = {10},
keywords = {social network analysis (SNA), stochastic network modeling, network evolution},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341549,
author = {Backstrom, Lars and Kumar, Ravi and Marlow, Cameron and Novak, Jasmine and Tomkins, Andrew},
title = {Preferential Behavior in Online Groups},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341549},
doi = {10.1145/1341531.1341549},
abstract = {Online communities in the form of message boards, listservs, and newsgroups continue to represent a considerable amount of the social activity on the Internet. Every year thousands of groups ourish while others decline into relative obscurity; likewise, millions of members join a new community every year, some of whom will come to manage or moderate the conversation while others simply sit by the sidelines and observe. These processes of group formation, growth, and dissolution are central in social science, and in an online venue they have ramifications for the design and development of community softwareIn this paper we explore a large corpus of thriving online communities. These groups vary widely in size, moderation and privacy, and cover an equally diverse set of subject matter. We present a broad range of descriptive statistics of these groups. Using metadata from groups, members, and individual messages, we identify users who post and are replied-to frequently by multiple group members; we classify these high-engagement users based on the longevity of their engagements. We show that users who will go on to become long-lived, highly-engaged users experience significantly better treatment than other users from the moment they join the group, well before there is an opportunity for them to develop a long-standing relationship with members of the groupWe present a simple model explaining long-term heavy engagement as a combination of user-dependent and group-dependent factors. Using this model as an analytical tool, we show that properties of the user alone are sufficient to explain 95% of all memberships, but introducing a small amount of per-group information dramatically improves our ability to model users belonging to multiple groups.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {117–128},
numpages = {12},
keywords = {large data sets, groups, social networks, online community},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341550,
author = {Kumar, Ravi and Tomkins, Andrew and Vee, Erik},
title = {Connectivity Structure of Bipartite Graphs via the KNC-Plot},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341550},
doi = {10.1145/1341531.1341550},
abstract = {In this paper we introduce the k-neighbor connectivity plot, or KNC-plot, as a tool to study the macroscopic connectiv-ity structure of sparse bipartite graphs. Given a bipartite graph G = (U, V, E), we say that two nodes in U are k-neighbors if there exist at least k distinct length-two paths between them; this defines a k-neighborhood graph on U where the edges are given by the k-neighbor relation. For example, in a bipartite graph of users and interests, two users are k-neighbors if they have at least k common interests. The KNC-plot shows the degradation of connectivity of the graph as a function of k. We show that this tool provides an effective and interpretable high-level characterization of the connectivity of a bipartite graphHowever, naive algorithms to compute the KNC-plot are inefficient for k &gt; 1. We give an efficient and practical algorithm that runs in sub-quadratic time O(|E|2-1/k) and is a non-trivial improvement over the obvious quadratic-time algorithms for this problem. We prove significant improvements in this runtime for graphs with power-law degree distributions, and give a different algorithm with near-linear runtime when V grows slowly as a function of the size of the graphWe compute the KNC-plot of four large real-world bipartite graphs, and discuss the structural properties of these graphs that emerge. We conclude that the KNC-plot represents a useful and practical tool for macroscopic analysis of large bipartite graphs.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {129–138},
numpages = {10},
keywords = {bipartite graphs, connected components, connectivity},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341552,
author = {Xing, Dikan and Xue, Gui-Rong and Yang, Qiang and Yu, Yong},
title = {Deep Classifier: Automatically Categorizing Search Results into Large-Scale Hierarchies},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341552},
doi = {10.1145/1341531.1341552},
abstract = {Organizing Web search results into hierarchical categories facilitates users' browsing through Web search results, especially for ambiguous queries where the potential results are mixed together. Previous methods on search result classification are usually based on pre-training a classification model on some fixed and shallow hierarchical categories, where only the top-two-level categories of a Web taxonomy is used. Such classification methods may be too coarse for users to browse, since most search results would be classified into only two or three shallow categories. Instead, a deep hierarchical classifier must provide many more categories. However, the performance of such classifiers is usually limited because their classification effectiveness can deteriorate rapidly at the third or fourth level of a hierarchy. In this paper, we propose a novel algorithm known as Deep Classifier to classify the search results into detailed hierarchical categories with higher effectiveness than previous approaches. Given the search results in response to a query, the algorithm first prunes a wide-ranged hierarchy into a narrow one with the help of some Web directories. Different strategies are proposed to select the training data by utilizing the hierarchical structures. Finally, a discriminative na\'{\i}ve Bayesian classifier is developed to perform efficient and effective classification. As a result, the algorithm can provide more meaningful and specific class labels for search result browsing than shallow style of classification. We conduct experiments to show that the Deep Classifier can achieve significant improvement over state-of-the-art algorithms. In addition, with sufficient off-line preparation, the efficiency of the proposed algorithm is suitable for online application},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {139–148},
numpages = {10},
keywords = {hierarchy pruning, hierarchical classification, deep classifier, search result mining},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341553,
author = {Shen, Dou and Walkery, Toby and Zhengy, Zijian and Yangz, Qiang and Li, Ying},
title = {Personal Name Classification in Web Queries},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341553},
doi = {10.1145/1341531.1341553},
abstract = {Personal names are an important kind of Web queries in Web search, and yet they are special in many ways. Strategies for retrieving information on personal names should therefore be different from the strategies for other types of queries. To improve the search quality for personal names, a first step is to detect whether a query is a personal name. Despite the importance of this problem, relatively little previous research has been done on this topic. Since Web queries are usually short, conventional supervised machine-learning algorithms cannot be applied directly. An alternative is to apply some heuristic rules coupled with name-term dictionaries. However, when the dictionaries are small, this method tends to make false negatives; when the dictionaries are large, it tends to generate false positives. A more serious problem is that this method cannot provide a good trade-off between precision and recall. To solve these problems, we propose an approach based on the construction of probabilistic name-term dictionaries and personal name grammars, and use this algorithm to predict the probability of a query to be a personal name. In this paper, we develop four different methods for building probabilistic name-term dictionaries in which a term is assigned with a probability value of the term being a name term. We compared our approach with baseline algorithms such as dictionary-based look-up methods and supervised classification algorithms including logistic regression and SVM on some manually labeled test sets. The results validate the effectiveness of our approach, whose F1 value is more than 79.8%, which outperforms the best baseline by more than 11.3%},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {149–158},
numpages = {10},
keywords = {probabilistic dictionaries, personal name classification, web search, web query},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341554,
author = {Mour\~{a}o, Fernando and Rocha, Leonardo and Ara\'{u}jo, Renata and Couto, Thierson and Gon\c{c}alves, Marcos and Meira, Wagner},
title = {Understanding Temporal Aspects in Document Classification},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341554},
doi = {10.1145/1341531.1341554},
abstract = {Due to the increasing amount of information present on the Web, Automatic Document Classification (ADC) has become an important research topic. ADC usually follows a standard supervised learning strategy, where we first build a model using preclassified documents and then use it to classify new unseen documents. One major challenge for ADC in many scenarios is that the characteristics of the documents and the classes to which they belong may change over time. However, most of the current techniques for ADC are applied without taking into account the temporal evolution of the collection of documentsIn this work, we perform a detailed study of the temporal evolution in the ADC, introducing an analysis methodology. We discuss that temporal evolution may be explained by three factors: 1) class distribution; 2) term distribution; and 3) class similarity. We employ metrics and experimental strategies capable of isolating each of these factors in order to analyze them separately, using two very different document collections: the ACM Digital Library and the Medline medical collections. Moreover, we present some preliminary results of potential gains that could be obtained by varying the training set to find the ideal size that minimizes the time effects. We show that by using just 69% of the ACM database, we are able to have an accuracy of 89.76%, and with only 25% of the Medline, an accuracy of 87.57%, which means gains of up to 20% in accuracy with much smaller training sets},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {159–170},
numpages = {12},
keywords = {temporal analysis, text classification, digital libraries},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341556,
author = {Vuong, Ba-Quy and Lim, Ee-Peng and Sun, Aixin and Le, Minh-Tam and Lauw, Hady Wirawan and Chang, Kuiyu},
title = {On Ranking Controversies in Wikipedia: Models and Evaluation},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341556},
doi = {10.1145/1341531.1341556},
abstract = {Wikipedia 1 is a very large and successful Web 2.0 example. As the number of Wikipedia articles and contributors grows at a very fast pace, there are also increasing disputes occurring among the contributors. Disputes often happen in articles with controversial content. They also occur frequently among contributors who are "aggressive" or controversial in their personalities. In this paper, we aim to identify controversial articles in Wikipedia. We propose three models, namely the Basic model and two Controversy Rank (CR) models. These models draw clues from collaboration and edit history instead of interpreting the actual articles or edited content. While the Basic model only considers the amount of disputes within an article, the two Controversy Rank models extend the former by considering the relationships between articles and contributors. We also derived enhanced versions of these models by considering the age of articles. Our experiments on a collection of 19,456 Wikipedia articles shows that the Controversy Rank models can more effectively determine controversial articles compared to the Basic and other baseline models},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {171–182},
numpages = {12},
keywords = {online dispute, controversy rank, wikipedia},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341557,
author = {Agichtein, Eugene and Castillo, Carlos and Donato, Debora and Gionis, Aristides and Mishne, Gilad},
title = {Finding High-Quality Content in Social Media},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341557},
doi = {10.1145/1341531.1341557},
abstract = {The quality of user-generated content varies drastically from excellent to abuse and spam. As the availability of such content increases, the task of identifying high-quality content sites based on user contributions --social media sites -- becomes increasingly important. Social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. In this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. As a test case, we focus on Yahoo! Answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. We introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. In particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {183–194},
numpages = {12},
keywords = {community question answering, user interactions, media},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341558,
author = {Heymann, Paul and Koutrika, Georgia and Garcia-Molina, Hector},
title = {Can Social Bookmarking Improve Web Search?},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341558},
doi = {10.1145/1341531.1341558},
abstract = {Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio. us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {195–206},
numpages = {12},
keywords = {social bookmarking, web search, collaborative tagging},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341559,
author = {Agarwal, Nitin and Liu, Huan and Tang, Lei and Yu, Philip S.},
title = {Identifying the Influential Bloggers in a Community},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341559},
doi = {10.1145/1341531.1341559},
abstract = {Blogging becomes a popular way for a Web user to publish information on the Web. Bloggers write blog posts, share their likes and dislikes, voice their opinions, provide suggestions, report news, and form groups in Blogosphere. Bloggers form their virtual communities of similar interests. Activities happened in Blogosphere affect the external world. One way to understand the development on Blogosphere is to find influential blog sites. There are many non-influential blog sites which form the "the long tail". Regardless of a blog site being influential or not, there are influential bloggers. Inspired by the high impact of the influentials in a physical community, we study a novel problem of identifying influential bloggers at a blog site. Active bloggers are not necessarily influential. Influential bloggers can impact fellow bloggers in various ways. In this paper, we discuss the challenges of identifying influential bloggers, investigate what constitutes influential bloggers, present a preliminary model attempting to quantify an influential blogger, and pave the way for building a robust model that allows for finding various types of the influentials. To illustrate these issues, we conduct experiments with data from a real-world blog site, evaluate multi-facets of the problem of identifying influential bloggers, and discuss unique challenges. We conclude with interesting findings and future work},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {207–218},
numpages = {12},
keywords = {blogosphere, influential bloggers, social networks},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341560,
author = {Jindal, Nitin and Liu, Bing},
title = {Opinion Spam and Analysis},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341560},
doi = {10.1145/1341531.1341560},
abstract = {Evaluative texts on the Web have become a valuable source of opinions on products, services, events, individuals, etc. Recently, many researchers have studied such opinion sources as product reviews, forum posts, and blogs. However, existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques. An important issue that has been neglected so far is opinion spam or trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews, which are opinion rich and are widely used by consumers and product manufacturers. In the past two years, several startup companies also appeared which aggregate opinions from product reviews. It is thus high time to study spam in reviews. To the best of our knowledge, there is still no published study on this topic, although Web spam and email spam have been investigated extensively. We will see that opinion spam is quite different from Web spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that opinion spam in reviews is widespread. This paper analyzes such spam activities and presents some novel techniques to detect them},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {219–230},
numpages = {12},
keywords = {fake reviews, opinion spam, review analysis, review spam},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341561,
author = {Ding, Xiaowen and Liu, Bing and Yu, Philip S.},
title = {A Holistic Lexicon-Based Approach to Opinion Mining},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341561},
doi = {10.1145/1341531.1341561},
abstract = {One of the important types of information on the Web is the opinions expressed in the user generated content, e.g., customer reviews of products, forum posts, and blogs. In this paper, we focus on customer reviews of products. In particular, we study the problem of determining the semantic orientations (positive, negative or neutral) of opinions expressed on product features in reviews. This problem has many applications, e.g., opinion mining, summarization and search. Most existing techniques utilize a list of opinion (bearing) words (also called opinion lexicon) for the purpose. Opinion words are words that express desirable (e.g., great, amazing, etc.) or undesirable (e.g., bad, poor, etc) states. These approaches, however, all have some major shortcomings. In this paper, we propose a holistic lexicon-based approach to solving the problem by exploiting external evidences and linguistic conventions of natural language expressions. This approach allows the system to handle opinion words that are context dependent, which cause major difficulties for existing algorithms. It also deals with many special words, phrases and language constructs which have impacts on opinions based on their linguistic patterns. It also has an effective function for aggregating multiple conflicting opinion words in a sentence. A system, called Opinion Observer, based on the proposed technique has been implemented. Experimental results using a benchmark product review data set and some additional reviews show that the proposed technique is highly effective. It outperforms existing methods significantly},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {231–240},
numpages = {10},
keywords = {opinion mining, sentiment analysis, context dependent opinions},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341563,
author = {Ghose, Anindya and Yang, Sha},
title = {An Empirical Analysis of Sponsored Search Performance in Search Engine Advertising},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341563},
doi = {10.1145/1341531.1341563},
abstract = {The phenomenon of sponsored search advertising - where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results - is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {241–250},
numpages = {10},
keywords = {paid search advertising, internet markets, online advertising, search engines, hierarchical bayesian modeling, electronic commerce},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341564,
author = {Chen, Yifan and Xue, Gui-Rong and Yu, Yong},
title = {Advertising Keyword Suggestion Based on Concept Hierarchy},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341564},
doi = {10.1145/1341531.1341564},
abstract = {The increasing growth of the World Wide Web constantly enlarges the revenue generated by search engine advertising. Advertisers bid on keywords associated with their products to display their ads on the search result pages. Keyword suggestion methods are proposed to fill the gap between the keywords chosen by advertisers and the popular queries, through finding new relevant keywords according to some statistical information (for example, the keyword co-occurrence). However, there is little effort taking semantic information, such as concept hierarchy, into account. In this paper, we propose a novel keyword suggestion method that fully exploits the semantic knowledge among concept hierarchy. Given a keyword, we first match it with some relevant concepts. Then the relevant concepts are used with their hierarchy to fertilize the meanings of the keywords. Finally new keywords are suggested according to the concept information rather than the statistical co-occurrence of the keyword itself. Experimental results show that our proposed method can successfully provide suggestion that meets the accuracy and coverage requirements},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {251–260},
numpages = {10},
keywords = {keyword suggestion, concept hierarchy, advertising},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341532,
author = {Garcia-Molina, Hector},
title = {Web Information Management: Past, Present and Future},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341532},
doi = {10.1145/1341531.1341532},
abstract = {In this talk I will give a brief retrospective on Web Information Management, and will discuss some of the key challenges for the future. I will not give a survey of all work in the area; instead I will give my personal perspective based on work in the InfoLab at Stanford. In particular, I will touch on our lab's work on crawling, indexing, ranking, personalization, and more recently on spam detection and social networking.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {1},
numpages = {1},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

@inproceedings{10.1145/1341531.1341533,
author = {Etzioni, Oren},
title = {Machine Reading at Web Scale},
year = {2008},
isbn = {9781595939272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341531.1341533},
doi = {10.1145/1341531.1341533},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
pages = {2},
numpages = {1},
keywords = {information extraction, machine reading},
location = {Palo Alto, California, USA},
series = {WSDM '08}
}

