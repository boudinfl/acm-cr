@inproceedings{10.1145/3250031,
author = {Talukdar, Partha},
title = {Session Details: Oral Session},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250031},
doi = {10.1145/3250031},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
numpages = {1},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509559,
author = {Singh, Sameer and Riedel, Sebastian and Martin, Brian and Zheng, Jiaping and McCallum, Andrew},
title = {Joint Inference of Entities, Relations, and Coreference},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509559},
doi = {10.1145/2509558.2509559},
abstract = {Although joint inference is an effective approach to avoid cascading of errors when inferring multiple natural language tasks, its application to information extraction has been limited to modeling only two tasks at a time, leading to modest improvements. In this paper, we focus on the three crucial tasks of automated extraction pipelines: entity tagging, relation extraction, and coreference. We propose a single, joint graphical model that represents the various dependencies between the tasks, allowing flow of uncertainty across task boundaries. Since the resulting model has a high tree-width and contains a large number of variables, we present a novel extension to belief propagation that sparsifies the domains of variables during inference. Experimental results show that our joint model consistently improves results on all three tasks as we represent more dependencies. In particular, our joint model obtains 12% error reduction on tagging over the isolated models.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {1–6},
numpages = {6},
keywords = {joint inference, information extraction, relation extraction, named entity recognition, coreference resolution},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509560,
author = {Ling, Xiao and Clark, Peter and Weld, Daniel S.},
title = {Extracting Meronyms for a Biology Knowledge Base Using Distant Supervision},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509560},
doi = {10.1145/2509558.2509560},
abstract = {Knowledge of objects and their parts, meronym relations, are at the heart of many question-answering systems, but manually encoding these facts is impractical. Past researchers have tried hand-written patterns, supervised learning, and bootstrapped methods, but achieving both high precision and recall has proven elusive. This paper reports on a thorough exploration of distant supervision to learn a meronym extractor for the domain of college biology. We introduce a novel algorithm, generalizing the ``at least one'' assumption of multi-instance learning to handle the case where a fixed (but unknown) percentage of bag members are positive examples. Detailed experiments compare strategies for mention detection, negative example generation, leveraging out-of-domain meronyms, and evaluate the benefit of our multi-instance percentage model.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {7–12},
numpages = {6},
keywords = {relation extraction, distant supervision, information extraction, automated knowledge base construction},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509561,
author = {Wick, Michael and Singh, Sameer and Kobren, Ari and McCallum, Andrew},
title = {Assessing Confidence of Knowledge Base Content with an Experimental Study in Entity Resolution},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509561},
doi = {10.1145/2509558.2509561},
abstract = {The purpose of this paper is to begin a conversation about the importance and role of confidence estimation in knowledge bases (KBs). KBs are never perfectly accurate, yet without confidence reporting their users are likely to treat them as if they were, possibly with serious real-world consequences. We define a notion of confidence based on the probability of a KB fact being true. For automatically constructed KBs we propose several algorithms for estimating this confidence from pre-existing probabilistic models of data integration and KB construction. In particular, this paper focuses on confidence estimation in entity resolution. A goal of our exposition here is to encourage creators and curators of KBs to include confidence estimates for entities and relations in their KBs.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {13–18},
numpages = {6},
keywords = {entity resolution, information extraction, uncertain data},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509562,
author = {Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William W.},
title = {Ontology-Aware Partitioning for Knowledge Graph Identification},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509562},
doi = {10.1145/2509558.2509562},
abstract = {Knowledge graphs provide a powerful representation of entities and the relationships between them, but automatically constructing such graphs from noisy extractions presents numerous challenges. Knowledge graph identification (KGI) is a technique for knowledge graph construction that jointly reasons about entities, attributes and relations in the presence of uncertain inputs and ontological constraints. Although knowledge graph identification shows promise scaling to knowledge graphs built from millions of extractions, increasingly powerful extraction engines may soon require knowledge graphs built from billions of extractions. One tool for scaling is partitioning extractions to allow reasoning to occur in parallel. We explore approaches which leverage ontological information and distributional information in partitioning. We compare these techniques with hash-based approaches, and show that using a richer partitioning model that incorporates the ontology graph and distribution of extractions provides superior results. Our results demonstrate that partitioning can result in order-of-magnitude speedups without reducing model performance.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {19–24},
numpages = {6},
keywords = {knowledge graph identification, partitioning probabilistic graphical models},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509563,
author = {Gordon, Jonathan and Van Durme, Benjamin},
title = {Reporting Bias and Knowledge Acquisition},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509563},
doi = {10.1145/2509558.2509563},
abstract = {Much work in knowledge extraction from text tacitly assumes that the frequency with which people write about actions, outcomes, or properties is a reflection of real-world frequencies or the degree to which a property is characteristic of a class of individuals. In this paper, we question this idea, examining the phenomenon of reporting bias and the challenge it poses for knowledge extraction. We conclude with discussion of approaches to learning commonsense knowledge from text despite this distortion.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {25–30},
numpages = {6},
keywords = {reporting bias, text frequency, knowledge extraction},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509564,
author = {Dalvi, Bhavana and Cohen, William W. and Callan, Jamie},
title = {Classifying Entities into an Incomplete Ontology},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509564},
doi = {10.1145/2509558.2509564},
abstract = {Exponential growth of unlabeled web-scale datasets, and class hierarchies to represent them, has given rise to new challenges for hierarchical classification. It is costly and time consuming to create a complete ontology of classes to represent entities on the Web. Hence, there is a need for techniques that can do hierarchical classification of entities into incomplete ontologies. In this paper we present Hierarchical Exploratory EM algorithm (an extension of the Exploratory EM algorithm [7]) that takes a seed class hierarchy and seed class instances as input. Our method classifies relevant entities into some of the classes from the seed hierarchy and on its way adds newly discovered classes into the hierarchy. Experiments with subsets of the NELL ontology and text datasets derived from the ClueWeb09 corpus show that our Hierarchical Exploratory EM approach improves seed class F1 by up to 21% when compared to its semi-supervised counterpart.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {31–36},
numpages = {6},
keywords = {expectation maximization (em), clustering, ontologies, knowledge bases, semi-supervised learning, hierarchical classification},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509565,
author = {Clark, Peter and Harrison, Philip and Balasubramanian, Niranjan},
title = {A Study of the Knowledge Base Requirements for Passing an Elementary Science Test},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509565},
doi = {10.1145/2509558.2509565},
abstract = {Our long-term interest is in machines that contain large amounts of general and scientific knowledge, stored in a "computable" form that supports reasoning and explanation. As a medium-term focus for this, our goal is to have the computer pass a fourth-grade science test, anticipating that much of the required knowledge will need to be acquired semi-automatically. This paper presents the first step towards this goal, namely a blueprint of the knowledge requirements for an early science exam, and a brief description of the resources, methods, and challenges involved in the semi-automatic acquisition of that knowledge. The result of our analysis suggests that as well as fact extraction from text and statistically driven rule extraction, three other styles of automatic knowledge base construction (AKBC) would be useful: acquiring definitional knowledge, direct 'reading' of rules from texts that state them, and, given a particular representational framework (e.g., qualitative reasoning), acquisition of specific instances of those models from text (e..g, specific qualitative models).},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {37–42},
numpages = {6},
keywords = {knowledge base construction, knowledge acquisition},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509566,
author = {Gal\'{a}rraga, Luis A. and Preda, Nicoleta and Suchanek, Fabian M.},
title = {Mining Rules to Align Knowledge Bases},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509566},
doi = {10.1145/2509558.2509566},
abstract = {The Semantic Web has made huge progress in the last decade, and now comprises hundreds of knowledge bases (KBs). The Linked Open Data cloud connects the KBs in this Web of data. However, the links between the KBs are mostly concerned with the instances, not with the schema. Aligning the schemas is not easy, because the KBs may differ not just in their names for relations and classes, but also in their inherent structure. Therefore, we argue in this paper that advanced schema alignment is needed to tie the Semantic Web together. We put forward a particularly simple approach to illustrate how that might look.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {43–48},
numpages = {6},
keywords = {linked data, ontology alignment, rule mining},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509567,
author = {Huet, Thomas and Biega, Joanna and Suchanek, Fabian M.},
title = {Mining History with Le Monde},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509567},
doi = {10.1145/2509558.2509567},
abstract = {The last decade has seen the rise of large knowledge bases, such as YAGO, DBpedia, Freebase, or NELL. In this paper, we show how this structured knowledge can help understand and mine trends in unstructured data. By combining YAGO with the archive of the French newspaper Le Monde, we can conduct analyses that would not be possible with word frequency statistics alone. We find indications about the increasing role that women play in politics, about the impact that the city of birth can have on a person's career, or about the average age of famous people in different professions.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {49–54},
numpages = {6},
keywords = {yago, knowledge base, le monde, culturomics},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/3250032,
author = {Suchanek, Fabian},
title = {Session Details: Poster Session},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250032},
doi = {10.1145/3250032},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
numpages = {1},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509568,
author = {Dalton, Jeffrey and Dietz, Laura},
title = {Constructing Query-Specific Knowledge Bases},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509568},
doi = {10.1145/2509558.2509568},
abstract = {Abstract Large general purpose knowledge bases (KB) support a variety of complex tasks because of their structured relationships. However, these KBs lack coverage for specialized topics or use cases. In these scenarios, users often use keyword search over large unstructured collections, such as the web. Instead, we propose constructing a 'knowledge sketch' that leverages existing KB data elements and relevant text documents to construct query-specific KB data. A knowledge sketch is a distribution over entities, documents, and relationships between entities, all for a specific information need. In our experiments we construct knowledge sketches for queries from the TREC 2004 Robust track, which emphasizes complex queries which perform poorly with existing text retrieval approaches.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {55–60},
numpages = {6},
keywords = {knowledge base construction, entity linking, relevance modeling},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509569,
author = {Downey, Doug and Bhagavatula, Chandra Sekhar and Yates, Alexander},
title = {Using Natural Language to Integrate, Evaluate, and Optimize Extracted Knowledge Bases},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509569},
doi = {10.1145/2509558.2509569},
abstract = {Web Information Extraction (WIE) systems extract billions of unique facts, but integrating the assertions into a coherent knowledge base and evaluating across different WIE techniques remains a challenge. We propose a framework that utilizes natural language to integrate and evaluate extracted knowledge bases (KBs). In the framework, KBs are integrated by exchanging probability distributions over natural language, and evaluated by how well the output distributions predict held-out text. We describe the advantages of the approach, and detail remaining research challenges.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {61–66},
numpages = {6},
keywords = {knowledge extraction, language modeling, knowledge integration},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509570,
author = {Wick, Michael and Singh, Sameer and Pandya, Harshal and McCallum, Andrew},
title = {A Joint Model for Discovering and Linking Entities},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509570},
doi = {10.1145/2509558.2509570},
abstract = {Entity resolution, the task of automatically determining which mentions refer to the same real-world entity, is a crucial aspect of knowledge base construction and management. However, performing entity resolution at large scales is challenging because (1) the inference algorithms must cope with unavoidable system scalability issues and (2) the search space grows exponentially in the number of mentions. Current conventional wisdom has been that performing coreference at these scales requires decomposing the problem by first solving the simpler task of entity-linking (matching a set of mentions to a known set of KB entities), and then performing entity discovery as a post-processing step (to identify new entities not present in the KB). However, we argue that this traditional approach is harmful to both entity-linking and overall coreference accuracy. Therefore, we embrace the challenge of jointly modeling entity-linking and entity-discovery as a single entity resolution problem. In order to make progress towards scalability we (1) present a model that reasons over compact hierarchical entity representations, and (2) propose a novel distributed inference architecture that does not suffer from the synchronicity bottleneck which is inherent in map-reduce architectures. We demonstrate that more test-time data actually improves the accuracy of coreference, and show that joint coreference is substantially more accurate than traditional entity-linking, reducing error by 75%.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {67–72},
numpages = {6},
keywords = {entity linking, entity resolution, coreference},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509571,
author = {Roth, Benjamin and Barth, Tassilo and Wiegand, Michael and Klakow, Dietrich},
title = {A Survey of Noise Reduction Methods for Distant Supervision},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509571},
doi = {10.1145/2509558.2509571},
abstract = {We survey recent approaches to noise reduction in distant supervision learning for relation extraction. We group them according to the principles they are based on: at-least-one constraints, topic-based models, or pattern correlations. Besides describing them, we illustrate the fundamental differences and attempt to give an outlook to potentially fruitful further research. In addition, we identify related work in sentiment analysis which could profit from approaches to noise reduction.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {73–78},
numpages = {6},
keywords = {relation extraction, machine learning, distant supervision},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509572,
author = {Yao, Limin and Riedel, Sebastian and McCallum, Andrew},
title = {Universal Schema for Entity Type Prediction},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509572},
doi = {10.1145/2509558.2509572},
abstract = {Categorizing entities by their types is useful in many applications, including knowledge base construction, relation extraction and query intent prediction. Fine-grained entity type ontologies are especially valuable, but typically difficult to design because of unavoidable quandaries about level of detail and boundary cases. Automatically classifying entities by type is challenging as well, usually involving hand-labeling data and training a supervised predictor.This paper presents a universal schema approach to fine-grained entity type prediction. The set of types is taken as the union of textual surface patterns (e.g. appositives) and pre-defined types from available databases (e.g. Freebase)---yielding not tens or hundreds of types, but more than ten thousands of entity types, such as financier, criminologist, and musical trio. We robustly learn mutual implication among this large union by learning latent vector embeddings from probabilistic matrix factorization, thus avoiding the need for hand-labeled data. Experimental results demonstrate more than 30% reduction in error versus a traditional classification approach on predicting fine-grained entities types.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {79–84},
numpages = {6},
keywords = {matrix factorization, entity},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509573,
author = {Siahbani, Maryam and Vadlapudi, Ravikiran and Whitney, Max and Sarkar, Anoop},
title = {Knowledge Base Population and Visualization Using an Ontology Based on Semantic Roles},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509573},
doi = {10.1145/2509558.2509573},
abstract = {This paper extracts facts using "micro-reading" of text in contrast to approaches that extract common-sense knowledge using "macro-reading" methods. Our goal is to extract detailed facts about events from natural language using a predicate-centered view of events (who did what to whom, when and how). We exploit semantic role labels in order to create a novel predicate-centric ontology for entities in our knowledge base. This allows users to find uncommon facts easily. To this end, we tightly couple our knowledge base and ontology to an information visualization system that can be used to explore and navigate events extracted from a large natural language text collection. We use our methodology to create a web-based visual browser of history events in Wikipedia.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {85–90},
numpages = {6},
keywords = {knowledge base, information extraction, ontology},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509574,
author = {Schuhmacher, Michael and Ponzetto, Simone Paolo},
title = {Exploiting DBpedia for Web Search Results Clustering},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509574},
doi = {10.1145/2509558.2509574},
abstract = {We present a knowledge-rich approach to Web search result clustering which exploits the output of an open-domain entity linker, as well as the types and topical concepts encoded within a wide-coverage ontology. Our results indicate that, thanks to an accurate and compact semantification of the search result snippets, we are able to achieve a competitive performance on a benchmarking dataset for this task.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {91–96},
numpages = {6},
keywords = {search result clustering, natural language processing, semantic networks, dbpedia},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509575,
author = {Li, Xinyu and Rastan, Roya and Shepherd, John and Paik, Hye Young},
title = {Automatic Affiliation Extraction from Calls-for-Papers},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509575},
doi = {10.1145/2509558.2509575},
abstract = {In this paper, we describe a system to collect information about academic affiliation (organisations where researchers work) from Calls-for-Papers for academic conferences. The system uses a range of heuristic approaches and open-source tools in order to extract and identify entities, and to incorporate the information into a pre-defined database schema. This forms part of a larger project to automatically populate and maintain a range of data related to academic research. The proposed system is currently being tested and some promising preliminary results are available.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {97–102},
numpages = {6},
keywords = {research data management, affiliation extraction, column-based structures},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509576,
author = {Yeh, Eric and Niekrasz, John and Freitag, Dayne},
title = {Unsupervised Discovery and Extraction of Semi-Structured Regions in Text via Self-Information},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509576},
doi = {10.1145/2509558.2509576},
abstract = {We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. Examples include tables, key-value listings, or repeated enumerations of properties. Because of their generally non-sentential nature, these regions can present problems for standard information extraction algorithms. Unlike previous work in table extraction, which relies on a relatively noiseless two-dimensional layout, our aim is to accommodate a wide variety of structure types. Our approach for identifying semi-structured regions is an unsupervised one, based on scoring unusual regularity inside the document. As content in semi-structured regions are governed by a schema, the occurrence of features encompassing textual content and visual appearance would be unusual compared to those seen in sentential language. Regularity refers to repetition of these unusual features, as semi-structured regions commonly encode more than a single row or group of information. To score this, we present a measure based on expected self-information, derived from statistics over patterns of textual categories and visual layout. We describe the results of an initial study to assess the ability of these measures to detect semi-structured text in a corpus culled from the web, and show that this measure outperform baseline methods on an average precision measure. We present initial work that uses these significant patterns to generate extraction rules, and conclude with a discussion of future directions.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {103–108},
numpages = {6},
keywords = {semi-structured information extraction},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

@inproceedings{10.1145/2509558.2509577,
author = {Radhakrishnan, Priya and Varma, Vasudeva},
title = {Extracting Semantic Knowledge from Wikipedia Category Names},
year = {2013},
isbn = {9781450324113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509558.2509577},
doi = {10.1145/2509558.2509577},
abstract = {Wikipedia being a large, freely available, frequently updated and community maintained knowledge base, has been central to much recent research. However, quite often we find that the information extracted from it has extraneous content. This paper proposes a method to extract useful information from Wikipedia, using Semantic Features derived from Wikipedia categories. The proposed method provides good performance as a Wikipedia category based method. Experimental results on benchmark datasets show that the proposed method achieves a correlation coefficient of 0.66 with human judgments. The Semantic Features derived by this method gave good correlation with human rankings in a web search query completion application.},
booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
pages = {109–114},
numpages = {6},
keywords = {semantic relatedness, information extraction},
location = {San Francisco, California, USA},
series = {AKBC '13}
}

