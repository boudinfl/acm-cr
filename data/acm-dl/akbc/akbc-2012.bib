@inproceedings{10.5555/2391200.2391201,
author = {Niepert, Mathias and Meilicke, Christian and Stuckenschmidt, Heiner},
title = {Towards Distributed MCMC Inference in Probabilistic Knowledge Bases},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Probabilistic knowledge bases are commonly used in areas such as large-scale information extraction, data integration, and knowledge capture, to name but a few. Inference in probabilistic knowledge bases is a computationally challenging problem. With this contribution, we present our vision of a distributed inference algorithm based on conflict graph construction and hypergraph sampling. Early empirical results show that the approach efficiently and accurately computes a-posteriori probabilities of a knowledge base derived from a well-known information extraction system.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {1–6},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391202,
author = {Dalvi, Bhavana and Cohen, William W. and Callan, Jamie},
title = {Collectively Representing Semi-Structured Data from the Web},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we propose a single low-dimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. Specifically we consider queries like set expansion, class prediction etc. We evaluate our methods on publicly available semi-structured datasets from the Web.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {7–12},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391203,
author = {Saggion, Horacio},
title = {Unsupervised Content Discovery from Concise Summaries},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Domain adaptation is a time consuming and costly procedure calling for the development of algorithms and tools to facilitate its automation. This paper presents an unsupervised algorithm able to learn the main concepts in event summaries. The method takes as input a set of domain summaries annotated with shallow linguistic information and produces a domain template. We demonstrate the viability of the method by applying it to three different domains and two languages. We have evaluated the generated templates against human templates obtaining encouraging results.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {13–18},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391204,
author = {Bronzi, Mirko and Guo, Zhaochen and Mesquita, Filipe and Barbosa, Denilson and Merialdo, Paolo},
title = {Automatic Evaluation of Relation Extraction Systems on Large-Scale},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The extraction of relations between named entities from natural language text is a longstanding challenge in information extraction, especially in large-scale. A major challenge for the advancement of this research field has been the lack of meaningful evaluation frameworks based on realistic-sized corpora. In this paper we propose a framework for large-scale evaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {19–24},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391205,
author = {Tamang, Suzanne and Ji, Heng},
title = {Relabeling Distantly Supervised Training Data for Temporal Knowledge Base Population},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We enhance a temporal knowledge base population system to improve the quality of distantly supervised training data and identify a minimal feature set for classification. The approach uses multi-class logistic regression to eliminate individual features based on the strength of their association with a temporal label followed by semi-supervised relabeling using a subset of human annotations and lasso regression. As implemented in this work, our technique improves performance and results in notably less computational cost than a parallel system trained on the full feature set.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {25–30},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391206,
author = {Li, Bin and Chen, Jiajun and Zhang, Yingjie},
title = {Web Based Collection and Comparison of Cognitive Properties in English and Chinese},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Cognitive properties of words are very useful in figurative language understanding, language acquisition and translation. To overcome the subjectivity and low efficiency in manual construction of such database, we propose a web-based method for automatic collection and analysis of cognitive properties. The method employs simile templates to query the search engines. With the help of a bilingual dictionary, the method is able to collect tens of thousands of "vehicle-adjective" items of high quality. Frequencies are then used to obtain the common and independent cognitive properties automatically. The method can be extended conveniently to other languages to construct multi-lingual cognitive property knowledgebase.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {31–34},
numpages = {4},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391207,
author = {Stern, Rosa and Sagot, Beno\^{\i}t},
title = {Population of a Knowledge Base for News Metadata from Unstructured Text and Web Data},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a practical use case of knowledge base (KB) population at the French news agency AFP. The target KB instances are entities relevant for news production and content enrichment. In order to acquire uniquely identified entities over news wires, i.e. textual data, and integrate the resulting KB in the Linked Data framework, a series of data models need to be aligned: Web data resources are harvested for creating a wide coverage entity database, which is in turn used to link entities to their mentions in French news wires. Finally, the extracted entities are selected for instantiation in the target KB. We describe our methodology along with the resources created and used for the target KB population.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {35–40},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391208,
author = {Nakashole, Ndapandula and Weikum, Gerhard},
title = {Real-Time Population of Knowledge Bases: Opportunities and Challenges},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Dynamic content is a frequently accessed part of the Web. However, most information extraction approaches are batch-oriented, thus not effective for gathering rapidly changing data. This paper proposes a model for fact extraction in real-time. Our model addresses the difficult challenges that timely fact extraction on frequently updated data entails. We point out a naive solution to the main research question and justify the choices we make in the model we propose.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {41–45},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391209,
author = {Gardner, Matthew},
title = {Adding Distributional Semantics to Knowledge Base Entities through Web-Scale Entity Linking},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Web-scale knowledge bases typically consist entirely of predicates over entities. However, the distributional properties of how those entities appear in text are equally important aspects of knowledge. If noun phrases mapped unambiguously to knowledge base entities, adding this knowledge would simply require counting. The many-to-many relationship between noun phrase mentions and knowledge base entities makes adding distributional knowledge about entities difficult. In this paper, we argue that this information should be explicitly included in web-scale knowledge bases. We propose a generative model that learns these distributional semantics by performing entity linking on the web, and we give some preliminary results that point to its usefulness.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {46–51},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391210,
author = {Akbik, Alan and L\"{o}ser, Alexander},
title = {KrakeN: N-Ary Facts in Open Information Extraction},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Current techniques for Open Information Extraction (OIE) focus on the extraction of binary facts and suffer significant quality loss for the task of extracting higher order N-ary facts. This quality loss may not only affect the correctness, but also the completeness of an extracted fact. We present KrakeN, an OIE system specifically designed to capture N-ary facts, as well as the results of an experimental study on extracting facts from Web text in which we examine the issue of fact completeness. Our preliminary experiments indicate that KrakeN is a high precision OIE approach that captures more facts per sentence at greater completeness than existing OIE approaches, but is vulnerable to noisy and ungrammatical text.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {52–56},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391211,
author = {Grishman, Ralph},
title = {Structural Linguistics and Unsupervised Information Extraction},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {A precondition for extracting information from large text corpora is discovering the information structures underlying the text. Progress in this direction is being made in the form of unsupervised information extraction (IE). We describe recent work in unsupervised relation extraction and compare its goals to those of grammar discovery for science sublanguages. We consider what this work on grammar discovery suggests for future directions in unsupervised IE.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {57–61},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391212,
author = {Stoyanov, Veselin and Mayfield, James and Xu, Tan and Oard, Douglas W. and Lawrie, Dawn and Oates, Tim and Finin, Tim},
title = {A Context-Aware Approach to Entity Linking},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Entity linking refers to the task of assigning mentions in documents to their corresponding knowledge base entities. Entity linking is a central step in knowledge base population. Current entity linking systems do not explicitly model the discourse context in which the communication occurs. Nevertheless, the notion of shared context is central to the linguistic theory of pragmatics and plays a crucial role in Grice's cooperative communication principle. Furthermore, modeling context facilitates joint resolution of entities, an important problem in entity linking yet to be addressed satisfactorily. This paper describes an approach to context-aware entity linking.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {62–67},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391213,
author = {Mayfield, James and Finin, Tim},
title = {Evaluating the Quality of a Knowledge Base Populated from Text},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The steady progress of information extraction systems has been helped by sound methodologies for evaluating their performance in controlled experiments. Annual events like MUC, ACE and TAC have developed evaluation approaches enabling researchers to score and rank their systems relative to reference results. Yet these evaluations have only assessed component technologies needed by a knowledge base population system; none has required the construction of a knowledge base that is then evaluated directly. We describe an approach to the direct evaluation of a knowledge base and an instantiation that will be used in a 2012 TAC Knowledge Base Population track.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {68–73},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391214,
author = {Clark, Peter and Harrison, Phil and Balasubramanian, Niranjan and Etzioni, Oren},
title = {Constructing a Textual KB from a Biology TextBook},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {As part of our work on building a "knowledgeable textbook" about biology, we are developing a textual question-answering (QA) system that can answer certain classes of biology questions posed by users. In support of that, we are building a "textual KB" - an assembled set of semi-structured assertions based on the book - that can be used to answer users' queries, can be improved using global consistency constraints, and can be potentially validated and corrected by domain experts. Our approach is to view the KB as systematically caching answers from a QA system, and the QA system as assembling answers from the KB, the whole process kickstarted with an initial set of textual extractions from the book text itself. Although this research is only in a preliminary stage, we summarize our progress and lessons learned to date.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {74–78},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391215,
author = {Kiddon, Chlo\'{e} and Domingos, Pedro},
title = {Knowledge Extraction and Joint Inference Using Tractable Markov Logic},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The development of knowledge base creation systems has mainly focused on information extraction without considering how to effectively reason over their databases of facts. One reason for this is that the inference required to learn a probabilistic knowledge base from text at any realistic scale is intractable. In this paper, we propose formulating the joint problem of fact extraction and probabilistic model learning in terms of Tractable Markov Logic (TML), a subset of Markov logic in which inference is low-order polynomial in the size of the knowledge base. Using TML, we can tractably extract new information from text while simultaneously learning a probabilistic knowledge base. We will also describe a testbed for our proposal: creating a biomedical knowledge base and making it available for querying on the Web.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {79–83},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391216,
author = {Lin, Thomas and Mausam and Etzioni, Oren},
title = {Entity Linking at Web Scale},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper investigates entity linking over millions of high-precision extractions from a corpus of 500 million Web documents, toward the goal of creating a useful knowledge base of general facts. This paper is the first to report on entity linking over this many extractions, and describes new opportunities (such as corpus-level features) and challenges we found when entity linking at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {84–88},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391217,
author = {Wick, Michael and Schultz, Karl and McCallum, Andrew},
title = {Human-Machine Cooperation with Epistemological DBs: Supporting User Corrections to Knowledge Bases},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Knowledge bases (KB) provide support for real-world decision making by exposing data in a structured format. However, constructing knowledge bases requires gathering data from many heterogeneous sources. Manual efforts for this task are accurate, but lack scalability, and automated approaches provide good coverage, but are not reliable enough for real-world decision makers to trust. These two approaches to KB construction have complementary strengths: in this paper we propose a novel framework for supporting humanproposed edits to knowledge bases.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {89–94},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391218,
author = {Napoles, Courtney and Gormley, Matthew and Van Durme, Benjamin},
title = {Annotated Gigaword},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {95–100},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391219,
author = {Balasubramanian, Niranjan and Soderland, Stephen and Mausam and Etzioni, Oren},
title = {Rel-Grams: A Probabilistic Model of Relations in Text},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We introduce the Rel-grams language model, which is analogous to an n-grams model, but is computed over relations rather than over words. The model encodes the conditional probability of observing a relational tuple R, given that R' was observed in a window of prior relational tuples. We build a database of Rel-grams co-occurence statistics from Re-Verb extractions over 1.8M news wire documents and show that a graphical model based on these statistics is useful for automatically discovering event templates. We make this database freely available and hope it will prove a useful resource for a wide variety of NLP tasks.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {101–105},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391220,
author = {Wang, Daisy Zhe and Chen, Yang and Goldberg, Sean and Grant, Christan and Li, Kun},
title = {Automatic Knowledge Base Construction Using Probabilistic Extraction, Deductive Reasoning, and Human Feedback},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We envision an automatic knowledge base construction system consisting of three inter-related components. MADden is a knowledge extraction system applying statistical text analysis methods over database systems (DBMS) and massive parallel processing (MPP) frameworks; ProbKB performs probabilistic reasoning over the extracted knowledge to derive additional facts not existing in the original text corpus; CAMeL leverages human intelligence to reduce the uncertainty resulting from both the information extraction and probabilistic reasoning processes.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {106–110},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391221,
author = {Singh, Sameer and Wick, Michael and McCallum, Andrew},
title = {Monte Carlo MCMC: Efficient Inference by Sampling Factors},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Conditional random fields and other graphical models have achieved state of the art results in a variety of NLP and IE tasks including coreference and relation extraction. Increasingly, practitioners are using models with more complex structure---higher tree-width, larger fanout, more features, and more data---rendering even approximate inference methods such as MCMC inefficient. In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors. We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference. In an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular MCMC inference.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {111–115},
numpages = {5},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391222,
author = {Yao, Limin and Riedel, Sebastian and McCallum, Andrew},
title = {Probabilistic Databases of Universal Schema},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In data integration we transform information from a source into a target schema. A general problem in this task is loss of fidelity and coverage: the source expresses more knowledge than can fit into the target schema, or knowledge that is hard to fit into any schema at all. This problem is taken to an extreme in information extraction (IE) where the source is natural language. To address this issue, one can either automatically learn a latent schema emergent in text (a brittle and ill-defined task), or manually extend schemas. We propose instead to store data in a probabilistic database of universal schema. This schema is simply the union of all source schemas, and the probabilistic database learns how to predict the cells of each source relation in this union. For example, the database could store Freebase relations and relations that correspond to natural language surface patterns. The database would learn to predict what freebase relations hold true based on what surface patterns appear, and vice versa. We describe an analogy between such databases and collaborative filtering models, and use it to implement our paradigm with probabilistic PCA, a scalable and effective collaborative filtering method.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {116–121},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

@inproceedings{10.5555/2391200.2391223,
author = {Gordon, Jonathan and Schubert, Lenhart K.},
title = {Using Textual Patterns to Learn Expected Event Frequencies},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Commonsense reasoning requires knowledge about the frequency with which ordinary events and activities occur: How often do people eat a sandwich, go to sleep, write a book, or get married? This paper introduces work to acquire a knowledge base pairing factoids about such events with frequency categories learned from simple textual patterns. We are releasing a collection of the resulting event frequencies, which are evaluated for accuracy, and we demonstrate an initial application of the results to the problem of knowledge refinement.},
booktitle = {Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction},
pages = {122–127},
numpages = {6},
location = {Montreal, Canada},
series = {AKBC-WEKEX '12}
}

