<top>
<num> Number: 340122401
<title> Local Self-Attention over Long Text for Efficient Document Retrieval

<desc> Description:
Neural models have shown successful results in a number of IR tasks [9, 10, 17]. [23] proposed a kernel pooling approach (KNRM) based on a bag-of-words representation of words. This was further extended by [5] to incorporate n-gram representations using convolutional architecture. Several others [8, 14] have highlighted important considerations for designing neural ranking models for documents that are distinct from dealing with passages and other short text. [26] have emphasized on efficiency in neural ranking models and introduced neural models for retrieving documents from a large corpus.

<narr> Narrative:

</top>

<top>
<num> Number: 340122402
<title> Local Self-Attention over Long Text for Efficient Document Retrieval

<desc> Description:
[16] use pretrained contextual embeddings, without fine-tuning, in downstream ranking models.

<narr> Narrative:

</top>

<top>
<num> Number: 340122403
<title> Local Self-Attention over Long Text for Efficient Document Retrieval

<desc> Description:
Classically, assessing relevance of documents based on relevant parts has been studied in many forms [3, 21] and this study continues that exploration in the context of neural models. Unlike [16, 24], our proposed model is trained in a fully-supervised setting and only requires query-document relevance labels for training.

<narr> Narrative:

</top>
