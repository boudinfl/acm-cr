<top>
<num> Number: 340126601
<title> Reranking for Efficient Transformer-based Answer Selection

<desc> Description:
In this paper, we study and propose solutions to improve the efficiency and cost of modern QA systems based on search engines and Transformer models. Though we mainly focus on AS2, the proposed solution is general, and can be applied to other QA paradigms, including machine reading tasks. Our main idea follows the successful cascade approach for ad-hoc document retrieval [19], which considers fast but less accurate rerankers together with more accurate but slower models.

<narr> Narrative:

</top>

<top>
<num> Number: 340126602
<title> Reranking for Efficient Transformer-based Answer Selection

<desc> Description:
Neural models for AS2 typically apply a series of non-linear transformations to the input question and answer, represented as compositions of word or character embeddings and then measure the similarity between the obtained representations. For example, the Rel-CNN [16] has two separate embedding layers for the question and answer, and relational embedding, which aims at connecting them.

<narr> Narrative:

</top>

<top>
<num> Number: 340126603
<title> Reranking for Efficient Transformer-based Answer Selection

<desc> Description:
In contrast, we propose an alternative (and compatible with the initiatives above) approach following previous work in document retrieval, e.g., the use of sequential rerankers [19]. [21] focused on quickly identifying a set of good candidate documents to be passed to the second and further rerankers of the cascade. [4] proposed two stage approaches using a limited set of textual features and a final model trained using a larger set of query- and document-dependent features. [7] presented a new general framework for learning an end-to-end cascade of rankers using backpropagation. [1] studied effectiveness/efficiency trade-offs with three candidate selection approaches.

<narr> Narrative:

</top>
