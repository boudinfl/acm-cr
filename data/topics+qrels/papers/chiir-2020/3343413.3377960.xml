<doc>
	<doi>10.1145/3343413.3377960</doi>
	<title>Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks</title>
	<abstract>We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. An eye-tracking study was conducted where participants read short news articles, and rated them as relevant or irrelevant for answering a trigger question. We encode participants' eye-movement scanpaths as images, and then train a convolutional neural network classifier using these scanpath images. The trained classifier is used to predict participants' perceived-relevance of news articles from the corresponding scanpath images. This method is content-independent, as the classifier does not require knowledge of the screen-content, or the user's information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, this scanpath image classification method outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents.</abstract>
	<contexts>
		<context id="01" section="introduction">
			<s cites="46,47">Information relevance is one of the fundamental concepts in Information Science in general, and Information Retrieval (IR) in particular [46, 47].</s>
			<s>The primary purpose of IR systems is to fetch content which is useful and relevant to people.</s>
			<s>Understanding the cognitive processes of even one individual is challenging enough, and IR systems have to cater to a variety of users, who may have wildly different mental models of what they consider to be useful and relevant.</s>
			<s>To add another layer of complexity, these mental models are not static.</s>
			<s>They evolve as users' knowledge and information needs change.</s>
			<s>Researchers have investigated various forms of ‘signals' generated by users interacting with IR systems, that can serve as proxies for their mental processes.</s>
			<s>Examples include search queries, mouse-clicks, logs of viewed documents, and other forms of interaction-data. These proxies have been studied to infer what kind of information is relevant to users' needs.</s>
			<s>Efforts from a system-centred perspective have been towards minimizing the gap between the users' query and the documents retrieved.</s>
			<s>The search query is considered to be an exact representation of the users' information needs.</s>
			<s>Documents matching the query using a given algorithm are deemed to contain the information that users are searching for, and are therefore relevant.</s>
			<s cites="48">This notion of relevance is regarded as algorithmic-, or system-relevance [48].</s>
			<s>The limitation of this perspective is that the query is seldom an exact representation of what the user is looking for.</s>
			<s>As a result, retrieved documents often do not satisfy the user's information needs.</s>
		</context>
		<context id="02" section="introduction">
			<s cites="3">In a human-centred perspective, relevance arises from interactions between a user's information need and information objects [3].</s>
			<s cites="48,29">This interaction results in several manifestations of relevance [48], and becomes meaningful "only ... in relation to goals and tasks" [29].</s>
			<s>Our interest is in situational relevance, or utility.</s>
			<s cites="62" anonymised="true">As introduced by [62], "situationally relevant items of information are those that answer, or logically help to answer, questions of concern".</s>
			<s>In this paper, we refer to situational relevance as the users' perceived-relevance of the documents they examine for answering a question.</s>
		</context>
		<context id="03" section="introduction">
			<s>Neuro-physiological methods provide an interesting avenue to observe users while they interact with information systems.</s>
			<s>One popular method is eye-tracking.</s>
			<s>It captures the eye-movement patterns of users as they examine information on a screen.</s>
			<s>Eye-tracking has been frequently used to assess if the screen-content is relevant to the user (Section 2.1).</s>
			<s>The method has some distinct advantages.</s>
			<s>Eye-tracking is non-invasive, and requires minimum to no effort from the user.</s>
			<s>Even when users are not clicking the mouse or typing a query, they are viewing the screen, and thus helping to provide continuous data in a more natural setting.</s>
			<s>Eye-tracking can give insights about the focus and progression on an information searcher's attention in real-time.</s>
			<s cites="35">Eye-movements are sometimes considered to be a closer proxy for human cognition [35], than queries and interaction logs.</s>
		</context>
		<context id="04" section="related work">
			<s cites="44" anonymised="true">One of the earliest studies employing eye-tracking for inferring users' perceived-relevance was reported by [44].</s>
			<s>Participants saw a question and a list of ten sentences.</s>
			<s>One sentence had the correct answer to the question, and the others were either relevant or irrelevant to the question.</s>
			<s>Hidden Markov Models were used to predict the type of sentences the participants were reading.</s>
			<s>Many subsequent studies have investigated the relationship between eye-movements and viewing relevant vs. irrelevant information.</s>
			<s>These studies employed similar experimental setups, where participants examined a list of words, sentences, or documents, and judged their relevance in relation to a specific query or task.</s>
		</context>
		<context id="05" section="related work">
			<s>In a majority of these relevance assessment studies, a common theme is to collapse the stream of eye-movement data into a set of single-number features, at various levels of analysis (stimulus, trial, or participant level).</s>
			<s>These features are then used for statistical inferences, classification, and prediction.</s>
			<s cites="14,15,19,21,39,41,59,63">For instance, some variants of aggregated fixation-count and fixation-duration were used in studies reported in [14, 15, 19, 21, 39, 41, 59, 63].</s>
			<s cites="14" anonymised="true">Eye-dwell time and/or visit time was used by [14].</s>
			<s cites="43,25" anonymised="true">[43] identified a comprehensive list of 22 such features, which were later used by others (e.g., [25]).</s>
		</context>
		<context id="06" section="related work">
			<s>While fixation-count, fixation-duration, and dwell-time are generic eye-movement features applicable to any type of stimuli, several studies used specific features for reading text.</s>
			<s>These works first labelled each eye-fixations as either reading or scanning/skimming.</s>
			<s>Then they used derived measures from these two types of fixations.</s>
			<s cites="6" anonymised="true">[6] used reading-to-skimming ratio to infer when participants were reading relevant text.</s>
			<s cites="19,20,21,22" anonymised="true">Over a group of studies, [19–22] reported that reading speed, number of fixations on words, count and length of reading sequences, count and percentage of words fixated upon, durations of reading and scanning, and distance covered by scanning proved to be good indicators of perceived-relevance for textual documents.</s>
		</context>
		<context id="07" section="related work">
			<s>Research on non-textual relevance assessment have also used the approach of aggregated features.</s>
			<s cites="5,16,17,24,26,36,67,23,40,64">For instance, relevance of images have been studied in [5, 16, 17, 24, 26, 36, 67], while that of live webpages were studied in [23, 40, 64].</s>
			<s cites="22">Though most studies used aggregate features for the whole stimuli duration, the authors of [22] report that features from two-second windows near the end of viewing had more discriminating power than those obtained near the beginning of viewing.</s>
			<s>Thus, collapsing eye-tracking data and thereby losing temporal information, results in our reduced understanding of human relevance assessment.</s>
		</context>
		<context id="08" section="related work">
			<s>In terms of models used, most studies employed popular classifiers like Random Forests (RF) and Support Vector Machines (SVM).</s>
			<s cites="50,7">Few studies employed Hidden Markov Models [50] and Neural Networks [7].</s>
			<s>Performance was varied, based on the choice of features.</s>
			<s cites="64" anonymised="true">For instance, [64] predicted user-satisfaction while examining search results.</s>
			<s>They used advanced mathematical features (e.g., max. and SD of integrated curvature of fixations, using Frenet frame and Bishop frame) which are usually difficult to conceive in information science research.</s>
			<s>They obtained F1 scores in the range of 0.5 - 0.7 using RF and SVM.</s>
			<s cites="52" anonymised="true">[52] predicted web-surfer's click-intention from eye-tracking features.</s>
			<s>They used a battery of classifiers, but the F1 scores were not promising.</s>
			<s>Thus, appropriate feature selection is crucial to obtain good prediction performance when aggregating eye-tracking data.</s>
		</context>
		<context id="09" section="related work">
			<s>Summarily, we see that use of aggregated eye-tracking features and traditional classification techniques resulted in unpromising performances for relevance prediction.</s>
			<s cites="23,50,52,59">While statistical tests were significant at the p &lt; .01 level, the classification and prediction accuracies were rarely more than 70% [23, 50, 52, 59].</s>
			<s>In our proposed method, we demonstrate that utilizing the entire eye-tracking data, and applying image classification technique, we can predict perceived-relevance with up to 80% accuracy.</s>
		</context>
		<context id="10" section="related work">
			<s cites="11,33,1,30">Several scanpath comparison algorithms have been proposed, which either use (a) the actual fixation points from the eye-movement trajectory [11, 33], or, (b) a string representation of the trajectory, using letter-labels to categorize each fixation [1, 30].</s>
			<s>The first approach works only with scanpaths having equal number of fixations.</s>
			<s>To deal with scanpaths having diferring number of fixations, the algorithm deletes or clusters some fixations together (simplification step) such that all scanpaths have identical number of fixations.</s>
			<s>We argue that such an approach may work well for non-reading tasks (e.g. viewing images), but for analyzing eye-movement while reading, all fixation points should be preserved.</s>
			<s>Nearby fixations on different distinct words should not be clustered together into one fixation, as they may contain important information pertinent to the reading task.</s>
			<s cites="4,12,10,60">The second scanpath comparison approach uses a string representation of the two scanpaths, and compares them using either the Levenshtein distance [4, 12] or the Needleman-Wunsch algorithm [10, 60].</s>
			<s>This method assumes that annotated data is available for all the fixations.</s>
			<s>However, such annotations are not available when we do not have pre-existing insights about the eye-movements for our task.</s>
			<s>A common limitation of both the methods is that they work for pairwise comparisons only, and cannot be easily extended to compare between groups of scanpaths.</s>
		</context>
		<context id="11" section="related work">
			<s>As introduced in Section 1, we propose an image-classification approach for predicting perceived-relevance.</s>
			<s>Over the last decade, image classification, and computer vision in general, has seen tremendous improvement by starting to re-use the Convolutional Neural Network (CNN).</s>
			<s>Although developed in the 1970s, CNNs did not play a major role in computer vision research until 2010s, due to lack of adequate computing capabilities for fast execution.</s>
			<s cites="8" anonymised="true">In 2012, [8] applied max-pooling operation after convolution, using dedicated hardware GPUs.</s>
			<s>This process significantly improved the benchmark performances of numerous computer vision algorithms.</s>
			<s cites="42">Around the same time, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [42] began to be organized annually.</s>
			<s>The goal of the challenge was to beat previous years' top-performances for object recognition tasks on more than 14 million annotated images.</s>
			<s>Various research institutions began participating in the challenge, and the competition spearheaded the emergence of high-performing CNN architectures that began to be regarded as benchmarks.</s>
			<s cites="51,31,27,28,56,57,55">Examples of such benchmark architectures are VGG [51], DenseNet [31], ResNet [27, 28], Inception [56, 57] and InceptionResNet (combination of Inception and ResNet architectures) [55].</s>
		</context>
		<context id="12" section="related work">
			<s>An interesting feature of CNN based image-classifiers is that the ‘knowledge' learnt by the network for solving one problem can be reused to solve another related problem.</s>
			<s>This is called transfer learning.</s>
			<s cites="37,65,66">The initial layers of a CNN based image classifier learns low-level image-features (edges, shapes, and corners), while the final layers learn increasingly abstract and task specific features [37, 65, 66].</s>
			<s>Since low-level image-feature detection is required in all forms of automated image-understanding, transfer learning works well for research problems having relatively low-sized datasets.</s>
			<s>For this reason, popular deep-learning frameworks (e.g., Keras, PyTorch etc.) include many benchmark CNN architectures, with their weights pre-trained on the ImageNet challenge.</s>
			<s>In this work, we utilize several such benchmark CNN image classifiers to predict the perceived-relevance of documents from scanpath images.</s>
		</context>
		<context id="13" section="related work">
			<s>A CNN is often considered as a "black-box", because its inner working are not easily understandable.</s>
			<s cites="49,53">Various methods have been proposed to understand why the network makes a particular prediction [49, 53].</s>
			<s cites="49">One such method is Gradient-weighted Class Activation Mapping (Grad-CAM) [49].</s>
			<s>The Grad-CAM method produces a heatmap, which is similar to an attention map, and highlights the regions of the input image that was focused on for making the prediction.</s>
			<s>For ‘known‘ research problems (e.g. detecting cats vs. dogs in images) this visualization helps to understand whether the CNN is paying attention to the relevant image regions.</s>
			<s>In our case of classifying scanpath images according to perceived-relevance, the Grad-CAM visualizations can offer new insights about human reading behaviour on relevant and irrelevant documents.</s>
		</context>
	</contexts>
	<references>
		<reference id="1">10.3758/s13428-014-0550-3</reference>
		<reference id="2">10.1016/B978-0-12-812133-7.00004-1</reference>
		<reference id="3">10.1002/asi.10286</reference>
		<reference id="4">10.1162/jocn.1997.9.1.27</reference>
		<reference id="5">10.1007/978-3-642-02812-0_39</reference>
		<reference id="6">10.1145/1358628.1358796</reference>
		<reference id="7">10.1109/CogInfoCom.2015.7390631</reference>
		<reference id="8">10.1109/CVPR.2012.6248110</reference>
		<reference id="9">10.1162/tacl_a_00018</reference>
		<reference id="10">10.3758/BRM.42.3.692</reference>
		<reference id="11">10.3758/s13428-012-0212-2</reference>
		<reference id="12">10.1145/1743666.1743719</reference>
		<reference id="13">https://www.splunk.com/blog/2017/04/18/deeplearning-with-splunk-and-tensorflow-for-security-catching-the-fraudsterin-neural-networks-with-behavioral-biometrics.html</reference>
		<reference id="14">10.1007/978-3-642-24958-7_17</reference>
		<reference id="15">10.3389/fnsys.2013.00039</reference>
		<reference id="16">10.1007/978-3-319-24917-9_8</reference>
		<reference id="17">10.1088/1367-2630/12/12/125011</reference>
		<reference id="18">None</reference>
		<reference id="19">10.1145/2637002.2637011</reference>
		<reference id="20">10.1145/2578153.2578198</reference>
		<reference id="21">10.1007/978-3-319-41402-7_18</reference>
		<reference id="22">10.5555/3204593.3204595</reference>
		<reference id="23">10.1145/2766462.2767795</reference>
		<reference id="24">10.1145/1878061.1878070</reference>
		<reference id="25">http://proceedings.mlr.press/v2/hardoon07a/hardoon07a.pdf</reference>
		<reference id="26">10.1145/1743666.1743734</reference>
		<reference id="27">10.1109/CVPR.2016.90</reference>
		<reference id="28">10.1007/978-3-319-46493-0_38</reference>
		<reference id="29">10.5555/1711713.1711733</reference>
		<reference id="30">https://lup.lub.lu.se/record/1852359</reference>
		<reference id="31">10.1109/CVPR.2017.243</reference>
		<reference id="32">10.1109/MCSE.2007.55</reference>
		<reference id="33">10.1145/1743666.1743718</reference>
		<reference id="34">10.3390/s18040966</reference>
		<reference id="35">https://psycnet.apa.org/record/1986-98384-000</reference>
		<reference id="36">10.1145/1460096.1460120</reference>
		<reference id="37">10.5555/2999134.2999257</reference>
		<reference id="38">10.1145/3269206.3271764</reference>
		<reference id="39">10.1145/1943403.1943431</reference>
		<reference id="40">10.1016/j.neucom.2015.05.108</reference>
		<reference id="41">10.1145/1390156.1390252</reference>
		<reference id="42">10.1007/s11263-015-0816-y</reference>
		<reference id="43">http://research.ics.aalto.fi/events/inips2005/inips2005proceedings.pdf#page=45</reference>
		<reference id="44">10.1007/11550822_80</reference>
		<reference id="45">10.1145/3239235.3267427</reference>
		<reference id="46">10.5555/1315930.1315946</reference>
		<reference id="47">10.2200/S00723ED1V01Y201607ICR050</reference>
		<reference id="48">None</reference>
		<reference id="49">10.1109/ICCV.2017.74</reference>
		<reference id="50">10.1016/j.cogsys.2008.01.002</reference>
		<reference id="51">https://arxiv.org/abs/1409.1556</reference>
		<reference id="52">10.1016/j.inffus.2016.09.003</reference>
		<reference id="53">https://arxiv.org/abs/1412.6806</reference>
		<reference id="54">10.1016/B978-0-12-810408-8.00003-1</reference>
		<reference id="55">10.5555/3298023.3298188</reference>
		<reference id="56">10.1109/CVPR.2015.7298594</reference>
		<reference id="57">10.1109/CVPR.2016.308</reference>
		<reference id="58">https://trec.nist.gov/pubs/trec11/papers/OVERVIEW.11.pdf</reference>
		<reference id="59">10.1088/1741-2552/aa7590</reference>
		<reference id="60">10.1145/1117309.1117360</reference>
		<reference id="61">10.1016/S0166-4115(08)61814-2</reference>
		<reference id="62">10.1016/0020-0271(73)90096-X</reference>
		<reference id="63">10.3389/fpsyg.2016.01790</reference>
		<reference id="64">10.1002/asi.24240</reference>
		<reference id="65">https://arxiv.org/abs/1506.06579</reference>
		<reference id="66">10.1007/978-3-319-10590-1_53</reference>
		<reference id="67">10.1145/1743666.1743674</reference>
	</references>
</doc>