<top>
<num> Number: 340981901
<title> Permutation Equivariant Document Interaction Network for Neural Learning-to-Rank

<desc> Description:
Recently, neural network based approaches have proven effective for LTR applications [4, 14, 15]. In this context, we formally define the permutation equivariance requirement for a scoring func- tion that models cross-document interactions.

<narr> Narrative:

</top>

<top>
<num> Number: 340981902
<title> Permutation Equivariant Document Interaction Network for Neural Learning-to-Rank

<desc> Description:
There are two settings for modeling cross-document interactions: re-ranking and full ranking. In the former setting, a base ranking is provided and the documents are reordered using the ranker. For example, [1] applies sequence modeling on the top k documents of the base ranking and then uses the final state vector to enrich each document for the re-ranking scoring. In the latter setting, we do not have a base ranking but start with a set of documents. For example, [6] takes a pair of documents as input and uses a DNN to produce a preference score for the input documents, while [2] propose a groupwise scoring function to model document interactions, sampling a subset of all permutations of each group and thus not guaranteeing permutation equivariance.

<narr> Narrative:

</top>

<top>
<num> Number: 340981903
<title> Permutation Equivariant Document Interaction Network for Neural Learning-to-Rank

<desc> Description:
While much of the research in LTR has been devoted to the evolution of ranking loss functions [11], the nature of the learned scoring function has largely remained the same: a univariate scoring function that computes a relevance score for a document in isolation.

<narr> Narrative:

</top>

<top>
<num> Number: 340981904
<title> Permutation Equivariant Document Interaction Network for Neural Learning-to-Rank

<desc> Description:
Most of the previous work in LTR [11] focuses on designing loss functions, ranging from pointwise to pairwise to listwise ones. Gradient Boosted Decision Trees (e.g., [10]) are regarded as the state-of-the-art models for LTR on benchmark datasets. Recently, neural network based models have attracted considerable attention [8, 12].

<narr> Narrative:

</top>
