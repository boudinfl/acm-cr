<doc>
	<doi>10.1145/3397271.3401467</doi>
	<title>Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances</title>
	<abstract>Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users' real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.</abstract>
	<contexts>
		<context id="01" section="introduction">
			<s cites="4">Recent years have witnessed the increased popularity and explosive growth of the World Wide Web, which has generated huge amounts of data and leaded to progressively severe information overload problem [4].</s>
			<s>In consequence, how to extract information (products or services) that satisfy users’ information requirements at the proper time and place has become increasingly important.</s>
			<s>This motivates several information retrieval mechanisms, such as search, recommendation, and online advertising.</s>
			<s cites="10">These retrieval mechanisms could generate a set of objects that best match users’ explicit or implicit preferences [10].</s>
			<s cites="21">Efforts have been made on developing supervised or unsupervised methods for these information retrieval mechanisms [21].</s>
			<s cites="41">However, since the widely use of mobile applications during the recent years, more and more information retrieval services have provided interactive functionality and products [41], these conventional techniques typically face several common challenges.</s>
			<s>First, most of traditional approaches consider information retrieval tasks in a static environment and extract information via a fixed greedy strategy, which may fail to catch the dynamic characteristics of users’ preferences (or environment).</s>
			<s cites="24">Second, the majority of existing methods aims to maximize user’s immediate satisfaction, while completely overlooking whether the generate information can benefit more to user’s preference in the long run [24].</s>
			<s cites="27">Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning [27].</s>
		</context>
		<context id="02" section="introduction">
			<s cites="25,26,16,17,14">Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].</s>
			<s>By integrating deep learning into reinforcement learning, DRL is not only capable of continuing sensing and learning to act, but also capturing complex patterns with the power of deep learning.</s>
			<s>Under the DRL schema, complex problems are addressed by acquiring experiences through interactions with a dynamic environment.</s>
			<s cites="13">The result is an optimal policy that can provide decision making solutions to complex tasks without any specific instructions [13].</s>
			<s>Introducing DRL to information retrieval community can naturally tackle the above-mentioned challenges.</s>
			<s>First, DRL considers information retrieval tasks as sequential interactions between an RL agent (system) and users (environment), where the agent continuously update the information retrieval strategies based on users’ real-time feedback, so as to generate information best match users’ dynamic preferences.</s>
			<s>Second, the DRL-based techniques targets to optimize users’ long-term satisfaction or engagement.</s>
			<s>Therefore, the agent could identify information to achieve the trade-off between users’ short-term and long-term satisfaction.</s>
		</context>
		<context id="03" section="introduction">
			<s cites="3,5,11,12,15,38,39,40">Given the advantages of reinforcement learning, there have been tremendous interests in developing RL based information retrieval techniques [3, 5, 11, 12, 15, 38–40].</s>
			<s>While these successes show the promise of DRL, applying learning from game-based DRL to information retrieval is fraught with unique challenges, including, but not limited to, extreme data sparsity, power-law distributed samples, and large state and action spaces.</s>
		</context>
		<context id="04" section="related work">
			<s cites="7,29">Search targets at retrieving and ranking a set of items (e.g. documents, records) according to a user query [7, 29].</s>
			<s cites="19" anonymised="true">For query understanding, [19] proposed a reinforcement learning based query reformulation task, which maximizes the number of recalled relevant documents via rewriting a query.</s>
			<s>In [18], a multi-agent reinforcement learning framework is proposed to increase the diverse query reformulation efficiency, where each agent learns a local policy that performs well on a subset of examples, and all agents are trained with parallelism to make the learning faster.</s>
			<s>For relevance ranking, conventional methods typically optimize the evaluation metric before a predefined position (e.g. NDCG@K), which ignores the information after rank K.</s>
			<s cites="31">MDPRank [31] is proposed to address this problem by using the metrics calculated upon all the positions as reward function, and the model parameters are be optimized via maximizing the accumulated rewards for all decisions.</s>
			<s cites="8,22">Beyond relevance ranking, another important goal is to increase the diversity of search results [8, 22], which needs to capture the utility of information users have perceived from the preceding documents in a sequential document selection.</s>
			<s cites="34">MDP-DIV [34] formalized diverse ranking as a continuous state Markov decision process, and policy gradient algorithm of REINFORCE is leveraged to maximize the accumulated long-term rewards in terms of the diversity metric.</s>
		</context>
		<context id="05" section="related work">
			<s>Recommender systems aim to learn users’ preferences based on their feedback and suggest items to match their preferences.</s>
			<s>User’s preference is assumed to be static in traditional recommendation algorithms such as collaborative filtering, which is usually not true in real-world recommender systems where users’ preferences are highly dynamic.</s>
			<s cites="33,37">Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions).</s>
			<s cites="6,9,42">Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over time.</s>
			<s cites="39">In [39], a user’s dynamic preference (state) is learned from her browsing history and feedback.</s>
			<s>Each time a user provides feedback (skip, click or purchase) to an item, the recommender system will update the state to capture user’s new preferences.</s>
			<s>Conventional recommender algorithms also suffer from the exploitation-exploration dilemma, where exploitation is to suggest items that best match users’ preferences, while exploration is to randomly suggest items to mine more users’ possible preferences.</s>
			<s cites="30,2,1">The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].</s>
		</context>
		<context id="06" section="related work">
			<s>Online advertising is to suggest the right ads to the right users so as to maximize the click-through rate (CTR) or return on investment (ROI) of the advertising campaign, which consists of two main marketing strategy, i.e., guaranteed delivery (GD) and real-time bidding (RTB).</s>
			<s cites="23">In guaranteed delivery setting, ads that grouped into campaigns are charged on a pay-per-campaign basis for the pre-specified number of deliveries [23].</s>
			<s cites="32">A multi-agent reinforcement learning approach [32] is proposed to derive cooperative policies for the publisher, where impression allocation problem is formulated as an auction problem, and publishers can submit virtual bids for impressions.</s>
			<s>In Real-Time Bidding setting, an advertiser submits a bid for each impression in a very short time frame.</s>
			<s cites="20,28,35,36">The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.</s>
			<s>Thus, the MDP setting is introduced.</s>
			<s cites="3">For example, a model-based RL framework is proposed in RTB setting [3], where the state value is approximated by neural network to address the scalability problem of large auction amounts and the limited budget.</s>
			<s cites="12">In [12], a multi-agent bidding model is proposed to jointly consider all the advertisers’ biddings in the system, and a clustering approach is introduced to deal with a large number of advertisers.</s>
		</context>
	</contexts>
	<references>
		<reference id="1">10.1023/A:1013689704352</reference>
		<reference id="2">10.1137/S0097539701398375</reference>
		<reference id="3">10.1145/3018661.3018702</reference>
		<reference id="4">10.1109/TKDE.2006.152</reference>
		<reference id="5">10.1609/aaai.v33i01.33013312</reference>
		<reference id="6">10.1145/3289600.3290999</reference>
		<reference id="7">10.1145/1924475.1924485</reference>
		<reference id="8">10.1145/1860702.1860709</reference>
		<reference id="9">https://arxiv.org/abs/1808.08013</reference>
		<reference id="10">10.1145/2018396.2018423</reference>
		<reference id="11">https://arxiv.org/abs/1803.07347</reference>
		<reference id="12">10.1145/3269206.3272021</reference>
		<reference id="13">10.5555/1622737.1622748</reference>
		<reference id="14">10.1177/0278364915619772</reference>
		<reference id="15">https://arxiv.org/abs/1810.12027</reference>
		<reference id="16">http://proceedings.mlr.press/v48/mniha16.pdf</reference>
		<reference id="17">https://arxiv.org/abs/1312.5602</reference>
		<reference id="18">https://arxiv.org/abs/1809.10658</reference>
		<reference id="19">10.18653/v1/D17-1061</reference>
		<reference id="20">https://ojs.aaai.org/index.php/AAAI/article/view/11888/11747</reference>
		<reference id="21">10.1145/3233770</reference>
		<reference id="22">http://infosense.cs.georgetown.edu/publication/mdp-ol-extended-abstract.pdf</reference>
		<reference id="23">10.1145/2396761.2398561</reference>
		<reference id="24">10.5555/1046920.1088715</reference>
		<reference id="25">10.1038/nature16961</reference>
		<reference id="26">10.1038/nature24270</reference>
		<reference id="27">10.1109/tnn.1998.712192</reference>
		<reference id="28">10.1145/2505515.2514700</reference>
		<reference id="29">https://trec.nist.gov/pubs/trec26/papers/georgetown-DD.pdf</reference>
		<reference id="30">https://www.researchgate.net/profile/Christopher-Watkins-4/publication/33784417_Learning_From_Delayed_Rewards/links/53fe12e10cf21edafd142e03/Learning-From-Delayed-Rewards.pdf</reference>
		<reference id="31">10.1145/3077136.3080685</reference>
		<reference id="32">https://arxiv.org/abs/1809.03152</reference>
		<reference id="33">10.1145/3209978.3210051</reference>
		<reference id="34">10.1145/3077136.3080775</reference>
		<reference id="35">http://papers.nips.cc/paper/5146-estimation-bias-in-multi-armed-bandit-algorithms-for-search-advertising.pdf</reference>
		<reference id="36">10.1109/ICDM.2016.0177</reference>
		<reference id="37">10.1145/2939672.2939878</reference>
		<reference id="38">10.1145/3240323.3240374</reference>
		<reference id="39">10.1145/3219819.3219886</reference>
		<reference id="40">https://arxiv.org/abs/1801.00209</reference>
		<reference id="41">10.1145/2505515.2505690</reference>
		<reference id="42">10.1145/3394486.3403384</reference>
	</references>
</doc>