<doc>
	<doi>10.1145/3397271.3401032</doi>
	<title>Measuring Recommendation Explanation Quality: The Conflicting Goals of Explanations</title>
	<abstract>Explanations have a large effect on how people respond to recommendations. However, there are many possible intentions a system may have in generating explanations for a given recommendation -from increasing transparency, to enabling a faster decision, to persuading the recipient. As a good explanation for one goal may not be good for others, we address the questions of (1) how to robustly measure if an explanation meets a given goal and (2) how the different goals interact with each other. Specifically, this paper presents a first proposal of how to measure the quality of explanations along seven common goal dimensions catalogued in the literature. We find that the seven goals are not independent, but rather exhibit strong structure. Proposing two novel explanation evaluation designs, we identify challenges in evaluation, and provide more efficient measurement approaches of explanation quality.</abstract>
	<contexts>
		<context id="01" section="introduction">
			<s>Recommendations are part of everyday life.</s>
			<s>Be they made by a person, or by an automated system, the recommendations are often accompanied with an explanation, or reason, underlying the suggestions provided.</s>
			<s cites="13,14,23,28">Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.</s>
		</context>
		<context id="02" section="introduction">
			<s>At the same time, automated recommender systems have recently proliferated.</s>
			<s cites="1,18">This has increased attention on explainable and transparent AI, both from technical and ethical perspectives [1, 18].</s>
			<s cites="5,29">While explainable system design is not new (dating back to rule-based expert systems of the 1980s [5]), the role of explanations has gained more attention in the past decade [29].</s>
		</context>
		<context id="03" section="">
			<s cites="26" anonymised="true">Our work starts with seven main goals of explanations, proposed by [26]: transparency, intended to explain how the system works; scrutability, allowing users to tell the system if it is wrong; trust, increasing users’ confidence in the system; effectiveness, helping users to make good decisions; efficiency, helping users to make decisions faster; persuasiveness, trying to convince users to select the given item; and satisfaction, increasing the ease of use of a system.</s>
			<s cites="26">They argued that these goals should be identified as distinct, even if they may interact [26].</s>
			<s cites="20,8,13,26">Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26].</s>
			<s>Yet, depending on the perspective of the explanation generator, different goals may be appropriate, and may need to be traded off.</s>
		</context>
		<context id="04" section="related work">
			<s cites="13,14,23,28">The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28].</s>
			<s cites="20">Explanations can be characterized along a number of dimensions, including their content, form of presentation, and system’s intended purpose [20].</s>
			<s>Our interest is in the latter category, where we use the term goal to refer to the objective or purpose of the explanation.</s>
			<s cites="20,2,6,19">Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19].</s>
		</context>
		<context id="05" section="related work">
			<s cites="26">We use the seven explanation goals identified in [26] as a basis; these are listed in Table 1.</s>
			<s>We note that there are possible refinements to these goals.</s>
			<s cites="20">For example, in [20] satisfaction is not considered as a single objective, but is split into ease to use, enjoyment, and usefulness.</s>
			<s>Nonetheless, these seven goals are regarded as the canonical categorization within explainability research for recommender systems, accurately reflecting the goals that have been studied in the past.</s>
			<s>Certain goals may be measured objectively and quantitatively.</s>
			<s cites="3,6,13,30">For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].</s>
			<s>Here, we aim to compare different goals on equal footing, and thus focus on the subjective perception of the recipient—measured at the time when a recommendation and explanation are shown.</s>
		</context>
		<context id="06" section="related work">
			<s cites="20,26">Most past studies are concerned with a single goal [20], and there is evidence each can be achieved individually [26].</s>
			<s>The interactions between two or more goals, however, are much less understood.</s>
			<s cites="20,12" anonymised="true">The most common explanation purpose, according to a large-scale literature review by [20], is transparency, which is also considered key to building user trust [12].</s>
			<s cites="23,8">Concerning the relationship between the two, one previous study indicates that transparency increases user trust [23], while another study finds that transparency and trust are not related [8].</s>
			<s cites="20,7">The second most frequent explanation purpose is effectiveness [20], which can be conflicting with persuasiveness [7].</s>
			<s>A systematic evaluation of explanations with respect to all goals has not been performed before.</s>
		</context>
		<context id="07" section="related work">
			<s cites="19,27">There is an important recognized difference between explanations (why a certain suggestion is given) and justifications (why the user may be interested in the item) [19, 27].</s>
			<s>The former consist of an honest account of the mechanism that generated the suggestion, while the latter provides a plausible reason, which may be decoupled from the underlying recommendation algorithm.</s>
		</context>
		<context id="08" section="related work">
			<s>There is a growing interest in generating natural language explanations and justifications.</s>
			<s cites="24,19">Given a sophisticated recommendation system, justifications may often be provided by filling in natural language templates, for example, by considering simple features such as actor and director names [24] or by extracting relevant and distinguishing characteristics from reviews [19].</s>
			<s>However, our work focuses on explanations.</s>
			<s cites="6">Justifications have in the past been created manually using crowdsourcing [6].</s>
			<s cites="6">A main difference between that and ours, is that we ask humans to pick the recommendation as well as explain it, while [6] perform only the latter.</s>
		</context>
		<context id="09" section="related work">
			<s cites="6,8,10,17,21,22,23">Subjective perceptions of explanations are often evaluated qualitatively based on user surveys, with responses typically given on Likert scales [6, 8, 10, 17, 21–23].</s>
			<s>Following standard practice, we design a user survey to capture the subjective perception of users regarding the seven goals.</s>
		</context>
	</contexts>
	<references>
		<reference id="1">10.1145/3173574.3174156</reference>
		<reference id="2">10.1145/3331184.3331211</reference>
		<reference id="3">https://www.cs.utexas.edu/~ml/papers/submit.pdf</reference>
		<reference id="4">10.1145/3109859.3109893</reference>
		<reference id="5">10.5555/1096485</reference>
		<reference id="6">10.1145/2959100.2959153</reference>
		<reference id="7">10.1145/2567948.2577276</reference>
		<reference id="8">10.1007/s11257-008-9051-3</reference>
		<reference id="9">10.1007/BF02310555</reference>
		<reference id="10">https://ses.library.usyd.edu.au/bitstream/handle/2123/10206/2006_Marek_Czarkowski_thesis.pdf</reference>
		<reference id="11">https://psycnet.apa.org/record/1991-98125-000</reference>
		<reference id="12">10.1007/978-3-642-23014-1_17</reference>
		<reference id="13">10.1016/j.ijhcs.2013.12.007</reference>
		<reference id="14">10.1145/358916.358995</reference>
		<reference id="15">10.1145/1357054.1357127</reference>
		<reference id="16">10.1145/2047196.2047202</reference>
		<reference id="17">10.1145/3109859.3109915</reference>
		<reference id="18">10.1145/3276742</reference>
		<reference id="19">10.1145/3320435.3320457</reference>
		<reference id="20">10.1007/s11257-017-9195-0</reference>
		<reference id="21">10.1145/1111449.1111475</reference>
		<reference id="22">10.1145/3172944.3173012</reference>
		<reference id="23">10.1145/506443.506619</reference>
		<reference id="24">http://delab.csd.auth.gr/papers/WEBKDD08snm.pdf</reference>
		<reference id="25">10.1007/s11257-011-9117-5</reference>
		<reference id="26">10.1007/978-1-4899-7637-6_10</reference>
		<reference id="27">10.1145/1502650.1502661</reference>
		<reference id="28">10.2307/249686</reference>
		<reference id="29">10.1561/1500000066</reference>
		<reference id="30">10.1145/2600428.2609579</reference>
	</references>
</doc>