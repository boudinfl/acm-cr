<top>
<num> Number: 3371820-1
<title> However, in the personal search setting [2, 5, 12, 28], where both queries and documents are private, building a representative validation set is a challenge, as it involves either relying upon donated data (which can be limited and biased) or utilizing a validation set that only contains partial information (where any sensitive raw data is not logged).

<desc> Description:

<narr> Narrative:

</top>

<top>
<num> Number: 3371820-2
<title> [7, 13] discuss the importance of parameter tuning, and [27] gives more insight into the difficulty of parameter management. Since new features are often added to search systems to improve the performance of retrieval functions, these features often increase the number of parameters which makes it more challenging to find optimal parameter values.

<desc> Description:

<narr> Narrative:

</top>

<top>
<num> Number: 3371820-3
<title> The majority of work regarding parameter tuning is for black-box systems, where no knowledge of the internal implementation is known. For these approaches, given an unknown function f, we are allowed to evaluate f(x) for any value x ∈ X in an effort to maximize (or minimize) f(x). Typical approaches include Bayesian optimization [11, 17] as well as hybrid methods using random search alongside multi-armed bandit techniques [19]. In particular, [17] discusses how Bayesian optimization can be used for optimizing retrieval systems. These approaches generally assume some prior belief over f and draw samples to get a posterior that better approximates f.

<desc> Description:

<narr> Narrative:

</top>

<top>
<num> Number: 3371820-4
<title> Offline evaluation is critical for production search systems to sanity check any new experiments and prevent system behavior that may be detrimental to the users’ experience. [14] use previously collected click data in order to perform interleaved comparison methods between various rankers and find that it can be less expensive than running live interleaving experiments. [18] provides a method of doing offline evaluation of different contextual bandit based article recommendations, where a primary motivation is to not hurt user experience by exposing potentially poor-quality algorithms.

<desc> Description:

<narr> Narrative:

</top>
