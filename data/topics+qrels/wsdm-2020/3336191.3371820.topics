<top>
<num> Number: 337182001
<title> Parameter Tuning in Personal Search Systems

<desc> Description:
Offline tuning using a validation set is another option [21]. For instance, as the retrieval component of a search system usually works with query strings and document contents directly, offline tuning in this setting involves collecting raw queries and historical click data and simulating how retrieval effectiveness changes when utilizing different parameters. However, in the personal search setting [2, 5, 12, 28], where both queries and documents are private, building a representative validation set is a challenge, as it involves either relying upon donated data (which can be limited and biased) or utilizing a validation set that only contains partial information (where any sensitive raw data is not logged).

<narr> Narrative:

</top>

<top>
<num> Number: 337182002
<title> Parameter Tuning in Personal Search Systems

<desc> Description:
[7, 13] discuss the importance of parameter tuning, and [27] gives more insight into the difficulty of parameter management. Since new features are often added to search systems to improve the performance of retrieval functions, these features often increase the number of parameters which makes it more challenging to find optimal parameter values.

<narr> Narrative:

</top>

<top>
<num> Number: 337182003
<title> Parameter Tuning in Personal Search Systems

<desc> Description:
The majority of work regarding parameter tuning is for black-box systems, where no knowledge of the internal implementation is known. For these approaches, given an unknown function f, we are allowed to evaluate f(x) for any value x ∈ X in an effort to maximize (or minimize) f(x). Typical approaches include Bayesian optimization [11, 17] as well as hybrid methods using random search alongside multi-armed bandit techniques [19]. In particular, [17] discusses how Bayesian optimization can be used for optimizing retrieval systems. These approaches generally assume some prior belief over f and draw samples to get a posterior that better approximates f.

<narr> Narrative:

</top>

<top>
<num> Number: 337182004
<title> Parameter Tuning in Personal Search Systems

<desc> Description:
Offline evaluation is critical for production search systems to sanity check any new experiments and prevent system behavior that may be detrimental to the users’ experience. [14] use previously collected click data in order to perform interleaved comparison methods between various rankers and find that it can be less expensive than running live interleaving experiments. [18] provides a method of doing offline evaluation of different contextual bandit based article recommendations, where a primary motivation is to not hurt user experience by exposing potentially poor-quality algorithms.

<narr> Narrative:

</top>

<top>
<num> Number: 337182005
<title> Parameter Tuning in Personal Search Systems

<desc> Description:
Whereas retrieval work in the literature primarily studies approaches such as BM25 [26], search engines in industry have an entirely different set of challenges. Commercial web search engines have been developed for decades by many engineers. The various search system components quickly become complicated and typically have numerous parameters to be tuned. As [7, 13] note that even small variations in parameters of information retrieval systems lead to large variations in retrieval effectiveness, it motivates thoroughly tuning parameters.

<narr> Narrative:

</top>
