<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta/>
    <article-meta>
      <title-group>
        <article-title>Choppy: Cut Transformer for Ranked List Truncation</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Dara Bahri∗</string-name>
          <email>dbahri@google.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Yi Tay</string-name>
          <email>yitay@google.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Che Zheng</string-name>
          <email>chezheng@google.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Donald Metzler</string-name>
          <email>metzler@google.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Andrew Tomkins</string-name>
          <email>tomkins@google.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Google Research</institution>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2020</year>
      </pub-date>
      <fpage>25</fpage>
      <lpage>30</lpage>
      <abstract>
        <p>Work in information retrieval has traditionally focused on ranking and relevance: given a query, return some number of results ordered by relevance to the user. However, the problem of determining how many results to return, i.e. how to optimally truncate the ranked result list, has received less attention despite being of critical importance in a range of applications. Such truncation is a balancing act between the overall relevance, or usefulness of the results, with the user cost of processing more results. In this work, we propose Choppy, an assumption-free model based on the widely successful Transformer architecture, to the ranked list truncation problem. Needing nothing more than the relevance scores of the results, the model uses a powerful multi-head attention mechanism to directly optimize any user-defined IR metric. We show Choppy improves upon recent state-of-the-art methods.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>CCS CONCEPTS</title>
      <p>• Information systems → Presentation of retrieval results;
Retrieval efectiveness.</p>
    </sec>
    <sec id="sec-2">
      <title>INTRODUCTION</title>
      <p>
        While much of the work in information retrieval has been centered
around ranking, there is growing interest in methods for ranked
list truncation - the problem of determining the appropriate cutof
 of candidate results [
        <xref ref-type="bibr" rid="ref1 ref9">1, 9</xref>
        ]. This problem has garnered attention
∗Corresponding author
in fields like legal search [
        <xref ref-type="bibr" rid="ref14">14</xref>
        ] and sponsored search [
        <xref ref-type="bibr" rid="ref16 ref3">3, 16</xref>
        ], where
there could be a monetary cost for users looking into an irrelevant
tail of documents or where showing too many irrelevant ads could
result in ad blindness. The fundamental importance of this problem
has led to development of methods that are automatically able to
learn  in a data-driven fashion [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ]. The focus of this paper is to
design more efective models for accurate and dynamic truncation
of ranked lists.
      </p>
      <p>
        The present state-of-the-art for this task is BiCut [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ], a
recurrentbased neural model that formulates the problem as a sequential
decision process over the list. BiCut trains a bidirectional LSTM
[
        <xref ref-type="bibr" rid="ref8">8</xref>
        ] model with a predefined loss that serves as a proxy to the
userdefined target evaluation metric. At every position in the ranked
list, BiCut makes a binary decision conditioned on both forward
and backward context: to continue to the next position, or to end
the output list.
      </p>
      <p>
        While BiCut outperforms non-neural methods [
        <xref ref-type="bibr" rid="ref1">1</xref>
        ], we argue it
has several drawbacks. Firstly, the model is trained with
teacherforcing, i.e. with ground truth context, but it is deployed
autoregressively at test time, where it is conditioned on its own
predictions. Thus, the model sufers from a train / test distribution
mismatch, often referred to as exposure bias [
        <xref ref-type="bibr" rid="ref12">12</xref>
        ], resulting in poor
model generalization. Secondly, the loss function used does not
capture the mutual exclusivity among the candidate cut positions.
In other words, the loss does not capture the condition that the list
can only be cut in at most one position. Furthermore, the proposed
training loss is unaligned with the user-defined evaluation metric.
Last but not least, BiCut employs BiLSTMs which are not only slow
and non-parallelizable, but also do not take into account global
long-range dependencies.
      </p>
      <p>
        This paper proposes Choppy, a new method that not only
ameliorates the limitations of the BiCut model but also achieves
state-ofthe-art performance on the ranked list truncation task. Our method
comprises two core technical contributions. The first is a
Transformer model [
        <xref ref-type="bibr" rid="ref15">15</xref>
        ] that is able to capture long-range dyadic
interactions between relevance scores. To the best of our knowledge,
this is the first successful application of self-attentive models on
scalar ranking scores. The second technical contribution is the
development of a loss function that optimizes the expected metric
value over all candidate cut positions. Overall, Choppy not only
improves the predictive performance on this task but also improves
the model inference speed by &gt; 3 times.
      </p>
      <p>Our contributions. The key contributions of the paper are
summarized as follows:
• We frame the ranked list truncation task as modeling the
joint distribution among all candidate cut positions, and we</p>
      <p>construct our training loss to be the expected metric value
over cut positions, for any choice of user-defined metric, such
as F1, precision, or discounted cumulative gain. As such, our
training loss and evaluation metric are fully aligned.
• We propose Choppy, a Cut Transformer model that achieves
a state-of-the-art 11.5% relative improvement over the BiCut
model. To predict the joint distribution over candidate cut
positions, our method learns a positional embedding and
leverages expressive bidirectional multi-headed attention.
2</p>
    </sec>
    <sec id="sec-3">
      <title>RELATED WORK</title>
      <p>
        Across the rich history of information retrieval research, there has
been extensive work focused on modeling score distributions of
IR systems. Early work in this area primarily focused on fitting
parametric probability distributions to score distributions [
        <xref ref-type="bibr" rid="ref1 ref10">1, 10</xref>
        ].
This is often achieved by making the assumption that the
overall distribution can be expressed as a mixture of a relevant and
a non-relevant distribution. The expectation-maximization (EM)
algorithm is often adopted to learn the parameters.
      </p>
      <p>
        There has been considerable recent interest in adopting machine
learning based models to optimize and improve the ranked list
truncation problem. For instance, cascade-style IR systems [
        <xref ref-type="bibr" rid="ref17">17</xref>
        ]
seek to achieve a balance between eficiency and efectiveness.
Notably, [
        <xref ref-type="bibr" rid="ref5">5</xref>
        ] investigates a number of machine learning approaches for
learning dynamic cutofs within cascade-style ranking systems.
Another recent study investigated how to leverage bidirectional Long
Short-Term Memory (LSTM) models to identify the best position
to truncate a given list [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ]. This model, BiCut, can be considered
the present state-of-the-art approach.
      </p>
      <p>
        Our work is closely related to the task of query performance
prediction [
        <xref ref-type="bibr" rid="ref4">4</xref>
        ]. In this task, the objective is to automatically
determine the efectiveness of a given query. This could be leveraged
to determine the optimal set of results to the user for any given
measure. Methods for query performance prediction include
preretrieval-based approaches [
        <xref ref-type="bibr" rid="ref7">7</xref>
        ], relevance-based approaches [
        <xref ref-type="bibr" rid="ref19 ref4">4, 19</xref>
        ],
and neural approaches [
        <xref ref-type="bibr" rid="ref18">18</xref>
        ].
      </p>
      <p>
        A system that determines the best number of results to display
to users has the potential to benefit a wide number of applications.
For example, in sponsored search, displaying too many irrelevant
ads to users may cause frustration, resulting in so-called query
blindness. This motivated research that investigated whether any
ads should be displayed at all [
        <xref ref-type="bibr" rid="ref3">3</xref>
        ]. It is also easy to see that a similar
and related problem formulation is to determine how many ads
should be displayed to the users [
        <xref ref-type="bibr" rid="ref16">16</xref>
        ]. Moreover, determining the
optimal number of ranked results is also important in a number of
other IR applications such as legal e-discovery [
        <xref ref-type="bibr" rid="ref14">14</xref>
        ], where there
is an significant financial or labor cost associated with reviewing
results. Finally, the ability to calibrate scores across queries and
diferent corpora has also been studied in the context of federated
search tasks [
        <xref ref-type="bibr" rid="ref13">13</xref>
        ] such as meta-search [
        <xref ref-type="bibr" rid="ref11">11</xref>
        ].
3
      </p>
    </sec>
    <sec id="sec-4">
      <title>CHOPPY</title>
      <p>We now describe Choppy, our proposed Cut Transformer approach
for ranked list truncation.</p>
      <p>Let (r1, . . . , r ) denote the sequence of results, ranked in
decreasing order of relevance, and let r have relevance score s and ground
truth relevance label y (1 if relevant, −1 if non-relevant). We now
describe our model piecemeal.
3.1</p>
    </sec>
    <sec id="sec-5">
      <title>Transformer Layer</title>
      <p>We briefly review the Transformer layer. In our work, we let all
model dimensions be . Let  ∈ R× represent the input to the
layer and ,  ,  ∈ R× . We define</p>
      <p>Atn
 ;,  ,</p>
      <p>
        = Act    ( )
where Act is an activation function. As done in [
        <xref ref-type="bibr" rid="ref15">15</xref>
        ], we take it to
perform row-wise softmax and normalize by √. Attention is often
augmented using multiple heads. In multi-headed attention, the
model dimension is split into multiple heads, each one performing
attention independently, and the result is concatenated together.
Let ℎ be the number of heads and suppose  is divisible by ℎ. Then,
MultiAtn  ;,  ,  = rConcat hAtn  ;() , () ,  () i
1≤ ≤ℎ
where () , () ,  () ∈ R×(/ℎ) . rConcat performs row-wise
concatenation. The output of multi-headed attention is
 = LayerNorm  + MultiAtn
 ;,  , 
where LayerNorm is layer normalization [
        <xref ref-type="bibr" rid="ref2">2</xref>
        ]. Finally, the output of
the Transformer layer is
      </p>
      <p>trans = LayerNorm ( + rFF ())
where rFF applies a single learnable feed-forward layer with ReLU
activation to each row of .
3.2</p>
    </sec>
    <sec id="sec-6">
      <title>Positional Embedding</title>
      <p>The vanilla Transformer incorporates positional encoding by adding
ifxed sinusoidal values to the input token embeddings. As the token
embeddings are trained, they have the flexibility to learn how to
best utilize the fixed positional information. In our setting however,
the inputs are fixed 1-dimensional relevance scores. Attempting
to apply a Transformer layer directly on the raw scores can limit
its complexity. To that end, we introduce a learnable positional
embedding  ∈ R×(−1) and feed in  = rConcat [s,  ] to the
the first Transformer layer only, where s is the column vector of
relevance scores.
3.3</p>
    </sec>
    <sec id="sec-7">
      <title>Loss</title>
      <p>So far, Choppy takes the -length vector of scores, augments it
with a positional embedding, and feeds the result into layers
Transformer layers. This produces trans ∈ R× . We arrive at the output
of Choppy by applying a final linear projection followed by a
softmax over positions:</p>
      <p>o = Softmax (trans  )
where  ∈ R×1. We interpret the output o to be a probability
distribution over candidate cutof positions. More concretely, we
take o = Prob [(r1, . . . , r )]. Let  be any user-defined evaluation
metric that should be maximized, such as F1 or precision. For each
training example  and every candidate cutof position  we compute
 (y( ) ), the value of the metric if the result list were to be truncated
at position , using the ground-truth relevance labels. Our proposed
loss follows as:</p>
      <p>s( ) , y( ) = − Õ o s( )  y( )</p>
      <p>=1
= −E ∼Categorical(o(s() ))  y( ) .</p>
      <p>With this loss, our model learns the conditional joint distribution
over candidate cut positions that maximizes the expected evaluation
metric on the training samples. We depict the loss and the predicted
distribution for a few training samples in Figure 1. We see that the
model tends to weight positions according to their corresponding
metric value. At test time we choose to cut at the argmax position.
Note that unlike BiCut, our loss has no tune-able hyperparameters.
4</p>
    </sec>
    <sec id="sec-8">
      <title>EXPERIMENTS</title>
      <p>This section describes our experimental setup and results.
4.1</p>
    </sec>
    <sec id="sec-9">
      <title>Dataset</title>
      <p>
        We evaluate our method using the TREC collection Robust04, used
in the TREC 2004 Robust Track. It consists of 250 queries over
528k news articles, where each query has 1000 total results and
an average of 70 relevant ones. This is the same dataset used in
[
        <xref ref-type="bibr" rid="ref9">9</xref>
        ]. We use a random 80/20 train/test that achieves comparable
performance to the reported results in [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ]. We evaluate the eficacy
of our truncation model using two diferent retrieval approaches
- BM25, a traditional tf-idf based model, and DRMM [
        <xref ref-type="bibr" rid="ref6">6</xref>
        ], a neural
model.
4.2
      </p>
    </sec>
    <sec id="sec-10">
      <title>Baselines</title>
      <p>We evaluate our method against the following baselines:
• Fixed- returns the top- results for a single value of 
across test queries.</p>
      <p>Oracle
Fixed- (5)
Fixed- (10)
Fixed- (50)</p>
      <p>Greedy-</p>
      <p>BiCut</p>
      <p>Choppy
Rel. % Gain</p>
      <p>BM25</p>
      <p>
        DRMM
• Greedy- chooses the single  that maximizes  over the
training set.
• Oracle uses knowledge of each test query’s true label to
optimize . It represents an upper-bound on the metric
performance that can be achieved.
• BiCut [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ] learns a multi-layer bidirectional LSTM model on
the entire training set, taking the score sequence as inputs.
At position  of the result list, the model predicts probability
 to continue and probability 1 −  to end. At inference
time, the cutof is made before the first occurrence of end.
      </p>
      <p>=1 log2 ( + 1)
We need to deviate from the usual definition of DCG since the usual
definition always increases monotonically with the length of the
returned ranked list and so the optimal solution under this definition
would be to not truncate at all. For methods that optimize F1 or
DCG, we report the performance of the model when it is optimized
specifically for that metric. Note that DCG is unsupported by BiCut.</p>
      <p>
        For Choppy, we blithely set layers = 3, ℎ (# heads) = 8, and
 = 128 across all settings, without any tuning. We optimize the
aforementioned custom loss function using Adam with default
learning rate 0.001, and a batch size of 64. As in [
        <xref ref-type="bibr" rid="ref9">9</xref>
        ], we only consider
the top-300 candidate results of each query.
5
5.1
      </p>
    </sec>
    <sec id="sec-11">
      <title>RESULTS AND DISCUSSION</title>
    </sec>
    <sec id="sec-12">
      <title>Results</title>
      <p>As shown in Table 1, Choppy achieves a significant improvement
over BiCut for both metrics and both retrieval types. This
improvement arises from Choppy’s ability to model the joint distribution
over all candidate cut positions and its direct optimization of the
evaluation metric. Furthermore, the attention mechanism is able to
efectively capture correlations between scores far apart in ranked
order. This is in contrast to LSTMs, as used in BiCut, whose
degradation with larger sequence length is well known.
5.2</p>
    </sec>
    <sec id="sec-13">
      <title>Ablation Study</title>
      <p>Choppy has three hyperparameters: the model dimension , the
number of heads ℎ, and the number of Transformer layers layers.
In Figure 2 we plot the impact of  and ℎ on predictive performance
(while keeping layers fixed to 3). We see that both F1 and DCG are
strong and stable across settings. Extrapolating beyond Robust04,
we expect Choppy to work out-of-the-box on many datasets. Unlike
BiCut, which required much tuning in our experience, Choppy
seems to work well while requiring little-to-no tuning.
6</p>
    </sec>
    <sec id="sec-14">
      <title>CONCLUSION</title>
      <p>We propose Choppy, a Transformer architecture for ranked list
truncation, that learns the score distribution of relevant and
nonrelevant documents and is able to directly optimize any user-defined
metric. We show that Choppy achieves state-of-the-art F1 and DCG
performance on Robust04, under both a traditional tf-idf as well as
modern neural ranking system. We then dig deeper into Choppy’s
architecture settings, showing strong and stable performance across
a range of values. We thus conclude that when faced with a ranked
list truncation task, one can apply Choppy and expect competitive
performance.</p>
    </sec>
    <sec id="sec-15">
      <title>ACKNOWLEDGMENTS</title>
      <p>We would like to thank the authors of BiCut for providing us with
the BM25 and DRMM ranked lists that were used for evaluation.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          [1]
          <string-name>
            <given-names>Avi</given-names>
            <surname>Arampatzis</surname>
          </string-name>
          , Jaap Kamps, and
          <string-name>
            <given-names>Stephen</given-names>
            <surname>Robertson</surname>
          </string-name>
          .
          <year>2009</year>
          .
          <article-title>Where to Stop Reading a Ranked List?: Threshold Optimization Using Truncated Score Distributions</article-title>
          .
          <source>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
          (Boston, MA, USA) (
          <article-title>SIGIR '09)</article-title>
          . ACM, New York, NY, USA,
          <fpage>524</fpage>
          -
          <lpage>531</lpage>
          . https://doi.org/10.1145/1571941.1572031
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          [2]
          <string-name>
            <given-names>Jimmy</given-names>
            <surname>Lei</surname>
          </string-name>
          <string-name>
            <surname>Ba</surname>
          </string-name>
          , Jamie Ryan Kiros, and
          <string-name>
            <given-names>Geofrey E</given-names>
            <surname>Hinton</surname>
          </string-name>
          .
          <year>2016</year>
          .
          <article-title>Layer normalization</article-title>
          .
          <source>arXiv preprint arXiv:1607.06450</source>
          (
          <year>2016</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          [3]
          <string-name>
            <given-names>Andrei</given-names>
            <surname>Broder</surname>
          </string-name>
          , Massimiliano Ciaramita, Marcus Fontoura, Evgeniy Gabrilovich, Vanja Josifovski, Donald Metzler, Vanessa Murdock, and
          <string-name>
            <given-names>Vassilis</given-names>
            <surname>Plachouras</surname>
          </string-name>
          .
          <year>2008</year>
          .
          <article-title>To Swing or Not to Swing: Learning when (Not) to Advertise</article-title>
          .
          <source>In Proceedings of the 17th ACM Conference on Information and Knowledge</source>
          Management (Napa Valley, California, USA) (
          <article-title>CIKM '08)</article-title>
          . ACM, New York, NY, USA,
          <fpage>1003</fpage>
          -
          <lpage>1012</lpage>
          . https: //doi.org/10.1145/1458082.1458216
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          [4]
          <string-name>
            <given-names>Steve</given-names>
            <surname>Cronen-Townsend</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Yun</given-names>
            <surname>Zhou</surname>
          </string-name>
          , and
          <string-name>
            <given-names>W. Bruce</given-names>
            <surname>Croft</surname>
          </string-name>
          .
          <year>2002</year>
          .
          <article-title>Predicting Query Performance</article-title>
          .
          <source>In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Tampere, Finland) (SIGIR '02)</source>
          . ACM, New York, NY, USA,
          <fpage>299</fpage>
          -
          <lpage>306</lpage>
          . https://doi.org/10.1145/564376. 564429
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          [5]
          <string-name>
            <given-names>J.</given-names>
            <surname>Shane</surname>
          </string-name>
          <string-name>
            <given-names>Culpepper</given-names>
            ,
            <surname>Charles L. A. Clarke</surname>
          </string-name>
          , and
          <string-name>
            <given-names>Jimmy</given-names>
            <surname>Lin</surname>
          </string-name>
          .
          <year>2016</year>
          .
          <article-title>Dynamic Cutof Prediction in Multi-Stage Retrieval Systems</article-title>
          .
          <source>In Proceedings of the 21st Australasian Document Computing Symposium (Caulfield</source>
          ,
          <string-name>
            <surname>VIC</surname>
          </string-name>
          , Australia) (
          <source>ADCS '16)</source>
          . ACM, New York, NY, USA,
          <fpage>17</fpage>
          -
          <lpage>24</lpage>
          . https://doi.org/10.1145/3015022.3015026
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          [6]
          <string-name>
            <given-names>Jiafeng</given-names>
            <surname>Guo</surname>
          </string-name>
          , Yixing Fan,
          <string-name>
            <given-names>Qingyao</given-names>
            <surname>Ai</surname>
          </string-name>
          , and
          <string-name>
            <given-names>W Bruce</given-names>
            <surname>Croft</surname>
          </string-name>
          .
          <year>2016</year>
          .
          <article-title>A deep relevance matching model for ad-hoc retrieval</article-title>
          .
          <source>In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</source>
          .
          <fpage>55</fpage>
          -
          <lpage>64</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          [7]
          <string-name>
            <given-names>Claudia</given-names>
            <surname>Hauf</surname>
          </string-name>
          , Djoerd Hiemstra, and Franciska de Jong.
          <year>2008</year>
          .
          <article-title>A Survey of Pre-retrieval Query Performance Predictors</article-title>
          .
          <source>In Proceedings of the 17th ACM Conference on Information and Knowledge</source>
          Management (Napa Valley, California, USA) (
          <article-title>CIKM '08)</article-title>
          . ACM, New York, NY, USA,
          <fpage>1419</fpage>
          -
          <lpage>1420</lpage>
          . https://doi.org/10.1145/ 1458082.1458311
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          [8]
          <string-name>
            <given-names>Sepp</given-names>
            <surname>Hochreiter</surname>
          </string-name>
          and
          <string-name>
            <given-names>Jürgen</given-names>
            <surname>Schmidhuber</surname>
          </string-name>
          .
          <year>1997</year>
          .
          <article-title>Long short-term memory</article-title>
          .
          <source>Neural computation 9</source>
          ,
          <issue>8</issue>
          (
          <year>1997</year>
          ),
          <fpage>1735</fpage>
          -
          <lpage>1780</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          [9]
          <string-name>
            <surname>Yen-Chieh</surname>
            <given-names>Lien</given-names>
          </string-name>
          , Daniel Cohen, and
          <string-name>
            <given-names>W. Bruce</given-names>
            <surname>Croft</surname>
          </string-name>
          .
          <year>2019</year>
          .
          <article-title>An AssumptionFree Approach to the Dynamic Truncation of Ranked Lists</article-title>
          .
          <source>In Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information</source>
          Retrieval (Santa Clara, CA, USA) (
          <article-title>ICTIR '19)</article-title>
          . ACM, New York, NY, USA,
          <fpage>79</fpage>
          -
          <lpage>82</lpage>
          . https: //doi.org/10.1145/3341981.3344234
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          [10]
          <string-name>
            <given-names>R.</given-names>
            <surname>Manmatha</surname>
          </string-name>
          ,
          <string-name>
            <given-names>T.</given-names>
            <surname>Rath</surname>
          </string-name>
          , and
          <string-name>
            <given-names>F.</given-names>
            <surname>Feng</surname>
          </string-name>
          .
          <year>2001</year>
          .
          <article-title>Modeling Score Distributions for Combining the Outputs of Search Engines</article-title>
          .
          <source>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
          (New Orleans, Louisiana, USA) (
          <article-title>SIGIR '01)</article-title>
          . ACM, New York, NY, USA,
          <fpage>267</fpage>
          -
          <lpage>275</lpage>
          . https://doi.org/10.1145/383952.384005
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          [11]
          <string-name>
            <given-names>Mark</given-names>
            <surname>Montague</surname>
          </string-name>
          and
          <string-name>
            <given-names>Javed A.</given-names>
            <surname>Aslam</surname>
          </string-name>
          .
          <year>2001</year>
          .
          <article-title>Relevance Score Normalization for Metasearch</article-title>
          .
          <source>In Proceedings of the Tenth International Conference on Information and Knowledge Management (Atlanta</source>
          , Georgia, USA) (
          <article-title>CIKM '01)</article-title>
          . ACM, New York, NY, USA,
          <fpage>427</fpage>
          -
          <lpage>433</lpage>
          . https://doi.org/10.1145/502585.502657
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          [12]
          <string-name>
            <surname>Marc'Aurelio Ranzato</surname>
            , Sumit Chopra,
            <given-names>Michael</given-names>
          </string-name>
          <string-name>
            <surname>Auli</surname>
            , and
            <given-names>Wojciech</given-names>
          </string-name>
          <string-name>
            <surname>Zaremba</surname>
          </string-name>
          .
          <year>2015</year>
          .
          <article-title>Sequence level training with recurrent neural networks</article-title>
          .
          <source>arXiv preprint arXiv:1511.06732</source>
          (
          <year>2015</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          [13]
          <string-name>
            <given-names>Milad</given-names>
            <surname>Shokouhi</surname>
          </string-name>
          and
          <string-name>
            <given-names>Luo</given-names>
            <surname>Si</surname>
          </string-name>
          .
          <year>2011</year>
          .
          <string-name>
            <given-names>Federated</given-names>
            <surname>Search</surname>
          </string-name>
          .
          <source>Found. Trends Inf. Retr. 5</source>
          ,
          <issue>1</issue>
          (Jan.
          <year>2011</year>
          ),
          <fpage>1</fpage>
          -
          <lpage>102</lpage>
          . https://doi.org/10.1561/1500000010
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          [14]
          <string-name>
            <surname>Stephen</surname>
            <given-names>Tomlinson</given-names>
          </string-name>
          , Douglas W. Oard,
          <string-name>
            <surname>Jason R. Baron</surname>
            , and
            <given-names>Paul</given-names>
          </string-name>
          <string-name>
            <surname>Thompson</surname>
          </string-name>
          .
          <year>2007</year>
          .
          <article-title>Overview of the TREC 2007 Legal Track</article-title>
          . In
          <source>In Proceedings of the 16th Text Retrieval Conference.</source>
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          [15]
          <string-name>
            <surname>Ashish</surname>
            <given-names>Vaswani</given-names>
          </string-name>
          , Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
          <string-name>
            <surname>Łukasz Kaiser</surname>
            , and
            <given-names>Illia</given-names>
          </string-name>
          <string-name>
            <surname>Polosukhin</surname>
          </string-name>
          .
          <year>2017</year>
          .
          <article-title>Attention is all you need</article-title>
          .
          <source>In Advances in neural information processing systems</source>
          .
          <volume>5998</volume>
          -
          <fpage>6008</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          [16]
          <string-name>
            <surname>Bo</surname>
            <given-names>Wang</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>Zhaonan</given-names>
            <surname>Li</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Jie</given-names>
            <surname>Tang</surname>
          </string-name>
          , Kuo Zhang, Songcan Chen, and
          <string-name>
            <given-names>Liyun</given-names>
            <surname>Ru</surname>
          </string-name>
          .
          <year>2011</year>
          .
          <article-title>Learning to Advertise: How Many Ads Are Enough?</article-title>
          .
          <source>In Proceedings of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data</source>
          Mining - Volume
          <string-name>
            <surname>Part</surname>
            <given-names>II</given-names>
          </string-name>
          (Shenzhen, China) (
          <source>PAKDD'11)</source>
          . Springer-Verlag, Berlin, Heidelberg,
          <fpage>506</fpage>
          -
          <lpage>518</lpage>
          . http://dl.acm.org/citation.cfm?id=
          <volume>2022850</volume>
          .
          <fpage>2022892</fpage>
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          [17]
          <string-name>
            <surname>Lidan</surname>
            <given-names>Wang</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jimmy Lin</surname>
            , and
            <given-names>Donald</given-names>
          </string-name>
          <string-name>
            <surname>Metzler</surname>
          </string-name>
          .
          <year>2011</year>
          .
          <article-title>A Cascade Ranking Model for Eficient Ranked Retrieval</article-title>
          .
          <source>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
          (Beijing, China) (
          <source>SIGIR '11)</source>
          . ACM, New York, NY, USA,
          <fpage>105</fpage>
          -
          <lpage>114</lpage>
          . https://doi.org/10.1145/2009916. 2009934
        </mixed-citation>
      </ref>
      <ref id="ref18">
        <mixed-citation>
          [18]
          <string-name>
            <given-names>Hamed</given-names>
            <surname>Zamani</surname>
          </string-name>
          ,
          <string-name>
            <given-names>W. Bruce</given-names>
            <surname>Croft</surname>
          </string-name>
          , and
          <string-name>
            <given-names>J. Shane</given-names>
            <surname>Culpepper</surname>
          </string-name>
          .
          <year>2018</year>
          .
          <article-title>Neural Query Performance Prediction Using Weak Supervision from Multiple Signals</article-title>
          .
          <source>In The 41st International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval</source>
          (Ann Arbor, MI, USA) (
          <article-title>SIGIR '18)</article-title>
          . ACM, New York, NY, USA,
          <fpage>105</fpage>
          -
          <lpage>114</lpage>
          . https://doi.org/10.1145/3209978.3210041
        </mixed-citation>
      </ref>
      <ref id="ref19">
        <mixed-citation>
          [19]
          <string-name>
            <given-names>Yun</given-names>
            <surname>Zhou</surname>
          </string-name>
          and
          <string-name>
            <given-names>W. Bruce</given-names>
            <surname>Croft</surname>
          </string-name>
          .
          <year>2007</year>
          .
          <article-title>Query Performance Prediction in Web Search Environments</article-title>
          .
          <source>In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Amsterdam</source>
          , The Netherlands)
          <article-title>(SIGIR '07)</article-title>
          . ACM, New York, NY, USA,
          <fpage>543</fpage>
          -
          <lpage>550</lpage>
          . https://doi. org/10.1145/1277741.1277835
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>