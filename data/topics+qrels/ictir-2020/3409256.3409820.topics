<top>
<num> Number: 340982001
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
Evaluation is essential for the development of search and recommendation systems [8, 14]. Before any ranking model is widely deployed it is important to first verify whether it is a true improvement over the currently-deployed model. A traditional way of evaluating relative differences between systems is through A/B testing, where part of the user population is exposed to the current system (“control") and the rest to the altered system (“treatment") during the same time period. Differences in behavior between these groups can then indicate if the alterations brought improvements, e.g. if the treatment group showed a higher Click-Through-Rate (CTR) or more revenue was made with this system [4].

<narr> Narrative:

</top>

<top>
<num> Number: 340982002
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
Interleaving has been introduced in Information Retrieval (IR) as a more efficient alternative to A/B testing [11]. Interleaving algorithms take the rankings produced by two ranking systems, and for each query create an interleaved ranking by combining the rankings from both systems. Clicks on the interleaved rankings directly indicate relative differences. Repeating this process over a large number of queries and averaging the results, leads to an estimate of which ranker would receive the highest CTR [10]. Previous studies have found that interleaving requires fewer interactions than A/B testing, which enables them to make consistent comparisons in a much shorter timespan [4, 21].

<narr> Narrative:

</top>

<top>
<num> Number: 340982003
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
More recently, counterfactual evaluation for rankings has been proposed by [13] to evaluate a ranking model based on clicks gathered using a different model. By correcting for the position bias introduced during logging, the counterfactual approach can unbiasedly estimate the CTR of a new model on historical data. To achieve this, counterfactual evaluation makes use of Inverse-Propensity-Scoring (IPS), where clicks are weighted inversely to the probability that a user examined them during logging [22]. A big advantage compared to interleaving and A/B testing, is that counterfactual evaluation does not require online interventions.

<narr> Narrative:

</top>

<top>
<num> Number: 340982004
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
Users generally do not examine all items that are displayed in a ranking but only click on examined items [5]. As a result, a lower probability of examination for an item also makes it less likely to be clicked. Position bias assumes that only the rank determines the probability of examination [6].

<narr> Narrative:

</top>

<top>
<num> Number: 340982005
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
We also assume that item-selection bias is present; this type of bias is an extreme form of position bias that results in zero examination probabilities for some items [16, 17]. This bias is unavoidable in top-k ranking settings, where only the k ∈ N>0 highest ranked items are displayed. Consequently, any item beyond rank k cannot be observed or examined by the user: ∀r ∈ N>0 (r > k → θr = 0). The distinction between item-selection bias and position bias is important because the original counterfactual evaluation method [13] is only able to correct for position bias when no item-selection bias is present [16, 17].

<narr> Narrative:

</top>

<top>
<num> Number: 340982006
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
Each interleaving method attempts to use randomization to counter position bias, without deviating too much from the original rankings so as to maintain the user experience [11]. Team-draft interleaving (TDI) randomly selects one ranker to place their top document first, then the other ranker places their top (unplaced) document next [20]. Then it randomly decides the next two documents, and this process is repeated until all documents are placed in the interleaved ranking. Clicks on the documents are attributed to the ranker that placed them. The ranker with the most attributed clicks is inferred to be preferred by the user. Probabilistic interleaving (PI) treats each ranking as a probability distribution over documents; at each rank a distribution is randomly selected and a document is drawn from it [9]. After clicks have been received, probabilistic interleaving computes the expected number of clicks documents per ranking system to infer preferences. Optimized interleaving (OI) casts the randomization as an optimization problem, and displays rankings so that if all documents are equally relevant no preferences are found [19].

<narr> Narrative:

</top>

<top>
<num> Number: 340982007
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
While every interleaving method attempts to deal with position bias, none is unbiased according to our definition (Section 2.2). This may be confusing because previous work on interleaving makes claims of unbiasedness [9, 10, 19]. However, they use different definitions of the term. More precisely, TDI, PI, and OI provably converge on the correct outcome if all documents are equally relevant [9, 10, 19, 20]. Moreover, if one assumes binary relevance and π1 ranks all relevant documents equal to or higher than π2, the binary outcome of PI and OI is proven to be correct in expectation [10, 19]. However, beyond the confines of these unambiguous cases, we can prove that these methods do not meet our definition of unbiasedness: for every method one can construct an example where it converges on the incorrect outcome. The rankers π1, π2 and position bias parameters θ can be chosen so that in expectation the wrong (binary) outcome is estimated; see Appendix A for a proof for each of the three interleaving methods. Thus, while more efficient than A/B testing, interleaving methods make systematic errors in certain circumstances and thus should not be considered to be unbiased w.r.t. CTR differences.

<narr> Narrative:

</top>

<top>
<num> Number: 340982008
<title> Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking"

<desc> Description:
Counterfactual evaluation is based on the idea that if certain biases can be estimated well, they can also be adjusted [12, 22]. While estimating relevance is considered the core difficulty of ranking evaluation, estimating the position bias terms θ is very doable. By randomizing rankings, e.g., by swapping pairs of documents [12] or exploiting data logged during A/B testing [1], differences in CTR for the same item on different positions can be observed directly. Alternatively, using Expectation Maximization (EM) optimization [23] or a dual learning objective [2], position bias can be estimated from logged data as well. Once the bias terms θ have been estimated, logged clicks can be weighted so as to correct for the position bias during logging. Hence, counterfactual evaluation can work with historically logged data. Existing counterfactual evaluation algorithms do not dictate which rankings should be displayed during logging: they do not perform interventions and thus we do not consider them to be online methods.

<narr> Narrative:

</top>
