%[1]
@inproceedings{10.1145/3331184.3331303,
author = {Dai, Zhuyun and Callan, Jamie},
title = {Deeper Text Understanding for IR with Contextual Neural Language Modeling},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331303},
doi = {10.1145/3331184.3331303},
abstract = {Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning querydocument relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads
to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {985–988},
numpages = {4},
keywords = {text understanding, neural-IR},
location = {Paris, France},
series = {SIGIR'19}
}

%[2]
@inproceedings{10.1145/3159652.3159659,
author = {Dai, Zhuyun and Xiong, Chenyan and Callan, Jamie and Liu, Zhiyuan},
title = {Convolutional Neural Networks for Soft-Matching N-Grams in Ad-Hoc Search},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159659},
doi = {10.1145/3159652.3159659},
abstract = {This paper presents textttConv-KNRM, a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for ad-hoc search. Instead of exact matching query and document n-grams, textttConv-KNRM uses Convolutional Neural Networks to represent n-grams of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score. textttConv-KNRM can be learned end-to-end and fully optimized from user feedback. The learned model»s generalizability is investigated by testing how well it performs in a related domain with small amounts of training data. Experiments on English search logs, Chinese search logs, and TREC Web track tasks demonstrated consistent advantages of textttConv-KNRM over prior neural IR methods and feature-based methods.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {126–134},
numpages = {9},
keywords = {relevance ranking, n-gram soft match, neural ir},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

%[3]
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

%[4]
@inproceedings{dietz2017trec,
  title={TREC Complex Answer Retrieval Overview.},
  author={Dietz, Laura and Verma, Manisha and Radlinski, Filip and Craswell, Nick},
  booktitle={TREC},
  year={2017}
}

%[5]
@inproceedings{10.1145/2983323.2983769,
author = {Guo, Jiafeng and Fan, Yixing and Ai, Qingyao and Croft, W. Bruce},
title = {A Deep Relevance Matching Model for Ad-Hoc Retrieval},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983769},
doi = {10.1145/2983323.2983769},
abstract = {In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {55–64},
numpages = {10},
keywords = {relevance matching, ad-hoc retrieval, semantic matching, neural models, ranking models},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

%[6]
@inproceedings{mihalcea-tarau-2004-textrank,
    title = "{T}ext{R}ank: Bringing Order into Text",
    author = "Mihalcea, Rada  and
      Tarau, Paul",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-3252",
    pages = "404--411",
}

%[7]
@misc{mitra2019updated,
      title={An Updated Duet Model for Passage Re-ranking}, 
      author={Bhaskar Mitra and Nick Craswell},
      year={2019},
      eprint={1903.07666},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

%[8]
@misc{mitra2019incorporating,
      title={Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks}, 
      author={Bhaskar Mitra and Corby Rosset and David Hawking and Nick Craswell and Fernando Diaz and Emine Yilmaz},
      year={2019},
      eprint={1907.03693},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

%[9]
@misc{bajaj2018ms,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%[10]
@misc{nogueira2020passage,
      title={Passage Re-ranking with BERT}, 
      author={Rodrigo Nogueira and Kyunghyun Cho},
      year={2020},
      eprint={1901.04085},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

%[11]
@misc{nogueira2019document,
      title={Document Expansion by Query Prediction}, 
      author={Rodrigo Nogueira and Wei Yang and Jimmy Lin and Kyunghyun Cho},
      year={2019},
      eprint={1904.08375},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

%[12]
@article{nogueira2019doc2query,
  title={From doc2query to docTTTTTquery},
  author={Nogueira, Rodrigo and Lin, Jimmy and Epistemic, AI}
}

%[13]
@article{10.1016/j.ijar.2008.11.006,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
title = {Semantic Hashing},
year = {2009},
issue_date = {July, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {50},
number = {7},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2008.11.006},
doi = {10.1016/j.ijar.2008.11.006},
abstract = {We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.},
journal = {Int. J. Approx. Reasoning},
month = jul,
pages = {969–978},
numpages = {10},
keywords = {Unsupervised learning, Information retrieval, Graphical models}
}

%[14]
@book{10.5555/576628,
author = {Salton, Gerard and McGill, Michael J.},
title = {Introduction to Modern Information Retrieval},
year = {1986},
isbn = {0070544840},
publisher = {McGraw-Hill, Inc.},
address = {USA}
}

%[15]
@inproceedings{10.1145/3077136.3080809,
author = {Xiong, Chenyan and Dai, Zhuyun and Callan, Jamie and Liu, Zhiyuan and Power, Russell},
title = {End-to-End Neural Ad-Hoc Ranking with Kernel Pooling},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080809},
doi = {10.1145/3077136.3080809},
abstract = {This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {55–64},
numpages = {10},
keywords = {embedding, relevance model, ranking, neural ir, kernel pooling},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}
