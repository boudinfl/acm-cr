{"name":"3397271.3401333.pdf","metadata":{"source":"META","title":"Feature Transformation for Neural Ranking Models","authors":["Honglei Zhuang","Xuanhui Wang","Michael Bendersky","Marc Najork"],"emails":["hlz@google.com","xuanhui@google.com","bemike@google.com","najork@google.com"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems → Learning to rank.\nKEYWORDS Feature transformation; neural ranking model; learning-to-rank\nACM Reference Format: Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Marc Najork. 2020. Feature Transformation for Neural Ranking Models. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China.ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401333"},{"heading":"1 INTRODUCTION","text":"Many machine learning models cannot easily handle features with drastically skewed distributions or extreme scales. Unfortunately, commonly used features often have these patterns. Particularly in web-related applications such as search and recommendation tasks,\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401333\ndata items are often represented by features with a peculiar statistical nature. For example, click counts can be in the scale of billions for some items while other items only have a dozen clicks. Using these numbers directly as input features can result in less optimal models and introduce numerical instability during training. Therefore, feature normalization or transformation is an essential step to improve the effectiveness of many machine learning models [13].\nIn the learning-to-rank setting, tree-based models [6, 11, 17] have been extensively studied in the past and remain competitive on public data sets which primarily consist of numerical features. The tree-based model architecture is generally immune to the adverse impact of directly using raw features. Recently, neural network based deep learning models attract lots of attention for learningto-rank tasks [1, 5]. However, few of them investigate the impact of feature transformation. A possible reason is that neural ranking models are regarded as universal function approximators [14], which leads to the misconception that the optimal ranking function can be automatically learned by current algorithms without feature transformation. Therefore, it is still unclear whether feature transformation is important for neural ranking models.\nWhile there are surveys providing empirical comparison of multiple feature transformation techniques in other domains [2, 19, 24], to the best of our knowledge there is no systematic comparison of feature transformation techniques for neural ranking models. Moreover, the optimal transformation could vary by feature due to different statistical nature. Possible solutions are either to manually specify feature transformation technique for each feature based on expert knowledge or to perform tedious empirical comparison for every single feature. Both practices require significant time and effort and become intractable when the number of features is large.\nIn this paper, we aim to evaluate the importance of feature transformation for neural rankingmodels. We first enumerate three basic feature transformation functions, including commonly used z-score and CDF transformations, and a simple yet effective symmetric logarithm transformation that has been less explored in prior work on ranking. Then, we propose a mixture feature transformation mechanism. In particular, it takes the set of basic feature transformation functions and automatically derives a mixture of them as the final transformation for each feature. The mixture feature transformation module can be jointly trained with the neural ranking model, which saves the human effort to choose transformation functions for different features.\nWe perform experiments on multiple learning-to-rank data sets to provide a thorough view of the effects of different feature transformation functions. We show that applying feature transformation can significantly improve the performance of neural ranking models compared to directly using the raw feature values. We also show that our mixture feature transformation can further improve the performance by selecting a proper transformation per feature without any human effort."},{"heading":"2 RELATEDWORK","text":"Feature scaling, normalization and transformation is one of themost fundamental data preprocessing techniques inmachine learning [12, 13]. There are surveys comparing different feature transformation techniques in various applications, such as disease detection [24], stock market prediction [19] and video classification [2].\nIn information retrieval, feature transformation is also a common practice. For example, the YAHOO Learning to Rank Challenge data set [8] applies cumulative distribution-based transformation on all features; the LETOR [23] data set also applies query-level minmax scaling on each feature. In traditional information retrieval, feature transformation has been extensively studied on both term frequency and inverse document frequency (e.g., BM25). However, such a study is still missing for neural ranking models.\nAutomatically learning to perform feature transformation is a relatively novel topic. There are only a few studies with similar objectives [3, 7]. Their methods focus on a family of functions such as linear or logistic ones and are not studied for ranking models. In contrast, our methods focus on neural ranking models and work with different forms of transformation functions."},{"heading":"3 PROBLEM FORMULATION","text":"Learning to rank. We represent a learning-to-rank data set with \uD835\uDC41 lists asD = {(X, y)}\uD835\uDC411 . For a list (X, y) ∈ D,X = {x\uD835\uDC56 } \uD835\uDC59 \uD835\uDC56=1 denotes a set of \uD835\uDC59 data items, each represented by an \uD835\uDC5B-dimensional feature vector x\uD835\uDC56 ∈ R\uD835\uDC5B ; y = {\uD835\uDC66\uD835\uDC56 }\uD835\uDC59\uD835\uDC56=1 are relevance labels of corresponding data items, where \uD835\uDC66\uD835\uDC56 ∈ R. We also use the notation X to represent all the data items in the entire data set, namely X = ⋃X∈D X.\nIn a supervised learning-to-rank setting, we are usually given a training data set D\uD835\uDC3F , where all the relevance labels y are known. We aim to develop a scoring function \uD835\uDC53 : R\uD835\uDC5B ↦→ R such that for any list X = {x\uD835\uDC56 }\uD835\uDC59\uD835\uDC56=1, the ranked list predicted by scoring and sorting items based on \uD835\uDC53 (x\uD835\uDC56 ) can be similar to the ground-truth ranked list obtained by sorting items based on \uD835\uDC66\uD835\uDC56 . Feature transformation. We denote the feature transformation function for the\uD835\uDC58-th feature as\uD835\uDF0E\uD835\uDC58 : R ↦→ R. For a feature vector x\uD835\uDC56 = (\uD835\uDC65\uD835\uDC561, \uD835\uDC65\uD835\uDC562, · · · , \uD835\uDC65\uD835\uDC56\uD835\uDC5B), we apply the feature transformation function on each feature vector and obtain the transformed feature vector as \uD835\uDF0E (x\uD835\uDC56 ) = (\uD835\uDF0E1 (\uD835\uDC65\uD835\uDC561), \uD835\uDF0E2 (\uD835\uDC65\uD835\uDC562), · · · , \uD835\uDF0E\uD835\uDC5B (\uD835\uDC65\uD835\uDC56\uD835\uDC5B)). The scoring function \uD835\uDC53 in the neural ranking model will be trained and evaluated based on the transformed feature vectors, i.e., the predicted ranked list will be obtained by sorting items based on \uD835\uDC53 (\uD835\uDF0E (x\uD835\uDC56 ))."},{"heading":"4 FEATURE TRANSFORMATION","text":"In this section, we first introduce several basic feature transformation techniques for single features. Then, we introduce a mixture feature transformation mechanism to automatically derive a mixture of multiple transformed values for each feature."},{"heading":"4.1 Basic Transformation","text":"Gaussian transformation (z-score). We fit a Gaussian distribution for all the data items x\uD835\uDC56 ∈ X in the given data set. We denote each data item as x\uD835\uDC56 = (\uD835\uDC65\uD835\uDC561, · · · , \uD835\uDC65\uD835\uDC56\uD835\uDC5B). For the \uD835\uDC58-th feature, we can\nestimate the mean and standard deviation by:\n\uD835\uDF07\uD835\uDC58 = 1 |X| ∑ x\uD835\uDC56 ∈X \uD835\uDC65\uD835\uDC56\uD835\uDC58 , \uD835\uDC60\uD835\uDC58 = √ 1 |X| ∑ x\uD835\uDC56 ∈X (\uD835\uDC65\uD835\uDC56\uD835\uDC58 − \uD835\uDF07\uD835\uDC58 )2\nAnd the derived transformation function is\n\uD835\uDF0EGauss,\uD835\uDC58 (\uD835\uDC65) = \uD835\uDC65 − \uD835\uDF07\uD835\uDC58 \uD835\uDC60\uD835\uDC58\n(1)\nWe denote the vector containing all transformed features as \uD835\uDF0EGauss (x\uD835\uDC56 ) = (\uD835\uDF0EGauss,1 (\uD835\uDC65\uD835\uDC561), · · · , \uD835\uDF0EGauss,\uD835\uDC5B (\uD835\uDC65\uD835\uDC56\uD835\uDC5B)). We will use similar notation for other feature transformation techniques. CDF transformation. Another idea is to estimate the cumulative distribution function (CDF) for each feature and use the CDF value to represent the feature. Concretely, we still use X as the set of all data items in the data set. For any value \uD835\uDC65 of the \uD835\uDC58-th feature, the transformation function is:\n\uD835\uDF0ECDF,\uD835\uDC58 (\uD835\uDC65) = ∑ x\uD835\uDC56 ∈X I(\uD835\uDC65\uD835\uDC56\uD835\uDC58 < \uD835\uDC65)\n|X| (2)\nSymmetric log1p transformation. We also use a symmetric logarithm function as a transformation function. A regular log(\uD835\uDC65) function cannot be directly applied to any function as it is undefined for \uD835\uDC65 ≤ 0. Moreover, when \uD835\uDC65 is close to 0, the absolute value of the transformed feature could be extremely large. Hence, we use a symmetric version of log(1 + \uD835\uDC65):\n\uD835\uDF0ELog1p,\uD835\uDC58 (\uD835\uDC65) = sgn(\uD835\uDC65) · log(1 + |\uD835\uDC65 |) (3) where sgn(\uD835\uDC65) is the sign function which returns 1 for positive numbers, −1 for negative numbers and 0 for \uD835\uDC65 = 0."},{"heading":"4.2 Mixture Transformation","text":"There may not be a single optimal feature transformation technique applicable for all scenarios. The effect of different feature transformation techniques depends on both the overall feature value distribution and the model structure. Hence, we introduce a mixture feature transformation module.\nSuppose for the \uD835\uDC58-th feature, we have\uD835\uDC5A feature transformation functions available as a “basis”, denoted as {\uD835\uDF0E1,\uD835\uDC58 , . . . , \uD835\uDF0E\uD835\uDC5A,\uD835\uDC58 }. We\nderive the mixture transformed feature value of the \uD835\uDC58-th feature \uD835\uDF0E\uD835\uDC58 (\uD835\uDC65) as:\n\uD835\uDF0EMixture,\uD835\uDC58 (\uD835\uDC65) = ∑ \uD835\uDC5A \uD835\uDC5D\uD835\uDC5A,\uD835\uDC58\uD835\uDF0E\uD835\uDC5A,\uD835\uDC58 (\uD835\uDC65) (4)\nwhere p\uD835\uDC58 = (\uD835\uDC5D1,\uD835\uDC58 , · · · , \uD835\uDC5D\uD835\uDC5A,\uD835\uDC58 ) is a weighting vector. We derive the weighting vector from an embedding of each feature by\np\uD835\uDC58 = softmax(We\uD835\uDC58 )\nwhere e\uD835\uDC58 is the \uD835\uDC51-dimensional embedding vector of the \uD835\uDC58-th feature andW is an\uD835\uDC5A×\uD835\uDC51 matrix to be learned. Notice that e\uD835\uDC58 is not related to any feature value \uD835\uDC65\uD835\uDC56\uD835\uDC58 , but rather an embedding vector of \uD835\uDC58-th feature per se. There are only \uD835\uDC5B such embedding vectors where \uD835\uDC5B is the number of features.\nNote that the mixture feature transformation layer can be jointly trained with the ranking model to automatically determine the best mixture of feature transformations for each feature."},{"heading":"4.3 Ranking Model","text":"The ranking model we employ is a feed-forward network where the first layer takes the transformed feature as input. Formally, for a data item x, we can derive the final ranking score \uD835\uDC66 by:\nz1 = ReLU(W1\uD835\uDF0E∗ (x) + b1) z2 = ReLU(W2z1 + b2) . . .\nz\uD835\uDC3B = ReLU(W\uD835\uDC3B z(\uD835\uDC3B−1) + b\uD835\uDC3B ) \uD835\uDC66 = W\uD835\uDC3B+1z\uD835\uDC3B + \uD835\uDC4F\uD835\uDC3B+1\nwhere zℎ is the output of the ℎ-th hidden layer; Wℎ and bℎ are weight matrix and bias vector of the ℎ-th hidden layer to be trained; ReLU(·) refers to the Rectifier [18] activation function; and \uD835\uDC66 is the final output. Notice that \uD835\uDF0E∗ (x) is the transformed input vector, where we can plug in any feature transformation methods mentioned above. For basic feature transformations such as CDF and Log1p, the transformed value \uD835\uDF0ECDF (x) or \uD835\uDF0ELog1p (x) will be fed into the model. For the mixture feature transformation, the module will be jointly trained with the entire model, as shown in Figure 1.\nWe insert a batch normalization [15] layer in front of the input of each layer. We also apply a dropout layer for the output of each hidden layer. The model is trained with an approximate NDCG5 loss [5, 22] with a stochastic treatment as described in [4]."},{"heading":"5 EXPERIMENTS","text":""},{"heading":"5.1 Data Sets","text":"We use two public learning-to-rank data sets with numerical features in our experiments. Another well-known public data set from YAHOO [8] is not used as its feature values are already transformed. WEB30K. WEB30K [21] is a public learning-to-rank data set released by Microsoft. The original data contains 5 folds. We only use Fold1 of the data. The data set is partitioned into three subsets: training, validation and testing. Each document is represented by 136 numerical features. In addition, a label with a 5-level relevance grade varying from 0 to 4 is provided for each document. ISTELLA. Istella released several learning-to-rank data sets to the public. We use the Istella full data set [9]. Each document is\nTable 1: Performance comparison (%). The best performances are bolded. Performances statistically significantly better (\uD835\uDEFC = 0.05) than Raw or Log1p are marked with † and ‡\nrespectively.\nData set Method NDCG1 NDCG5 NDCG10\nWEB30K Raw 45.43 45.25 47.26 Gauss 48.18† 46.81† 48.68† CDF 49.17† 47.75† 49.45† Log1p 49.12† 48.01† 49.88† Mixture 50.55†‡ 48.54†‡ 50.16†‡\nISTELLA Raw 65.81 62.32 67.09 Gauss 66.31 62.41 67.22 CDF 65.94 62.62† 67.55† Log1p 66.60† 63.29† 68.12† Mixture 66.91† 63.57†‡ 68.42†‡\nrepresented by 220 numerical features. The data set is also labeled with graded relevance judgments from 0 to 4. We remove 2 features which contain illegitimate values and only use the remaining 218."},{"heading":"5.2 Parameter Configurations","text":"In all of our experiments, the ranking model network contains 3 hidden layers with 1024, 512, 256 units respectively. We set the momentum of the batch normalization layers as 0.4 and the dropout rate as 0.5. For all experiments, we set the training batch size to 128 and run for 100,000 steps. We use AdaGrad [10] as the optimizer and tune the learning rate on each data set. The learning rate is set to 0.5 for WEB30K and 0.1 for ISTELLA respectively. The experiments are implemented based on the open-sourced TensorFlow Ranking library [20] and trained on TPU."},{"heading":"5.3 Comparison Settings","text":"We apply each basic feature transformation and compare the performance to the practice of directly using raw input features (represented by “Raw”). We also include the performance of applying the mixture feature transformation function into the comparison. For each feature \uD835\uDC65\uD835\uDC56\uD835\uDC58 , the mixture transformation takes the raw feature value along with all the three transformed feature values {\uD835\uDC65\uD835\uDC56\uD835\uDC58 , \uD835\uDF0EGauss,\uD835\uDC58 (\uD835\uDC65\uD835\uDC56\uD835\uDC58 ), \uD835\uDF0ECDF,\uD835\uDC58 (\uD835\uDC65\uD835\uDC56\uD835\uDC58 ), \uD835\uDF0ELog1p,\uD835\uDC58 (\uD835\uDC65\uD835\uDC56\uD835\uDC58 )} as the basis and generates the mixture transformation.\nWe adopt normalized discounted cumulative gain (NDCG) [16] as the evaluation metric. In our experiments, we utilize NDCG\uD835\uDC58 to measure the quality of the top-\uD835\uDC58 ranked items where \uD835\uDC58 ∈ {1, 5, 10}."},{"heading":"5.4 Results","text":"The overall results are shown in Figure 1. As one can observe, applying feature transformation techniques to neural ranking models substantially improves the performance, varying from +1% to +3% (NDCG5) on both data sets. This verifies the importance of utilizing feature transformation methods for neural ranking models.\nAmong all the basic feature transformation methods, Log1p seems to outperform the other two transformation methods. We believe that Log1p is especially effective to transform features with a power law distribution, which are prevalent in web-related data sets likeWEB30K and ISTELLA. The distribution of the transformed feature values will no longer be as skewed as raw feature values,\nallowing neural ranking models to better handle these features in training.\nAnother observation is that the Mixture method outperforms all the basic feature transformation methods on both data sets. On the WEB30K data set, the Mixture method improves NDCG1 by more than +1% compared to the best basic feature transformation method. It shows the effectiveness of the proposed mixture mechanism to automatically derive a better transformation function for each feature from the basic transformation functions. In-depth analysis of theMixture feature transformation. We also take a deeper look into the mixture weights derived by the mixture feature transformation.We select a few features inWEB30K and plot their learned weighting vector p\uD835\uDC58 . By default we select the feature for “body”. We visualize these weighting vectors in Figure 2.\nFirst, for some features that are sophisticated ranking scores per se (e.g., Sum of TF*IDF, BM25 and LMIR.DIR), the model only uses their raw scores, suggesting no further transformation is needed. For features with potentially extremely large values and/or power law distribution (e.g., Sum of TF and PageRank), the model learns that applying Log1p transformation methods is beneficial. The mixture method also applies CDF transformation for some features with smaller values (e.g. Sum of Stream Length Normalized TF) or sparse distributions (e.g., Outlink Number)."},{"heading":"6 CONCLUSIONS","text":"In this paper, we study the effect of feature transformation for neural ranking models. Through empirical evidence on multiple learning-to-rank data sets, we show that appropriate input feature transformation is crucial to improve the performance of neural ranking models. More importantly, we propose a mixture feature transformation model that can automatically select the optimal transformation given a set of basic transformation functions.\nBased on our study, a further examination of feature transformations in the context of neural ranking models can be appropriate. Although our experiments are only conducted on a feed-forward network ranking models, as the focus of this paper is on feature transformation, the techniques studied and proposed in this work are applicable to any neural ranking models. It would be interesting\nto see whether they can achieve better performances with feature transformations. Moreover, our current work mainly looks into numerical features. It is also intriguing to study how to conduct effective feature transformation for embedding-based features (e.g., text features) in the future."}],"references":[{"title":"Learning a deep listwise context model for ranking refinement","author":["Qingyao Ai","Keping Bi","Jiafeng Guo","W Bruce Croft"],"venue":"In 41st International ACM SIGIR Conference on Research & Development in Information Retrieval","citeRegEx":"1","shortCiteRegEx":"1","year":2018},{"title":"The effect of normalization in violence video classification performance","author":["Ashikin Ali","Norhalina Senan"],"venue":"IOP Conference Series: Materials Science and Engineering 226,","citeRegEx":"2","shortCiteRegEx":"2","year":2017},{"title":"Dynamic feature scaling for online learning of binary classifiers","author":["Danushka Bollegala"],"venue":"Knowledge-Based Systems","citeRegEx":"3","shortCiteRegEx":"3","year":2017},{"title":"A stochastic treatment of learning to rank scoring functions","author":["Sebastian Bruch","Shuguang Han","Michael Bendersky","Marc Najork"],"venue":"In 13th International Conference on Web Search and Data","citeRegEx":"4","shortCiteRegEx":"4","year":2020},{"title":"Revisiting approximate metric optimization in the age of deep neural networks","author":["Sebastian Bruch","Masrour Zoghi","Mike Bendersky","Marc Najork"],"venue":"In 42nd International ACM SIGIR Conference on Research & Development in Information Retrieval","citeRegEx":"5","shortCiteRegEx":"5","year":2019},{"title":"From RankNet to LambdaRank to LambdaMART: An overview","author":["Christopher J.C. Burges"],"venue":"Technical Report MSR-TR-2010-82. Microsoft Research","citeRegEx":"6","shortCiteRegEx":"6","year":2010},{"title":"A robust data scaling algorithm to improve classification accuracies in biomedical data","author":["Xi Hang Cao","Ivan Stojkovic","Zoran Obradovic"],"venue":"BMC Bioinformatics 17,","citeRegEx":"7","shortCiteRegEx":"7","year":2016},{"title":"Yahoo! Learning to rank challenge overview","author":["Olivier Chapelle","Yi Chang"],"venue":"In Proceedings of the Learning to Rank Challenge","citeRegEx":"8","shortCiteRegEx":"8","year":2011},{"title":"Fast ranking with additive ensembles of oblivious and non-oblivious regression trees","author":["Domenico Dato","Claudio Lucchese","Franco Maria Nardini","Salvatore Orlando","Raffaele Perego","Nicola Tonellotto","Rossano Venturini"],"venue":"ACM Transactions on Information Systems 35,","citeRegEx":"9","shortCiteRegEx":"9","year":2016},{"title":"Adaptive subgradient methods for online learning and stochastic optimization","author":["John Duchi","Elad Hazan","Yoram Singer"],"venue":"Journal of Machine Learning Research 12,","citeRegEx":"10","shortCiteRegEx":"10","year":2011},{"title":"Greedy function approximation: A gradient boosting machine","author":["Jerome H Friedman"],"venue":"Annals of Statistics 29,","citeRegEx":"11","shortCiteRegEx":"11","year":2001},{"title":"Data Preprocessing in Data","author":["Salvador García","Julián Luengo","Francisco Herrera"],"venue":null,"citeRegEx":"12","shortCiteRegEx":"12","year":2015},{"title":"Data Mining: Concepts and Techniques","author":["Jiawei Han","Jian Pei","Micheline Kamber"],"venue":null,"citeRegEx":"13","shortCiteRegEx":"13","year":2011},{"title":"Multilayer feedforward networks are universal approximators","author":["K. Hornik","M. Stinchcombe","H.White"],"venue":"Neural Networks","citeRegEx":"14","shortCiteRegEx":"14","year":1989},{"title":"Batch normalization: Accelerating deep network training by reducing internal covariate shift","author":["Sergey Ioffe","Christian Szegedy"],"venue":"In 32nd International Conference on International Conference on Machine Learning","citeRegEx":"15","shortCiteRegEx":"15","year":2015},{"title":"Cumulated gain-based evaluation of IR techniques","author":["Kalervo Järvelin","Jaana Kekäläinen"],"venue":"ACM Transactions on Information Systems 20,","citeRegEx":"16","shortCiteRegEx":"16","year":2002},{"title":"LightGBM: A highly efficient gradient boosting decision tree","author":["Guolin Ke","Qi Meng","Thomas Finley","Taifeng Wang","Wei Chen","Weidong Ma","Qiwei Ye","Tie-Yan Liu"],"venue":"In Advances in Neural Information Processing Systems","citeRegEx":"17","shortCiteRegEx":"17","year":2017},{"title":"Rectified linear units improve restricted Boltzmann machines","author":["Vinod Nair","Geoffrey E Hinton"],"venue":"In 27th International Conference on Machine Learning","citeRegEx":"18","shortCiteRegEx":"18","year":2010},{"title":"The impact of data normalization on stock market prediction: Using SVM and technical indicators","author":["Jiaqi Pan","Yan Zhuang","Simon Fong"],"venue":"In International Conference on Soft Computing in Data","citeRegEx":"19","shortCiteRegEx":"19","year":2016},{"title":"TF-Ranking: Scalable tensorflow library for learning-to-rank","author":["Rama Kumar Pasumarthi","Sebastian Bruch","Xuanhui Wang","Cheng Li","Michael Bendersky","Marc Najork","Jan Pfeifer","Nadav Golbandi","Rohan Anil","Stephan Wolf"],"venue":"In 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","citeRegEx":"20","shortCiteRegEx":"20","year":2019},{"title":"Introducing LETOR 4.0 Datasets","author":["Tao Qin","Tie-Yan Liu"],"venue":null,"citeRegEx":"21","shortCiteRegEx":"21","year":2013},{"title":"A general approximation framework for direct optimization of information retrieval measures","author":["Tao Qin","Tie-Yan Liu","Hang Li"],"venue":"Information Retrieval 13,","citeRegEx":"22","shortCiteRegEx":"22","year":2010},{"title":"LETOR: A benchmark collection for research on learning to rank for information retrieval","author":["Tao Qin","Tie-Yan Liu","Jun Xu","Hang Li"],"venue":"Information Retrieval 13,","citeRegEx":"23","shortCiteRegEx":"23","year":2010},{"title":"Investigations on impact of feature normalization techniques on classifier’s performance in breast tumor classification","author":["Bikesh Kumar Singh","Kesari Verma","A.S. Thoke"],"venue":"International Journal of Computer Applications 116,","citeRegEx":"24","shortCiteRegEx":"24","year":2015}],"referenceMentions":[{"referenceID":12,"context":"Therefore, feature normalization or transformation is an essential step to improve the effectiveness of many machine learning models [13].","startOffset":133,"endOffset":137},{"referenceID":5,"context":"In the learning-to-rank setting, tree-based models [6, 11, 17] have been extensively studied in the past and remain competitive on public data sets which primarily consist of numerical features.","startOffset":51,"endOffset":62},{"referenceID":10,"context":"In the learning-to-rank setting, tree-based models [6, 11, 17] have been extensively studied in the past and remain competitive on public data sets which primarily consist of numerical features.","startOffset":51,"endOffset":62},{"referenceID":16,"context":"In the learning-to-rank setting, tree-based models [6, 11, 17] have been extensively studied in the past and remain competitive on public data sets which primarily consist of numerical features.","startOffset":51,"endOffset":62},{"referenceID":0,"context":"Recently, neural network based deep learning models attract lots of attention for learningto-rank tasks [1, 5].","startOffset":104,"endOffset":110},{"referenceID":4,"context":"Recently, neural network based deep learning models attract lots of attention for learningto-rank tasks [1, 5].","startOffset":104,"endOffset":110},{"referenceID":13,"context":"A possible reason is that neural ranking models are regarded as universal function approximators [14], which leads to the misconception that the optimal ranking function can be automatically learned by current algorithms without feature transformation.","startOffset":97,"endOffset":101},{"referenceID":1,"context":"While there are surveys providing empirical comparison of multiple feature transformation techniques in other domains [2, 19, 24], to the best of our knowledge there is no systematic comparison of feature transformation techniques for neural ranking models.","startOffset":118,"endOffset":129},{"referenceID":18,"context":"While there are surveys providing empirical comparison of multiple feature transformation techniques in other domains [2, 19, 24], to the best of our knowledge there is no systematic comparison of feature transformation techniques for neural ranking models.","startOffset":118,"endOffset":129},{"referenceID":23,"context":"While there are surveys providing empirical comparison of multiple feature transformation techniques in other domains [2, 19, 24], to the best of our knowledge there is no systematic comparison of feature transformation techniques for neural ranking models.","startOffset":118,"endOffset":129},{"referenceID":11,"context":"Feature scaling, normalization and transformation is one of themost fundamental data preprocessing techniques inmachine learning [12, 13].","startOffset":129,"endOffset":137},{"referenceID":12,"context":"Feature scaling, normalization and transformation is one of themost fundamental data preprocessing techniques inmachine learning [12, 13].","startOffset":129,"endOffset":137},{"referenceID":23,"context":"There are surveys comparing different feature transformation techniques in various applications, such as disease detection [24], stock market prediction [19] and video classification [2].","startOffset":123,"endOffset":127},{"referenceID":18,"context":"There are surveys comparing different feature transformation techniques in various applications, such as disease detection [24], stock market prediction [19] and video classification [2].","startOffset":153,"endOffset":157},{"referenceID":1,"context":"There are surveys comparing different feature transformation techniques in various applications, such as disease detection [24], stock market prediction [19] and video classification [2].","startOffset":183,"endOffset":186},{"referenceID":7,"context":"For example, the YAHOO Learning to Rank Challenge data set [8] applies cumulative distribution-based transformation on all features; the LETOR [23] data set also applies query-level minmax scaling on each feature.","startOffset":59,"endOffset":62},{"referenceID":22,"context":"For example, the YAHOO Learning to Rank Challenge data set [8] applies cumulative distribution-based transformation on all features; the LETOR [23] data set also applies query-level minmax scaling on each feature.","startOffset":143,"endOffset":147},{"referenceID":2,"context":"There are only a few studies with similar objectives [3, 7].","startOffset":53,"endOffset":59},{"referenceID":6,"context":"There are only a few studies with similar objectives [3, 7].","startOffset":53,"endOffset":59},{"referenceID":17,"context":"where zh is the output of the h-th hidden layer; Wh and bh are weight matrix and bias vector of the h-th hidden layer to be trained; ReLU(·) refers to the Rectifier [18] activation function; and y is the final output.","startOffset":165,"endOffset":169},{"referenceID":14,"context":"We insert a batch normalization [15] layer in front of the input of each layer.","startOffset":32,"endOffset":36},{"referenceID":4,"context":"The model is trained with an approximate NDCG5 loss [5, 22] with a stochastic treatment as described in [4].","startOffset":52,"endOffset":59},{"referenceID":21,"context":"The model is trained with an approximate NDCG5 loss [5, 22] with a stochastic treatment as described in [4].","startOffset":52,"endOffset":59},{"referenceID":3,"context":"The model is trained with an approximate NDCG5 loss [5, 22] with a stochastic treatment as described in [4].","startOffset":104,"endOffset":107},{"referenceID":7,"context":"Another well-known public data set from YAHOO [8] is not used as its feature values are already transformed.","startOffset":46,"endOffset":49},{"referenceID":20,"context":"WEB30K [21] is a public learning-to-rank data set released by Microsoft.","startOffset":7,"endOffset":11},{"referenceID":9,"context":"We use AdaGrad [10] as the optimizer and tune the learning rate on each data set.","startOffset":15,"endOffset":19},{"referenceID":19,"context":"The experiments are implemented based on the open-sourced TensorFlow Ranking library [20] and trained on TPU.","startOffset":85,"endOffset":89},{"referenceID":15,"context":"We adopt normalized discounted cumulative gain (NDCG) [16] as the evaluation metric.","startOffset":54,"endOffset":58}],"year":2020,"abstractText":"Although neural network models enjoy tremendous advantages in handling image and text data, tree-based models still remain competitive for learning-to-rank tasks with numerical data. A major strength of tree-based ranking models is the insensitivity to different feature scales, while neural ranking models may suffer from features with varying scales or skewed distributions. Feature transformation or normalization is a simple technique which preprocesses input features to mitigate their potential adverse impact on neural models. However, due to lack of studies, it is unclear to what extent feature transformation can benefit neural ranking models. In this paper, we aim to answer this question by providing empirical evidence for learning-to-rank tasks. First, we present a list of commonly used feature transformation techniques and perform a comparative study on multiple learning-to-rank data sets. Then we propose a mixture feature transformation mechanism which can automatically derive a mixture of basic feature transformation functions to achieve the optimal performance. Our experiments show that applying feature transformation can substantially improve the performance of neural ranking models compared to directly using the raw features. In addition, the proposed mixture transformation method can further improve the performance of the ranking model without any additional human effort.","creator":"LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2020/01/14 v7.00d Hypertext links for LaTeX"}}
{"name":"3397271.3401032.pdf","metadata":{"source":"CRF","title":"Measuring Recommendation ExplanationQuality: The Conflicting Goals of Explanations","authors":["Krisztian Balog","Filip Radlinski"],"emails":["krisztianb@google.com","filiprad@google.com"],"sections":[{"heading":"CCS CONCEPTS","text":"• Information systems → Recommender systems; Presentation of retrieval results; •Human-centered computing→ Natural language interfaces; HCI design and evaluation methods."},{"heading":"KEYWORDS","text":"Recommendations; explanations; evaluation"},{"heading":"ACM Reference Format:","text":"Krisztian Balog and Filip Radlinski. 2020. Measuring Recommendation Explanation Quality: The Conflicting Goals of Explanations. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China.ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401032"},{"heading":"1 INTRODUCTION","text":"Recommendations are part of everyday life. Be they made by a person, or by an automated system, the recommendations are often accompanied with an explanation, or reason, underlying the suggestions provided. Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.\nAt the same time, automated recommender systems have recently proliferated. This has increased attention on explainable and transparent AI, both from technical and ethical perspectives [1, 18].\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401032\nWhile explainable system design is not new (dating back to rulebased expert systems of the 1980s [5]), the role of explanations has gained more attention in the past decade [29].\nWe study the role of the recommender’s intention when generating explanations, which we refer to as explanation goals. Our focus is on assessing how the choice of goal affects explanations, and how the extent to which a given explanation satisfies different goals can be measured robustly.\nOur work starts with seven main goals of explanations, proposed by Tintarev and Masthoff [26]: transparency, intended to explain how the systemworks; scrutability, allowing users to tell the system if it is wrong; trust, increasing users’ confidence in the system; effectiveness, helping users to make good decisions; efficiency, helping users to make decisions faster; persuasiveness, trying to convince users to select the given item; and satisfaction, increasing the ease of use of a system. They argued that these goals should be identified as distinct, even if they may interact [26]. Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26]. Yet, depending on the perspective of the explanation generator, different goals may be appropriate, and may need to be traded off.\nWe ask three key research questions about such goals:\nRQ1 How can one robustly measure if an explanation (with a particular goal) provided with a recommendation creates the intended effect on the recipient?\nRQ2 Can ordinary people write explanations that optimize a given goal, and if so, how does that target goal affect how the explanation is perceived by recipients of recommendations?\nRQ3 How do different goals relate to each other, or more specifically does optimizing particular goals reduce or increase the extent to which other goals are satisfied? Can recipients of recommendations even distinguish the goals from each other?\nFor example, how does optimizing persuasiveness affect trust?What is the trade-off between effectiveness and efficiency? To the best of our knowledge, there have not been any holistic studies of the interaction between goals. This work aims to fill that gap.\nTaking advantage of the fact that people are commonly able to explain their own recommendations, we perform a set of user studies to assess both the generation and measurement of explanations. In our work, recommendations and explanations are generated, and evaluated, by people using a crowdsourcing platform. We study if people can optimize given goals, and how to efficiently measure whether different goals are satisfied. We present and compare two alternative designs for this evaluation: an item-wise setting, and a list-wise setting, each with strengths and weaknesses."},{"heading":"SIGIR ’20, July 25–30, 2020, Virtual Event, China Krisztian Balog and Filip Radlinski","text":"Our main findings are: (1) Item-wise evaluation of explanations is more sensitive than list-wise evaluation; (2) There are strong correlations between the measured values of different goals, with some goals (such as satisfaction) being correlated with many others, while other goals (such as scrutability) being more distinct; (3) The intended goal that the person is asked to optimize has a large effect on explanation quality, yet is not always the one with highest measured rating.\nIn summary, our main contributions are twofold. First, we develop an experimental protocol and multiple survey designs for evaluating explanations for item recommendations. Second, we present an analysis of explanations generated for a selected domain. These lead to a number of specific recommendations for evaluation of explanations in general."},{"heading":"2 RELATEDWORK","text":"The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28]. Explanations can be characterized along a number of dimensions, including their content, form of presentation, and system’s intended purpose [20]. Our interest is in the latter category, where we use the term goal to refer to the objective or purpose of the explanation. Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19]."},{"heading":"2.1 Explanation Goals","text":"Weuse the seven explanation goals identified in [26] as a basis; these are listed in Table 1. We note that there are possible refinements to these goals. For example, in [20] satisfaction is not considered as a single objective, but is split into ease to use, enjoyment, and usefulness. Nonetheless, these seven goals are regarded as the canonical categorization within explainability research for recommender systems, accurately reflecting the goals that have been studied in the past. Certain goals may be measured objectively and quantitatively. For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30]. Here, we aim to compare different goals on equal footing, and thus focus on the subjective perception of the recipient—measured at the time when a recommendation and explanation are shown.\nMost past studies are concerned with a single goal [20], and there is evidence each can be achieved individually [26]. The interactions\nbetween two or more goals, however, are much less understood. The most common explanation purpose, according to a large-scale literature review by Nunes and Jannach [20], is transparency, which is also considered key to building user trust [12]. Concerning the relationship between the two, one previous study indicates that transparency increases user trust [23], while another study finds that transparency and trust are not related [8]. The second most frequent explanation purpose is effectiveness [20], which can be conflicting with persuasiveness [7]. A systematic evaluation of explanations with respect to all goals has not been performed before."},{"heading":"2.2 Generating Explanations","text":"There is an important recognized difference between explanations (why a certain suggestion is given) and justifications (why the user may be interested in the item) [19, 27]. The former consist of an honest account of the mechanism that generated the suggestion, while the latter provides a plausible reason, whichmay be decoupled from the underlying recommendation algorithm.\nThere is a growing interest in generating natural language explanations and justifications. Given a sophisticated recommendation system, justifications may often be provided by filling in natural language templates, for example, by considering simple features such as actor and director names [24] or by extracting relevant and distinguishing characteristics from reviews [19]. However, our work focuses on explanations. Justifications have in the past been created manually using crowdsourcing [6]. A main difference between that and ours, is that we ask humans to pick the recommendation as well as explain it, while [6] perform only the latter.\nTo summarize, by obtaining recommendations and explanations from the same person, we can focus on explanations rather than justifications; we also believe the explanations obtained are genuine. Similarly, we believe recommendations are better when accompanied by an explicit explanation. An alternative of taking items from a state of the art recommender system accompanied with humangenerated justifications would introduce further limitations in any analysis."},{"heading":"2.3 Evaluating Explanations","text":"Subjective perceptions of explanations are often evaluated qualitatively based on user surveys, with responses typically given on Likert scales [6, 8, 10, 17, 21–23]. Following standard practice, we design a user survey to capture the subjective perception of users regarding the seven goals."},{"heading":"3 EXPERIMENT DESIGN","text":"This section presents our approach for generating and evaluating explanations accompanying item recommendations. We designed it to test the hypothesis that people are capable of writing explanations that satisfy different goals (RQ2), and that these explanations influence how the recipient of the explanation feels about the recommendation and the recommendation system (RQ1). Measuring all goals for the sample explanations allows the study of how the goals interact (RQ3). Given that this work is a first attempt to perform such a holistic evaluation of the effect of explanations on recipients, a number of the steps allow for different designs. Our"},{"heading":"Measuring Recommendation ExplanationQuality: The Conflicting Goals of Explanations SIGIR ’20, July 25–30, 2020, Virtual Event, China","text":"experiments test a number of alternative approaches, which we anticipate to be further refined in future work.\nOverall, our experiment involves three main steps, which are depicted in Figure 1. At a high level, these are: Step 1 Test subjects are recruited and asked about their prefer-\nences. They provide an informative description of items they like/dislike, as well as specific examples, that another person can interpret to make a recommendation.\nStep 2 Crowd workers are tasked with recommending items for test subjects from a pool of options. These workers also write a short explanation for why they made this particular choice, instructed to be written to serve a particular goal. Together with the item recommended, these explanations are treated as candidate explanations, and are filtered by other crowd workers in Step 2b to ensure high quality. Step 3 The original test subjects are provided with the recommendations, and evaluate the explanations selected by completing a questionnaire. We compare two designs for this evaluation, for sensitivity and consistency.\nWe detail each of these steps below, although we defer a detailed presentation of the mechanics of experimental conditions, and matching crowd workers to test subject profiles, to Section 4. Also, the experiments in this study are performed on the movies domain, and hence some of the instructions have been specifically tailored to that. Nevertheless, the same experimental design is applicable to any item recommendation and explanation domain."},{"heading":"3.1 Step 1: Eliciting User Preferences","text":"Test subjects are recruited via a crowdsourcing platform. To be able to generate personalized recommendations, we first need these subjects’ preferences. This is done by asking them to fill out a short questionnaire, consisting of the following four questions: (1) What sort of movies do you like? (2) Name three of your favorite movies. (3) What sort of movies do you dislike? (4) Name three movies that you really disliked (or hated).\nThis mirrors narrative-driven recommendation, often seen extemporaneously in forums, where the recommendation is driven by both information about the user’s past transactions (positive/negative examples) and a narrative describing desired items [4].\nFor the first and third questions, the required answer length is minimum 150 characters. Further, we manually checked the responses to ensure the user profiles are of high quality before inviting the test subject to continue as part of the experiment."},{"heading":"3.2 Step 2: Generating Explanations","text":"We engage a large pool of crowd workers to generate personalized item recommendations along with explanations, based on the user interest narratives provided in Step 1. This yields a set of candidate recommendations for each test subject.\nWe also note that, while our design could have been to generate explanations programmatically, this would inherently reflect just one particular interpretation of each goal—that of the designers of the algorithm and introduce another important variable in the analysis. Rather, having people explain their recommendation using\na goal-focused prompt provides a better insight into how a given goal should be satisfied. We now detail how this was done.\n3.2.1 Generating Candidate Explanations. There are two main design considerations in this step. One is to anchor the task in a realistic setting that workers can easily relate to (referred to as the context of use in [4]). Therefore crowd workers were presented with the scenario where they have to recommend a movie to a friend who will be traveling on a plane. The worker is provided with the target person’s self-described preferences, i.e., descriptions and specific examples of movies liked and disliked. The pool of available movies is described as coming from the flight entertainment system’s limited selection of movies.\nThe second design consideration is to impose a limit on the pool of items so that workers can reasonably familiarize themselves with the candidate set. We thus solicited recommendations from one of three different pools, each comprising 20 distinct movies. The pools were manually created to be disjoint yet contain a wide variety of movies to suit different tastes consisting of (1) Hollywood movies, (2) independent movies, and (3) recent movies.\nIn addition to selecting a movie to recommend, the crowd worker was asked explain the recommendation using a short description based on a goal-specific prompt. We required it to be at least 100 characters long. We note that the wording of the goal is key to our experiment. As we expected that the specific wording of the goal could affect the perceived adequacy of explanations, we tested two different wordings for each of the seven goals. In a preliminary evaluation (described in Section 3.5), we selected among these wordings.\n3.2.2 Selecting the Best Explanations. To ensure the highest quality of recommendations and explanations, a filtering process is next employed to select the highest quality explanations from those created for the recipient. This two-step process resembles the MapReduce paradigm, which has been shown to be effective for solving complex problems through micro-task platforms [16]. Specifically, we group the explanations generated for each user and goal pair. These are shown to an independent set of crowd workers, who are asked to select the best (item, explanation) pair. Table 2 lists examples of explanations voted as best for each target goal."},{"heading":"SIGIR ’20, July 25–30, 2020, Virtual Event, China Krisztian Balog and Filip Radlinski","text":"Table 2: Examples of recommendations and explanations generated by crowd workers.\nGoal Recommendation Explanation\nEffectiveness Pulp Fiction (1994) Because you have a hard time with a lot of PG or PG 13 movies and feel that they aren’t real enough for you to enjoy I think you’ll find this movie very gripping and real. It will keep you aware of harsh realities of life. Efficiency Fantastic Beasts: The Crimes of Grindelwald (2018) You would enjoy this movie as it is family friendly, doesn’t have a lot of violence and no nudity. It has a lot of magical creatures and is just a fun film. Persuasiveness Love, Simon (2018) Love Simon is a heartwarming story. It involves Simon, a young boy who is coming of age. Get ready to laugh, smile, and maybe even cry, as you follow along with Simon’s daily life. Satisfaction A Quiet Place (2018) I chose this movie because from your preferences I can tell that you enjoy movies which have some suspense and plot twists. This movie does this at a very high level. Scrutability Moneyball (2011) I chose this awesome movie for you since you really seem to like feel-good movies that have likable characters and nice stories. Transparency The Lion King (1994) Since you love love romantic comedies, comedy, mystery, some action, documentaries, biographies, Lifetime Movie Network movies, Hallmark Movies, this is a combination of them all. You will be sure to like. Trust Back to the Future (1985) I think you’ll like Back to the Future. It has a bit of everything including humor, drama, and sappy love. Yes, it is science fiction,\nbut there is so much more to it that I think you will enjoy it. I think the storyline, the acting, and the dialogue supersede your possible dislike of the genre. Besides, most of the plot takes place in the 1950s and the 1980s.\nTable 3: Survey questions for evaluating explanation goals.\nGoal Statement Reverse statement This explanation ... This explanation ...\nEffectiveness helps me to determine how well I will like this movie does not help me make a decision about this item Efficiency helps me to decide faster if I will like this movie does not save me time Persuasiveness makes me want to watch this movie fails to make this item appeal to me Satisfaction would improve how easy it is to pick a recommendation does not satisfy me Scrutability would allow me to give feedback on how well my preferences have been\nunderstood would make it difficult for me to correct the reasoning behind the recommendation\nTransparency helps me to understand what the recommendation is based on fails to reveal the reasoning behind this recommendation Trust helps me to trust the recommendation does not seem credible"},{"heading":"3.3 Step 3: Evaluating Explanations","text":"Finally, the test subjects are presented with the recommendations created for them, and asked to evaluate the corresponding explanations by filling out a questionnaire.\nWe develop two different experimental designs so as to evaluate the extent to which the explanations, generated to serve a specific goal, succeed in meeting that goal. The designs are illustrated in Figure 2. Our first is an item-wise evaluation design, which is widely used in the literature [6, 25, 27]. Here, test subjects are presented with a single recommended item along with a single explanation. We also propose an alternative list-wise evaluation design, which gives the test subject three recommended items, each from a distinct pool of movies, along with explanations for each, all for the same goal. We expect that the item-wise design has a lower cognitive load since users need to consider a single explanation. At the same time, we hypothesize the list-wise design to yield more robust observations, as responses are less likely to be influenced by the quality of a single explanation. We will therefore analyze to what degree the results obtained with the two designs align with each other.\nIn both cases, the test subject is asked to fill out a survey below the recommendation(s) and explanation(s). The survey consists of five parts:\n(1) A summary of the preferences provided earlier, meant as a refresher, displaying the data provided by the test subject in the preference elicitation phase (i.e., interest characterizations as well as names of movies liked/disliked).\n(2) An item recommendation (item-wise design) or a list of three item recommendations (list-wise design), accompanied by explanation(s).\n(3) Seven statements, presented in random order, each targeting a specific goal, to be rated on a 4-point Likert scale. These are shown in the Statement column of Table 3. The Likert scale used was (1) not at all, (2) slightly, (3) moderately, and (4) a great deal.\n(4) Seven reverse statements, to check the consistency of answers provided, are presented separately to avoid confusion [11], also in random order and each targeting one specific goal. These statements, listed in the Reverse statement column in Table 3, were created with a negative wording, to be rated on a 4-point Likert scale: (1) strongly disagree, (2) moderately disagree, (3) moderately agree, and (4) strongly agree.\n(5) We ask how personalized the explanation(s) felt on a 5-point Likert scale: (1) not at all, (2) a little, (3) a moderate amount, (4) a lot, and (5) a great deal. In the item-wise design, where recommendations are a single item, we also ask test subjects whether they have seen that movie or not. Depending on the answer, we ask a follow-up question “Would you watch it?” or “How did you like it?” In both cases, answers are given on a 5-point Likert scale. Subjects were further required to fill out a free-form text box describing what improvements they thought the explanation(s) needed.\nFor each test subject, parts (2)–(5) are repeated three times (limited to avoid survey fatigue) each time showing different items as recommendations (i.e., no movie is recommended more than once). Each iteration presented explanation(s) created by crowd workers targeting different goal(s).\nThe wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6])."},{"heading":"Measuring Recommendation ExplanationQuality: The Conflicting Goals of Explanations SIGIR ’20, July 25–30, 2020, Virtual Event, China","text":"3. Questionnaire part I (statements)\n2. Recommendation(s) + explanation(s) Item-wise design List-wise design\n4. Questionnaire part II (reverse statements)\n5. Questionnaire part III (personalization)\nrepeat 3x\nItem-wise design List-wise design\nFigure 2: Survey designs for evaluating explanations."},{"heading":"3.4 Further Quality Considerations","text":"It is known to be difficult to identify answers provided by malicious or inattentive users when collecting user opinions, and questions do not have verifiable answers [15]. Since measurements are subjective and qualitative, we take a number of precautions for quality control: (1) We restrict participation to crowd workers who are experienced and have a reputation for delivering high quality work; (2) We check for inconsistencies in ratings given to the positive and reverse statements, removing participants where the score of the positive and negative statements disagreed by more than one point at least three times; (3) We require participants to fill out a free-text box, ensuring they had attended to the task."},{"heading":"3.5 Goal Wording Calibration","text":"We hypothesize that the instructions given to crowd workers writing explanations affect how the explanations are perceived. Therefore, our crowdsourcing experiment was preceded by a calibration study to select the exact wording of instructions that resulted in the best explanations for each goal.\nSpecifically, 5,188 recommendations and explanations were generated for 120 test subjects (for all seven goals for each test subject), where two alternative wordings of instructions for each goal were used. This resulted in 6.18 explanations on average per (test subject, goal) pair. Then, a second group of crowd workers were asked to select the single best explanation that best serves the given goal from this set. The two alternative wordings with the corresponding statistics, as well as the wording given to voters, are shown in Table 4.\nWe test the results for significance using a Binomial test. Statistically significant differences at the 95% confidence level are found for two of the goals, namely Scrutability and Transparency. This confirms our hypothesis that wording can have a strong impact. In our main experiment, detailed below, we used the instruction for each goal that received the higher percentage of votes as shown in the table."},{"heading":"SIGIR ’20, July 25–30, 2020, Virtual Event, China Krisztian Balog and Filip Radlinski","text":""},{"heading":"4 DATA COLLECTION","text":"We performed our user study on a large crowdsourcing platform. Our worker pool consisted of workers in the United States with a high (over 98%) task approval rate. Personal worker information, including demographics, was not judged essential hence was not collected. However, the worker pool is known to consist of diverse crowd workers."},{"heading":"4.1 Mechanics and Experimental Conditions","text":"4.1.1 Step 1: Eliciting User Preferences. In Step 1 of our study, we collected preferences from 240 test subjects (after filtering out low quality responses). There were no experimental conditions, as all test subjects experienced the same survey.\n4.1.2 Step 2: Generating Explanations. The second step consisted of two sub-parts. In Step 2a, crowd workers were tasked with generating personalized recommendations and corresponding explanations with a given target goal and movie pool as the experimental conditions. Each micro-task asked for recommendations for 5 different user profiles with the same target goal. We solicited recommendations from three separate movie pools (Hollywood, independent, and recent movies), each consisting of 20 highly rated movies. To ensure diversity, workers were restricted to doing at most 10 microtasks. A total of 7,340 personalized explanations were generated by 702 crowd workers.1 The average task completion time was 534s, that is, 107s for making a single recommendation and writing the corresponding explanation.\nIn a subsequent filtering step (2b), other workers were asked to vote on the best explanation for each test subject and goal pair. Each micro-task contained explanations targeting a specific goal (3.4 explanations on average) for 5 test subjects.\nTable 5 displays descriptive statistics of the data collected (after filtering participants with three or more inconsistent responses).\nBaseline. As a baseline, “neutral” explanations were also collected. Specifically, the description of movies (limited to 500 characters) was extracted from the information panel of a major web search\n1Since our surveys evaluate three goals for each study subject, we only generate explanations for those goals that will actually be shown in the surveys, and not for all possible subject-goal pairs.\nengine. We note that these are not personalized, and are usually written by human experts per movie. As such they are likely to have high quality.\n4.1.3 Step 3: Evaluating Explanations. In Step 3, each test subject was presented with explanations for three different goals. The experimental conditions were the combination of goals and the experiment design (item-wise or list-wise). Each test subject was assigned a combination of three goals in a Round Robin fashion, such that the number of observations for each combination of goals is approximately the same. The order of the three goals was randomized for each survey. For each target goal, the best (i.e., highest voted) explanation was chosen across the three movie pools, with the corresponding movie shown as the recommendation.\nTo compare to the baseline explanation, each test subject was also presented with a fourth case: We selected the movie that was recommended by the most workers, but that had not been selected for any of the three goals evaluated for this particular subject. This movie was then shown to the test subject along with the neutral (baseline) explanation.\nWe made sure that the same movie is not repeated for a given subject in case of the item-wise design, and that the list of suggestions always had three distinct movies for the list-wise design. To avoid low quality responses, those with a too short completion time (below 60 seconds) were removed.\nA total of 107 study subjects provided valid responses in this final step (amounting to a 44.5% return from Step 1). We obtained 84 responses for the item-wise design and 88 for the list-wise design, with 65 participants completing both.2 Each target goal evaluated by a study subject constitutes an observation. Thus, the total number of observations is three times the number of responses.\n2For those that participated in both designs, at least one week elapsed between the two surveys."},{"heading":"Measuring Recommendation ExplanationQuality: The Conflicting Goals of Explanations SIGIR ’20, July 25–30, 2020, Virtual Event, China","text":""},{"heading":"5 RESULTS","text":"The aggregate results of the perceived quality of the recommendations generated for each goal are presented in Table 6. In this section we present a detailed analysis of these results. As a high level summary, we found that different goals have a strong effect on how users reacted to explanations. We found that some pairs of goals are significantly more correlated than others. We also found that the item-wise design was most sensitive."},{"heading":"5.1 Metric Correlation","text":"We start the analysis by considering the different goals as metrics for the quality of explanations, independently of how they are generated. Thus, for the moment, the approaches used to create the explanations are not relevant. Rather, we consider how test subjects respond to recommendations along the seven goal dimensions, and how the different goal metrics interact. For example, does higher Persuasiveness correlate with lower Trust?\nFigure 3 shows the Pearson Correlation between each pair of metrics, by analyzing the responses of each user who was presented with a recommendation across both experimental conditions. For each correlation, the figure also shows a 95% confidence interval for the correlation measured using bootstrap sampling. Note also that we compared the correlations in the item-wise and list-wise designs, as well as comparing the correlations taking only seen versus unseen movies, or highly rated versus low rated recommendations. In all cases, we found that the correlations do not differ substantially, nor with statistical significance.\nWe find that the explanation metrics are strongly correlated on average, yet exhibit clear structure. All seven goals have at least moderate correlation. Effectiveness and Efficiency are particularly strongly correlated. We see that five goals, namely Persuasiveness, Trust, Satisfaction, Effectiveness and Efficiency, appear to move together. It is particularly interesting to see that for instance Persuasiveness and Trust are strongly correlated. On the other hand, Transparency and Scrutability stand out as less correlated with each other, and with all the other five goals.\nOne possible reason is that there is only a limited amount of information that can be conveyed in a few hundred characters, and hence it is difficult to tailor an explanation to a particular goal. Further, we hypothesize that a different result may be observed if explanations were longer and more detailed. On the other hand, for typical use-cases, this is about the desired explanation length because of limited screen real-estate. As argued by Tintarev and Masthoff [25], “brevity is important in a context where the user has to review many possible options.” An alternative possible reason considered was that crowd workers may not be very good at creating explanations for any given goal, and that the explanations rather optimize a goal that depends more on the worker writing the explanation than on the instructions provided. However, our results in Table 6 show this not to be the case, as the different goals presented to workers generating explanations result in significantly different values of the scores for each goal.\nWe conclude that Satisfaction is most correlated metric with all goals for explanations, and is the single most predictive goal. The most distinct secondary metric is Scrutability, which is also consistent with intuition that an invitation for feedback may create\na different user experience. Finally, Transparency is third most distinct metric.\nAn important limitation that must be considered in our design is whether the questions proposed in Table 3 actually measure the goals intended to measure. Even though there exist ways to measure individual explanation goals, there is no established evaluation approach for comparing and contrasting multiple goals systematically and holistically, thus a direct comparison to other approaches is not possible. Following DeVellis [11] recommendations for scale development, we measure the internal consistency of each metric using Cronbach’s \uD835\uDEFC [9] for each goal. As shown in Table 7, we find that most metrics have suitable internal consistency, with Scrutability and Trust somewhat lower.3 Thus we see that the answers to the positive and negative questions mirror each other as designed, despite the questions intentionally using significantly different wording. While refinements to the questions are likely possible, we are confident that the goal labels correspond to the intention of the questions."},{"heading":"5.2 Goal-Metric Agreement","text":"Next, taking the goals targeted into account, we start with a surface level analysis of the agreement between the goal targeted by the crowd worker, and the ratings provided by the test subject for explanations.\nTable 6 shows each goal targeted as one row, with the number of test subjects who received recommendations for this goal indicated. We show the mean score for each metric, recalling that these were collected on a Likert scale of 1 to 4, taking the average of the positive and reverse statement scores from Table 3 for each test subject. An ↓ symbol indicates that the score for that metric is statistically significantly worse than the maximum for the goal optimized (using a two sample t-test, with 95% confidence).\nWe may expect a diagonal form for the matrix: the maximum of each metric may be expected to coincide with the same goal being targeted by the people who produced the explanations. This is not the result seen. Rather, we observe that some target goals are much more effective at producing high scoring explanations across all metrics. This tells us that the wording of the goal provided to the crowd workers is critical in determining the quality of the explanations. Even if some of the goals are considered more important to a system designer, it may be that better explanations are obtained by instructing crowd workers to optimize a different goal.\nA second key observation is that the neutral explanation performs particularly well. Recall that this is a synopsis taken from the information panel of a major web search engine. Thus it is not personalized to the user, and usually consists of basic metadata about the movie recommended. One possible explanation that must be excluded is that crowd workers may be particularly poor at generating explanations for a given goal, especially in comparison to a neutral sentence that is likely to have been reviewed and refined by a number of experts. However, the crowd workers are always capable of creating explanations that are equally as good as the neutral explanation, and the explanations have been filtered in Step 3We also tested the Cronbach’s \uD835\uDEFC without filtering the 29 (6%) raters with many inconsistent responses. This results in Cronbach’s\uD835\uDEFC values around 0.07 lower. However, as crowd workers are known to produce answers with variable quality, we consider the filtering appropriate for our setting."},{"heading":"SIGIR ’20, July 25–30, 2020, Virtual Event, China Krisztian Balog and Filip Radlinski","text":"2b of our experiment. This raises a question of potential headroom, whether it is possible to outperform a neutral summary with an explanation consisting of 100-200 characters on average."},{"heading":"5.3 Experimental Design Analysis","text":"Next, we compare the two experiment designs. We can see in Table 6 that the item-wise experiment design is much more sensitive to differences among the different goals. There are almost no statistically significant differences in metric performance in the list-wise design across all goals, given very similar sample sizes across the two designs. This surprising result suggests a further analysis of the limitations of the two designs. In particular, we expected that by presenting a single item, the perceived quality of the explanation would be strongly influenced by the specific item recommended. Recipients may react to the explanations differently based on whether the item recommended appears relevant or not, especially if they have already watched the movie. We assess this next.\nFigure 4 presents a more fine-grained view of the results, breaking down the item-wise design split by these two effects. The top row splits the explanation ratings by whether or not the recommendation was considered good. Specifically, if the test subject had seen the movie, they were asked if they liked the movie. Otherwise, the test subject was asked if they would watch it. Ratings of 4 or 5 were considered “rated highly”, the others were considered as “rated low”. We find that most (69%) of recommendations were considered good. We also see that there is a large difference in mean score across all values, supporting our hypothesis.\nFinally, the bottom row in Figure 4 splits the recommendations by whether the test subject had already seen the movie or not. This is roughly an even split, and we see a similar effect: Explanations for movies the user has already seen receive substantially different scores.\nReturning to the motivating task for this paper, namely that of making and explaining recommendations, we believe that the results on unseen items are the most important condition — such recommendations provide the most value to the recipients. Combining with the observation that the item-wise approach is more sensitive, this suggests that future work on evaluating the impact of explanations should filter evaluations for items that have not already been seen (or otherwise consumed by) the test subject to avoid seen items affecting the measured performance metrics. It is important to note, however, that despite these differences by item rating and consumption seen here, the correlation between metrics is not affected by these conditions."},{"heading":"Measuring Recommendation ExplanationQuality: The Conflicting Goals of Explanations SIGIR ’20, July 25–30, 2020, Virtual Event, China","text":""},{"heading":"6 DISCUSSION","text":"We now revisit our research questions and answer them based on the experimental results obtained. We further suggest some points of advice for evaluation of the impact of recommendation explanations on people who receive such explanations.\n(RQ1) How can one robustly measure if an explanation provided with a recommendation creates the intended effect on the recipient? We compared two alternative survey designs, consisting the same series of question and reverse pairs for each explanation goal. These questions showed good internal consistency. The item-wise experimental design appears statistically more powerful than the list-wise one, although it is likely to be beneficial to filter for items that are novel recommendations to the recipient (i.e., the user has not seen/consumed them).\n(RQ2) Can ordinary people write explanations that optimize a given goal, and if so, how does that target goal affect how the explanation is perceived by recipients of recommendations? We showed that crowd workers can indeed generate explanations that are equally\nas good as neutral explanations written by experts. The precise wording of the goal of the explanations is found to be key to obtaining high quality explanations. However, the wording need not necessarily align with the goals considered most important by a system designer.\n(RQ3) How do different goals relate to each other, or more specifically does optimizing particular goals reduce or increase the extent to which other goals are satisfied? Can recipients of recommendations even distinguish the goals from each other? We found that all seven goals are moderately correlated, while some pairs are particularly strongly correlated with each other. As measurement of the impact of explanations on recipients is expensive, it appears that Satisfaction, Scrutability and Transparency — if they are desirable properties for a given system — may provide the most complete assessment of explanation quality across the seven established goals from [26]."},{"heading":"SIGIR ’20, July 25–30, 2020, Virtual Event, China Krisztian Balog and Filip Radlinski","text":""},{"heading":"6.1 Limitations","text":"As a first analysis of the interaction of different goals for explanations, this study is not without limitations. First, we performed this analysis in a single domain, with a single length limit to explanations. It must be verified that our findings transfer to domains beyond movies. It must also be assessed whether the results generalize to recommendations made by a state-of-the-art algorithm rather than crowd workers. Differences among individuals generating explanations, as well as individuals receiving explanations, may affect results—although this effect would be better teased apart at larger scales where fewer experimental design variants are considered. Expert copy-writers may also be able to generate more compelling explanations.\nFrom the perspective of measurement, it is likely that there exist refinements to the questionnaire that would increase sensitivity and internal consistency. It is also likely that further refinements to the experiment design would increase sensitivity. It should be noted that the inherent correlation and dependencies of some of the goals makes this particularly difficult to control for in any design."},{"heading":"7 CONCLUSION","text":"We have presented a first analysis of how different goals, or intentions behind recommendation explanations, can be measured. We found that when treated as metrics, the seven goals are often highly correlated, yet exhibit clear structure and interact. This suggests that it may not be necessary to separately consider so many distinct goals. In asking crowd workers to generate explanations, we also found that the wording of the goal has a strong impact on the quality of the explanations as perceived by the test subjects. Two experiment designs were presented, and found to exhibit similar patterns yet different sensitivity and some biases depending on the items recommended rather than just the explanations.\nThis study represents a step towards the development of appropriate evaluation methodology for explainable recommender systems. Future directions concern the generalization of findings to other domains and further refinements to the survey and experimental designs. While our main focus has been on the measurement aspects, our methodology also facilitates the large-scale collection of high-quality explanations for a given goal. The collected datamay be utilized for generating explanations automatically (for example, as training data for generative neural models)."}],"references":[{"title":"Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda","author":["Ashraf Abdul","Jo Vermeulen","Danding Wang","Brian Y. Lim","Mohan Kankanhalli"],"venue":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","citeRegEx":"1","shortCiteRegEx":"1","year":2018},{"title":"Transparent, Scrutable and Explainable User Models for Personalized Recommendation","author":["Krisztian Balog","Filip Radlinski","Shushan Arakelyan"],"venue":"In The 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"2","shortCiteRegEx":"2","year":2019},{"title":"Explaining Recommendations: Satisfaction vs. Promotion","author":["Mustafa Bilgic","Raymond J. Mooney"],"venue":"In Proceedings of Beyond Personalization 2005: A Workshop on the Next Stage of Recommender Systems Research at the 2005 International Conference on Intelligent User Interfaces","citeRegEx":"3","shortCiteRegEx":"3","year":2005},{"title":"Defining and Supporting Narrative-driven Recommendation","author":["Toine Bogers","Marijn Koolen"],"venue":"In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys","citeRegEx":"4","shortCiteRegEx":"4","year":2017},{"title":"Rule Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project (The Addison- Wesley Series in Artificial Intelligence)","author":["Bruce G. Buchanan","Edward H. Shortliffe"],"venue":null,"citeRegEx":"5","shortCiteRegEx":"5","year":1984},{"title":"Crowd-Based Personalized Natural Language Explanations for Recommendations","author":["Shuo Chang","F. Maxwell Harper","Loren Gilbert Terveen"],"venue":"In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys","citeRegEx":"6","shortCiteRegEx":"6","year":2016},{"title":"Sentiment-enhanced Explanation of Product Recommendations","author":["Li Chen","Feng Wang"],"venue":"In Proceedings of the 23rd International Conference on World Wide Web (WWW","citeRegEx":"7","shortCiteRegEx":"7","year":2014},{"title":"The Effects of Transparency on Trust in and Acceptance of a Content-based Art Recommender","author":["Henriette Cramer","Vanessa Evers","Satyan Ramlal","Maarten Someren","Lloyd Rutledge","Natalia Stash","Lora Aroyo","Bob Wielinga"],"venue":"User Modeling and User-Adapted Interaction","citeRegEx":"8","shortCiteRegEx":"8","year":2008},{"title":"Coefficient Alpha and the Internal Structure of Tests","author":["Lee J. Cronbach"],"venue":"Psychometrika 16,","citeRegEx":"9","shortCiteRegEx":"9","year":1951},{"title":"A scrutable adaptive hypertext","author":["Marek Czarkowski"],"venue":"Ph.D. Dissertation. University of Sydney","citeRegEx":"10","shortCiteRegEx":"10","year":2006},{"title":"Scale development: Theory and applications","author":["Robert F. DeVellis"],"venue":"Applied Social Research Methods,","citeRegEx":"11","shortCiteRegEx":"11","year":1991},{"title":"Understanding Recommendations by Reading the Clouds","author":["Fatih Gedikli","Mouzhi Ge","Dietmar Jannach"],"venue":"In E-Commerce and Web Technologies","citeRegEx":"12","shortCiteRegEx":"12","year":2011},{"title":"How Should I Explain? A Comparison of Different Explanation Types for Recommender Systems","author":["Fatih Gedikli","Dietmar Jannach","Mouzhi Ge"],"venue":"Int. J. Hum.-Comput. Stud. 72,","citeRegEx":"13","shortCiteRegEx":"13","year":2014},{"title":"Explaining Collaborative Filtering Recommendations","author":["Jonathan L. Herlocker","Joseph A. Konstan","John Riedl"],"venue":"In Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work (CSCW","citeRegEx":"14","shortCiteRegEx":"14","year":2000},{"title":"Crowdsourcing User Studies with Mechanical Turk","author":["Aniket Kittur","Ed H. Chi","Bongwon Suh"],"venue":"In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI","citeRegEx":"15","shortCiteRegEx":"15","year":2008},{"title":"CrowdForge: Crowdsourcing Complex Work","author":["Aniket Kittur","Boris Smus","Robert Kraut"],"venue":"Extended Abstracts on Human Factors in Computing Systems (CHI EA","citeRegEx":"16","shortCiteRegEx":"16","year":2011},{"title":"User Preferences for Hybrid Explanations","author":["Pigi Kouki","James Schaffer","Jay Pujara","John O’Donovan","Lise Getoor"],"venue":"In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys","citeRegEx":"17","shortCiteRegEx":"17","year":2017},{"title":"AI, Explain Yourself","author":["Don Monroe"],"venue":"Communications of the ACM 61,","citeRegEx":"18","shortCiteRegEx":"18","year":2018},{"title":"Justifying Recommendations Through Aspect-based Sentiment Analysis of Users Reviews","author":["Cataldo Musto","Pasquale Lops","Marco de Gemmis","Giovanni Semeraro"],"venue":"In Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization (UMAP","citeRegEx":"19","shortCiteRegEx":"19","year":2019},{"title":"A Systematic Review and Taxonomy of Explanations in Decision Support and Recommender Systems","author":["Ingrid Nunes","Dietmar Jannach"],"venue":"User Model. User-Adap. Inter. 27,","citeRegEx":"20","shortCiteRegEx":"20","year":2017},{"title":"Trust Building with Explanation Interfaces","author":["Pearl Pu","Li Chen"],"venue":"In Proceedings of the 11th International Conference on Intelligent User Interfaces (IUI","citeRegEx":"21","shortCiteRegEx":"21","year":2006},{"title":"Explaining Recommendations Using Contexts","author":["Masahiro Sato","Budrul Ahsan","Koki Nagatani","Takashi Sonoda","Qian Zhang","Tomoko Ohkuma"],"venue":"In 23rd International Conference on Intelligent User Interfaces (IUI","citeRegEx":"22","shortCiteRegEx":"22","year":2018},{"title":"The Role of Transparency in Recommender Systems","author":["Rashmi Sinha","Kirsten Swearingen"],"venue":"In CHI ’02 Extended Abstracts on Human Factors in Computing Systems (CHI EA","citeRegEx":"23","shortCiteRegEx":"23","year":2002},{"title":"Justified Recommendations based on Content and Rating Data","author":["Panagiotis Symeonidis","Ros Nanopoulos","Yannis Manolopoulos"],"venue":"In WebKDD workshop on Web Mining and Web Usage Analysis","citeRegEx":"24","shortCiteRegEx":"24","year":2008},{"title":"Evaluating the Effectiveness of Explanations for Recommender Systems","author":["Nava Tintarev","Judith Masthoff"],"venue":"User Modeling and User-Adapted Interaction 22,","citeRegEx":"25","shortCiteRegEx":"25","year":2012},{"title":"Explaining Recommendations: Design and Evaluation","author":["Nava Tintarev","Judith Masthoff"],"venue":"In Recommender Systems Handbook","citeRegEx":"26","shortCiteRegEx":"26","year":2015},{"title":"Tagsplanations: Explaining Recommendations Using Tags","author":["Jesse Vig","Shilad Sen","John Riedl"],"venue":"In Proceedings of the 14th International Conference on Intelligent User Interfaces (IUI","citeRegEx":"27","shortCiteRegEx":"27","year":2009},{"title":"The Impact of Explanation Facilities on User Acceptance of Expert Systems Advice","author":["L. Richard Ye","Paul E. Johnson"],"venue":"MIS Quarterly 19,","citeRegEx":"28","shortCiteRegEx":"28","year":1995},{"title":"Explainable Recommendation: A Survey and New Perspectives","author":["Yongfeng Zhang","Xu Chen"],"venue":null,"citeRegEx":"29","shortCiteRegEx":"29","year":2018},{"title":"Explicit Factor Models for Explainable Recommendation Based on Phrase-level Sentiment Analysis","author":["Yongfeng Zhang","Guokun Lai","Min Zhang","Yi Zhang","Yiqun Liu","Shaoping Ma"],"venue":"In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR","citeRegEx":"30","shortCiteRegEx":"30","year":2014}],"referenceMentions":[{"referenceID":12,"context":"Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.","startOffset":89,"endOffset":105},{"referenceID":13,"context":"Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.","startOffset":89,"endOffset":105},{"referenceID":22,"context":"Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.","startOffset":89,"endOffset":105},{"referenceID":27,"context":"Explanations are known to strongly impact how the recipient of a recommendation responds [13, 14, 23, 28], yet the effect is still not well understood.","startOffset":89,"endOffset":105},{"referenceID":0,"context":"This has increased attention on explainable and transparent AI, both from technical and ethical perspectives [1, 18].","startOffset":109,"endOffset":116},{"referenceID":17,"context":"This has increased attention on explainable and transparent AI, both from technical and ethical perspectives [1, 18].","startOffset":109,"endOffset":116},{"referenceID":4,"context":"3401032 While explainable system design is not new (dating back to rulebased expert systems of the 1980s [5]), the role of explanations has gained more attention in the past decade [29].","startOffset":105,"endOffset":108},{"referenceID":28,"context":"3401032 While explainable system design is not new (dating back to rulebased expert systems of the 1980s [5]), the role of explanations has gained more attention in the past decade [29].","startOffset":181,"endOffset":185},{"referenceID":25,"context":"Our work starts with seven main goals of explanations, proposed by Tintarev and Masthoff [26]: transparency, intended to explain how the systemworks; scrutability, allowing users to tell the system if it is wrong; trust, increasing users’ confidence in the system; effectiveness, helping users to make good decisions; efficiency, helping users to make decisions faster; persuasiveness, trying to convince users to select the given item; and satisfaction, increasing the ease of use of a system.","startOffset":89,"endOffset":93},{"referenceID":25,"context":"They argued that these goals should be identified as distinct, even if they may interact [26].","startOffset":89,"endOffset":93},{"referenceID":19,"context":"Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26].","startOffset":72,"endOffset":76},{"referenceID":7,"context":"Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26].","startOffset":121,"endOffset":132},{"referenceID":12,"context":"Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26].","startOffset":121,"endOffset":132},{"referenceID":25,"context":"Most previous studies on generating explanations optimize a single goal [20], and only a handful consider multiple goals [8, 13, 26].","startOffset":121,"endOffset":132},{"referenceID":25,"context":"Table 1: Explanation goals and their definitions [26].","startOffset":49,"endOffset":53},{"referenceID":12,"context":"The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28].","startOffset":156,"endOffset":172},{"referenceID":13,"context":"The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28].","startOffset":156,"endOffset":172},{"referenceID":22,"context":"The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28].","startOffset":156,"endOffset":172},{"referenceID":27,"context":"The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13, 14, 23, 28].","startOffset":156,"endOffset":172},{"referenceID":19,"context":"Explanations can be characterized along a number of dimensions, including their content, form of presentation, and system’s intended purpose [20].","startOffset":141,"endOffset":145},{"referenceID":19,"context":"Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19].","startOffset":122,"endOffset":126},{"referenceID":1,"context":"Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19].","startOffset":140,"endOffset":150},{"referenceID":5,"context":"Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19].","startOffset":140,"endOffset":150},{"referenceID":18,"context":"Specifically, our focus is on natural language explanations, the most commonly used way of presentation both historically [20] and recently [2, 6, 19].","startOffset":140,"endOffset":150},{"referenceID":25,"context":"Weuse the seven explanation goals identified in [26] as a basis; these are listed in Table 1.","startOffset":48,"endOffset":52},{"referenceID":19,"context":"For example, in [20] satisfaction is not considered as a single objective, but is split into ease to use, enjoyment, and usefulness.","startOffset":16,"endOffset":20},{"referenceID":2,"context":"For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].","startOffset":150,"endOffset":156},{"referenceID":5,"context":"For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].","startOffset":150,"endOffset":156},{"referenceID":12,"context":"For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].","startOffset":217,"endOffset":221},{"referenceID":5,"context":"For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].","startOffset":248,"endOffset":251},{"referenceID":29,"context":"For example, effectiveness may be measured as the change of a user’s rating of (or reported interest in) an item before and after consuming that item [3, 6], efficiency may be measured by time spent on rating an item [13] or reading an explanation [6], and persuasiveness may be measured in terms of click through rate [30].","startOffset":319,"endOffset":323},{"referenceID":19,"context":"Most past studies are concerned with a single goal [20], and there is evidence each can be achieved individually [26].","startOffset":51,"endOffset":55},{"referenceID":25,"context":"Most past studies are concerned with a single goal [20], and there is evidence each can be achieved individually [26].","startOffset":113,"endOffset":117},{"referenceID":19,"context":"The most common explanation purpose, according to a large-scale literature review by Nunes and Jannach [20], is transparency, which is also considered key to building user trust [12].","startOffset":103,"endOffset":107},{"referenceID":11,"context":"The most common explanation purpose, according to a large-scale literature review by Nunes and Jannach [20], is transparency, which is also considered key to building user trust [12].","startOffset":178,"endOffset":182},{"referenceID":22,"context":"Concerning the relationship between the two, one previous study indicates that transparency increases user trust [23], while another study finds that transparency and trust are not related [8].","startOffset":113,"endOffset":117},{"referenceID":7,"context":"Concerning the relationship between the two, one previous study indicates that transparency increases user trust [23], while another study finds that transparency and trust are not related [8].","startOffset":189,"endOffset":192},{"referenceID":19,"context":"The second most frequent explanation purpose is effectiveness [20], which can be conflicting with persuasiveness [7].","startOffset":62,"endOffset":66},{"referenceID":6,"context":"The second most frequent explanation purpose is effectiveness [20], which can be conflicting with persuasiveness [7].","startOffset":113,"endOffset":116},{"referenceID":18,"context":"There is an important recognized difference between explanations (why a certain suggestion is given) and justifications (why the user may be interested in the item) [19, 27].","startOffset":165,"endOffset":173},{"referenceID":26,"context":"There is an important recognized difference between explanations (why a certain suggestion is given) and justifications (why the user may be interested in the item) [19, 27].","startOffset":165,"endOffset":173},{"referenceID":23,"context":"Given a sophisticated recommendation system, justifications may often be provided by filling in natural language templates, for example, by considering simple features such as actor and director names [24] or by extracting relevant and distinguishing characteristics from reviews [19].","startOffset":201,"endOffset":205},{"referenceID":18,"context":"Given a sophisticated recommendation system, justifications may often be provided by filling in natural language templates, for example, by considering simple features such as actor and director names [24] or by extracting relevant and distinguishing characteristics from reviews [19].","startOffset":280,"endOffset":284},{"referenceID":5,"context":"Justifications have in the past been created manually using crowdsourcing [6].","startOffset":74,"endOffset":77},{"referenceID":5,"context":"A main difference between that and ours, is that we ask humans to pick the recommendation as well as explain it, while [6] perform only the latter.","startOffset":119,"endOffset":122},{"referenceID":3,"context":"This mirrors narrative-driven recommendation, often seen extemporaneously in forums, where the recommendation is driven by both information about the user’s past transactions (positive/negative examples) and a narrative describing desired items [4].","startOffset":245,"endOffset":248},{"referenceID":3,"context":"One is to anchor the task in a realistic setting that workers can easily relate to (referred to as the context of use in [4]).","startOffset":121,"endOffset":124},{"referenceID":15,"context":"This two-step process resembles the MapReduce paradigm, which has been shown to be effective for solving complex problems through micro-task platforms [16].","startOffset":151,"endOffset":155},{"referenceID":5,"context":"Our first is an item-wise evaluation design, which is widely used in the literature [6, 25, 27].","startOffset":84,"endOffset":95},{"referenceID":24,"context":"Our first is an item-wise evaluation design, which is widely used in the literature [6, 25, 27].","startOffset":84,"endOffset":95},{"referenceID":26,"context":"Our first is an item-wise evaluation design, which is widely used in the literature [6, 25, 27].","startOffset":84,"endOffset":95},{"referenceID":10,"context":"(4) Seven reverse statements, to check the consistency of answers provided, are presented separately to avoid confusion [11], also in random order and each targeting one specific goal.","startOffset":120,"endOffset":124},{"referenceID":25,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":76,"endOffset":80},{"referenceID":9,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":163,"endOffset":167},{"referenceID":16,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":184,"endOffset":192},{"referenceID":21,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":184,"endOffset":192},{"referenceID":5,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":207,"endOffset":214},{"referenceID":21,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":207,"endOffset":214},{"referenceID":7,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":229,"endOffset":236},{"referenceID":9,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":229,"endOffset":236},{"referenceID":5,"context":"The wording of survey questions is based on the definitions of the goals in [26] and is inspired by questions asked in prior work (specifically, for effectiveness [10], persuasiveness [17, 22], satisfaction [6, 22], transparency [8, 10], and trust [6]).","startOffset":248,"endOffset":251},{"referenceID":14,"context":"It is known to be difficult to identify answers provided by malicious or inattentive users when collecting user opinions, and questions do not have verifiable answers [15].","startOffset":167,"endOffset":171},{"referenceID":24,"context":"As argued by Tintarev and Masthoff [25], “brevity is important in a context where the user has to review many possible options.","startOffset":35,"endOffset":39},{"referenceID":10,"context":"Following DeVellis [11] recommendations for scale development, we measure the internal consistency of each metric using Cronbach’s α [9] for each goal.","startOffset":19,"endOffset":23},{"referenceID":8,"context":"Following DeVellis [11] recommendations for scale development, we measure the internal consistency of each metric using Cronbach’s α [9] for each goal.","startOffset":133,"endOffset":136},{"referenceID":10,"context":"Goal Measured Cronbach’s α Interpretation [11]","startOffset":42,"endOffset":46},{"referenceID":25,"context":"As measurement of the impact of explanations on recipients is expensive, it appears that Satisfaction, Scrutability and Transparency — if they are desirable properties for a given system — may provide the most complete assessment of explanation quality across the seven established goals from [26].","startOffset":293,"endOffset":297}],"year":2020,"abstractText":"Explanations have a large effect on how people respond to recommendations. However, there are many possible intentions a system may have in generating explanations for a given recommendation— from increasing transparency, to enabling a faster decision, to persuading the recipient. As a good explanation for one goal may not be good for others, we address the questions of (1) how to robustly measure if an explanation meets a given goal and (2) how the different goals interact with each other. Specifically, this paper presents a first proposal of how to measure the quality of explanations along seven common goal dimensions catalogued in the literature.We find that the seven goals are not independent, but rather exhibit strong structure. Proposing two novel explanation evaluation designs, we identify challenges in evaluation, and provide more efficient measurement approaches of explanation quality.","creator":"LaTeX with acmart 2019/12/18 v1.66 Typesetting articles for the Association for Computing Machinery and hyperref 2020/01/14 v7.00d Hypertext links for LaTeX"}}
{"name":"3397271.3401050.pdf","metadata":{"source":"META","title":"Using Phoneme Representations to Build Predictive Models Robust to ASR Errors","authors":["Anjie Fang","Simone Filice","Nut Limsopatham","Oleg Rokhlenko"],"emails":["njfn@amazon.com","filicesf@amazon.com","nutli@microsoft.com","olegro@amazon.com","permissions@acm.org."],"sections":[{"heading":null,"text":"CCS CONCEPTS •Computingmethodologies→Natural language processing; Neural networks; Speech recognition; Phonology / morphology.\nKEYWORDS Phoneme Embeddings, Deep Learning, Natural Language Understanding, ASR Errors, Virtual Assistant\n∗This work was completed while Nut Limsopatham was at Amazon.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401050\nACM Reference Format: Anjie Fang, Simone Filice, Nut Limsopatham, and Oleg Rokhlenko. 2020. Using Phoneme Representations to Build Predictive Models Robust to ASR Errors. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25– 30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3397271.3401050"},{"heading":"1 INTRODUCTION","text":"Nowadays, voice-enabled systems are gaining more and more popularity, and virtual assistants, such as Amazon Alexa, Apple Siri or Google Home, are becoming part of our daily life. In particular, they have been used, for example, for accessing contents on theWeb, controlling smart devices, and managing calendars, through different applications, e.g. voice search engine [35] and voice shopping [14].\nIn such systems, the Spoken Language Understanding (SLU) is usually performed in two steps: first an Automatic Speech Recognition (ASR) is used to transcribe human speech; then Natural Language Understanding (NLU) models are applied on ASR transcriptions to interpret users’ requests. Different from traditional approaches, where NLU is applied on the original text, applying it on ASR transcriptions poses new challenges, as ASR systems often generate transcriptions with errors [6, 20]. These ASR errors can cause failures in downstream applications of virtual assistants, such as intention classification or slot filling [28], affecting the end-user experience.\nTraditionally, researchers distinguish between three main types of ASR errors: insertions, deletions, and substitutions. However, all these errors are just an outcome of a phonetic confusion in the ASR model, causing a phrase in a human speech to be incorrectly transcribed to a “quasi-oronym\", i.e., phrases with different meanings that sound very similar. Therefore, classic approaches that operate on word or even character-level representations cannot recover from such errors. In this paper we explore the usage of lower level representations, namely phoneme-based representations, to alleviate this problem. As phonemes are the smallest units of sound in a language, we expect the ASR transcription to be more similar to the correct utterance at the phoneme level than at the character or word levels. Hence, we argue that injecting phonetic information into NLU models can improve their robustness to ASR errors. More specifically, we propose to represent ASR transcriptions as sequences of phonemes. Following the deep learning approach for text processing, we map phonemes to phoneme embeddings and propose several methods to train phoneme embeddings that are able to capture pronunciation similarities. Finally, we use these\npre-trained embeddings as inputs to Neural Network architectures for solving NLU tasks.\nThe contribution of this paper is fourfold: (i) we design fourmethods for training phoneme embeddings using sequence-to-sequence and word2vec-based models, and evaluate them; (ii) we define a pipeline for contaminating existing datasets with ASR errors, and we use this pipeline to generate noisy versions of four well-known Natural Language Processing datasets1; (iii) we describe how to integrate phoneme embeddings into existing Neural Network architectures, e.g., LSTM and CNN, showing how the proposed phoneme embeddings can be jointly usedwith standard embeddings, i.e., character and word embeddings; (iv) we conduct an intensive experimental evaluation on the generated datasets, as well as on real utterances spoken to the Alexa virtual assistant; our experimental results demonstrate that models exploiting our phoneme representation can significantly improve classification performance on datasets containing ASR errors compared to models operating only on standard character or word representations.\nThe rest of the paper is organized as follows: Section 2 discusses related work. Section 3 describes our phoneme-level representations and the proposed methods to automatically learn phoneme embeddings. Section 4 explains the data generation pipeline that is used to automatically generate datasets containing ASR errors. We run a qualitative analysis of our phoneme embedding spaces in Section 5 and report experimental results on the tasks of sentiment analysis and question classification in Section 6 using the generated datasets. In Section 7 we confirm these results by investigating the domain classification task on a real Alexa dataset. Finally, we provide concluding remarks in Section 8."},{"heading":"2 RELATEDWORK","text":"Some previous works have explored the possibility of using error detection systems to trigger clarification questions to users. Tam et al. [33] tackled the error detection task with a Recurrent Neural Network, while Pellegrini and Trancoso [23] used features obtained from different knowledge sources to complement an encoderdecoder model.\nOther works tried to directly correct the ASR transcriptions. Sarma and Palmer [27] proposed an unsupervised method based on lexical co-occurrence statistics for detecting and correcting ASR errors. Shivakumar et al. [29] designed a noisy channel model for error correction that can learn from past ASR errors. D’Haro and Banchs [5] proposed an correction procedure using a phrase-based machine translation system.\nIn all the above approaches, the benefit comes at the cost of introducing additional components in the NLU pipeline. In our work, instead, we explore a different research direction: we aim to make downstream models more robust to ASR errors by using phonetic-aware text representations. In particular, we propose to adopt phoneme embeddings to replace or complement common text representations, e.g., word embeddings [18, 24, 25], or character embeddings [11].\nFew existing works studied phoneme embeddings. Li et al. [13] explored the application of phoneme embeddings for the task of speech-driven talking avatar synthesis to create more realistic and\n1https://registry.opendata.aws/asr-error-robustness\nexpressive visual gestures. Silfverberg et al. [30] proposed an approach to learn phoneme embeddings that can be used to perform phonological analogies. Toshniwal and Livescu [34] discussed the usage of phoneme embeddings for the task of grapheme-tophoneme conversion. To our best knowledge, no previous work focused on the application of phoneme embeddings to improve NLU models operating on transcriptions containing ASR errors.\nAnother line of works handle ASR errors at downstream tasks. The general approach is to pass intermediate ASR results, in the forms of lattices or embeddings, to the downstream model. Lattices can be either at the word level or at the phoneme level [15]. Other solutions consist of developing end-to-end models for SLU [2]. In this case ASR and SLU models are integrated and typically need a lot of data to be trained. Conversely, our proposed approach can rely on off-the-shelf ASR systems (which typically do not give access to intermediate results) and train only SLU models, which typically require much less data."},{"heading":"3 PHONEME-LEVEL REPRESENTATIONS","text":"In deep learning methods for NLP, text is typically represented as a sequence of tokens, e.g., words or characters, which are modeled using embeddings. This approach is proven to be effective for written text [10, 11, 36], however, it is not inherently robust to ASR errors. Table 1 lists three typical examples of ASR transcriptions containing quasi-oronym errors2. In the first example, the words what & canadian are incorrectly transcribed to words with completely different meanings, i.e., well & comedian. Since word embeddings typically reflect word semantics, the word embeddings of these misrecognized words will be very dissimilar from the reference ones. On the other hand, when an ASR model does not correctly recognize a word or a phrase, it typically confuses it with a quasioronym, e.g., canadian vs. comedian and affecting vs. defecting in the first and the third example in Table 13. This suggests that the sequence of phonemes of an ASR transcription tends to be similar to the sequence of phonemes of the correct text.\nA common metric to evaluate the performance of speech recognition or machine translation systems is Word Error Rate (WER). It measures the percentage of incorrectly transcribed words (Substitutions (S), Insertions (I), Deletions (D)). It is defined as follows:\n\uD835\uDC4A\uD835\uDC38\uD835\uDC45 = \uD835\uDC46 + \uD835\uDC37 + \uD835\uDC3C\n\uD835\uDC41 (1)\nwhere \uD835\uDC41 is the number of words in the reference text. In the same vein, we further define two additional metrics, Character Error Rate (CER) and Phoneme Error Rate (PER), as the extensions of WER to characters and phonemes, respectively. Intuitively, if ASR confuses similar sounding words or phrases, PER will be smaller than CER and WER (see Table 1). In Section 4 we further confirm this intuition on entire datasets. Hence, we argue that representing text as a sequence of phoneme embeddings can help when dealing with ASR errors.\n2The ASR transcriptions were created by the data generation pipeline, as described in Section 4. 3We convert text to phonemes using the phonemizer tool (github.com/bootphon/ phonemizer), which is based on the speech synthesis system Festival [1]. A phoneme is represented by 2-letter notation.\nASR or REF utterance\nSimilar to word or character embeddings, phoneme embeddings can be directly learned during the training process of a Neural Network that is designed to solve a specific task. However, these learned phoneme embeddings do not necessarily capture pronunciation aspects. Therefore, in Sections 3.1 and 3.2 we propose four methods, including a sequence-to-sequence (seq2seq) model and variants of word2vec, to train phoneme embeddings reflecting pronunciation similarity. For readability purposes, we denote the correct utterance as the reference (REF), while we denote the text transcribed by an ASR model as the ASR utterance. Note that, different from standard word embeddings, our phoneme models require both REF and ASR utterances for training."},{"heading":"3.1 Phoneme2Vec","text":"Word2vec [17, 18] is widely used to learn word embeddings using a shallow neural network trained on language modeling tasks. There are two variants of word2vec, namely the continuous bag-of-words and the skip-gram model. In the skip-gram architecture, the model uses the current word to predict the surrounding words in a context window. To learn phoneme embeddings we design phoneme2vec, a modified skip-gram model that operates at the phoneme level, instead of the word level. We propose three variants by considering different definitions of context.\n3.1.1 p2vc: phoneme2vec on surrounding phonemes. This is the natural extension of word2vec to phonemes: given a phoneme we want to predict its surrounding phonemes. Specifically, an utterance (either a REF or an ASR utterance) is represented by its sequence of phonemes (the padding symbol is used to separate words). The traditional word2vec procedure is then applied on the phoneme sequence to predict phonemes in the same context windows. Figure 1 illustrates an example with a 2-size context window: given the central phoneme \uD835\uDC5D4, p2vc has to predict phonemes \uD835\uDC5D3, \uD835\uDC5D5 and \uD835\uDC5D6, as well as the padding symbol (pad).\nWe decided not to limit the context of a phoneme to its word as the ASR might have failed the word segmentation. However, we explicitly consider the padding symbol as it represents the “absence of sound” captured by the ASR.\nWord2vec was designed to generate word embeddings reflecting semantic and syntactic aspects; however, we aim to capture pronunciation similarities. Intuitively, two phonemes are similar if the ASR often confuses them. Following this intuition we propose two variants of phoneme2vec, as well as a sequence-to-sequence model for training phoneme embeddings in the rest of this section. In p2vc , ASR or REF utterances are always analyzed individually. Conversely, our proposed models operate on <ASR, REF> pairs, to leverage their dissimilarity and automatically learn which sounds the ASR confuses.\n3.1.2 p2vm: phoneme2vec on mixed REF and ASR utterances. In this approach we mix REF and ASR utterances at phoneme level in an alternating way, as shown in Figure 2: if \uD835\uDC5D\uD835\uDC451 , \uD835\uDC5D \uD835\uDC45 2 , . . . and \uD835\uDC5D\uD835\uDC341 , \uD835\uDC5D \uD835\uDC34 2 , . . . are the sequences of phonemes in the REF and ASR utterances, respectively, \uD835\uDC5D\uD835\uDC451 , \uD835\uDC5D \uD835\uDC34 1 , \uD835\uDC5D \uD835\uDC45 2 , \uD835\uDC5D \uD835\uDC34 2 . . . is the resulting mixed sequence. Given a phoneme, the model aims to predict the surrounding phonemes in the mixed sequence. For example, let us consider the REF utterance “when (w-ih-n) iPhone 7 was released\" and its ASR counterpart “win (w-eh-n) iPhone 7 was released\". The underlying idea is trying to make two confused phonemes (in this case, eh and ih) appear in their reciprocal context windows. The assumption is that, if the ASR utterance contains few errors, phonemes with a similar pronunciation appear in very similar, possibly the same, positions and therefore they will be close in the mixed sequence. This means that they will occur in their reciprocal contexts, allowing phoneme2vec to learn embeddings reflecting pronunciation similarities.\n3.1.3 p2va: phoneme2vec on aligned REF and ASR utterances. The previous approach relies on the hypothesis that REF and ASR utterances have a similar number of phonemes, and that the two phoneme sequences naturally align when mixed. Although this is often true, we propose a more general solution that involves an explicit alignment. We directly pair phonemes in a REF utterance with their aligned phonemes in the ASR utterance using the Needleman-Wunsch alignment algorithm [19], as shown in Figure 3. Then the context of a given phoneme in the REF [ASR] utterance is its aligned phoneme in the ASR [REF] utterance as well as the phonemes surrounding it. For example, if we consider a window\nsize of 1, the context phoneme of ih are w, eh and n, as shown in Figure 3.\n3.2 Phoneme Embeddings from Seq2Seq - s2s A very intuitive way of training phoneme embeddings is to use a seq2seq model [9, 32], since it can map the entire REF utterance (i.e., the input) to its ASR utterance (i.e., the output). An advantage of the seq2seqmodel is that it does not require any phoneme alignment procedure.\nFigure 4 shows our seq2seq model, where LSTM layers are used in both the encoder and decoder. A REF utterance is represented as a sequence, i.e., {\uD835\uDC5D\uD835\uDC451 , \uD835\uDC5D \uD835\uDC45 2 , ..., \uD835\uDC5D \uD835\uDC45 \uD835\uDC41 }. During the encoding phase, at\neach time step \uD835\uDC61 , the LSTM reads a phoneme of the sentence and updates the hidden states ℎ\uD835\uDC38\uD835\uDC61 :\nℎ\uD835\uDC38\uD835\uDC61 = \uD835\uDC53 (ℎ\uD835\uDC38\uD835\uDC61−1, \uD835\uDC5D \uD835\uDC45 \uD835\uDC61 ) (2)\nwhere \uD835\uDC53 represents LSTM operations [8], and \uD835\uDC5D\uD835\uDC45\uD835\uDC61 indicates the current phoneme in the REF utterance. After reading the entire utterance, the last hidden state of the LSTM,ℎ\uD835\uDC38\n\uD835\uDC41 , is passed to decoder.\nThe initial state ℎ\uD835\uDC380 of the LSTM encoder is a zero vector. At step \uD835\uDC61 , the hidden state of the LSTM decoder ℎ\uD835\uDC37\uD835\uDC61 is calculated as:\nℎ\uD835\uDC37\uD835\uDC61 = \uD835\uDC53 (ℎ\uD835\uDC37\uD835\uDC61−1, \uD835\uDC5D \uD835\uDC34 \uD835\uDC61−1) (3)\nwhereℎ\uD835\uDC380 = ℎ \uD835\uDC37 \uD835\uDC41 , \uD835\uDC5D\uD835\uDC340 is the start symbol “GO\", and \uD835\uDC5D \uD835\uDC34 \uD835\uDC61−1 is the (\uD835\uDC61−1)th phoneme of the ASR utterance. We train the seq2seq model to predict the next correct phoneme of the ASR utterance given the REF utterance and the previous ASR phonemes. The next phoneme \uD835\uDC5D\uD835\uDC34∗\uD835\uDC61 (i.e., output of the LSTM decoder) is predicted using conditional distribution:\n\uD835\uDC43 (\uD835\uDC5D\uD835\uDC34∗\uD835\uDC61 |\uD835\uDC5D\uD835\uDC34\uD835\uDC61−1, \uD835\uDC5D \uD835\uDC34 \uD835\uDC61−2, ..., \uD835\uDC5D \uD835\uDC34 1 , ℎ \uD835\uDC38 \uD835\uDC41 ) = \uD835\uDC54(ℎ \uD835\uDC37 \uD835\uDC61 ) (4)\nwhere \uD835\uDC54 is the softmax activation function. As a loss function, we use the categorical cross-entropy between the prediction \uD835\uDC54(ℎ\uD835\uDC37\uD835\uDC61 ) and the one-hot encoding of \uD835\uDC5D\uD835\uDC34\uD835\uDC61 .\nIn this sequence-to-sequence architecture, we add phoneme embedding layers before the encoder and decoder. During the training process, the REF utterances and their corresponding ASR utterances are transformed into sequences of phonemes that are given as inputs to the encoder and decoder, respectively4. Finally, the embedding layer of the decoder is used as pre-trained phoneme embeddings5."},{"heading":"4 GENERATION OF NOISY DATASETS","text":"The proposed training procedures for learning phoneme embeddings require a corpus of corresponding REF and ASR utterances. In the ASR literature there are several corpora [e.g., in 3, 21] containing text and the associated human speech, which could be provided as inputs to an ASR system to obtain the required ⟨REF, ASR⟩ utterance pairs. However, to verify the impact of the proposed phoneme embeddings on specific prediction tasks, e.g., classification tasks, we also need such data to be annotated according to a desired class taxonomy. Unfortunately, to the best of our knowledge, human annotated speech corpora are not publicly available. Since speech transcription and annotation are expensive and labor-intensive processes, we propose an automatic data generation pipeline, as shown in Figure 5. In particular, we use this pipeline to automatically generate ASR transcriptions of existing annotated datasets.\nFirst, we use a Text To Speech (TTS) tool to generate speech audios of sentences from a textual corpus (REF utterances). To produce realistic speech, we inject different types of synthetic noise into the audio. By using SSML tags6, it is possible to directly apply several effects to the produced speech, such as changing the prosody, emphasizing or pausing. Moreover, we add 20 types of ambient noise7 to the audio, e.g., traffic noise, or restaurant noise. Overall, for 4We also tried the opposite, but the results did not show significant differences. 5We also tried the phoneme embeddings from the encoder layer, but we did not observe substantial differences. 6www.w3.org/TR/speech-synthesis11/ 7We use the ambient noise from www.pacdv.com/sounds/ambience_sounds.html.\na given audio file, we add one random synthetic noise using SSML tags and one random ambient noise. Based on our manual analysis, we set the volume of the ambient noise to -5 dB and that of the audio speech to 0 dB, to obtain reasonably understandable audios. Lastly, the noisy audios are passed to an ASR tool to generate the transcriptions. We keep only transcriptions containing ASR errors, by repeating the process for correctly transcribed utterances.\nEven though the above pipeline is generic, in our experiments we use two standard off-the-shelf tools for TTS and ASR, namely Amazon Polly8 and Amazon Transcribe9. We invoke the proposed pipeline to obtain noisy versions of four datasets:\n• SST. The Stanford Sentiment Treebank (SST) dataset [31] contains sentences with their labels from a five-point sentiment scale. • TQ. The TREC Question classification dataset contains questions with 6 (TQ-6) or 50 (fine-grained, TQ-50) question types [12]. • SQuAD. This dataset contains approximately 150k crowdsourced questions regarding a number of Wikipedia articles [26]. We randomly select 20 questions from each of a total of 442 Wikipedia articles10. • SUBJ. This is the subjectivity dataset (10k sentences) fromPang and Lee [22]. We randomly select 5k sentences out of 10k10.\nWe list some statistics of the four datasets in Table 2. The PER in the four datasets is lower than their WER and CER, confirming the intuition that a phoneme-based text representation is the least affected by ASR errors."},{"heading":"5 QUALITATIVE ANALYSIS","text":"As discussed in Section 3, we defined four different models for pretraining phoneme embeddings: the seq2seq model s2s and three phoneme2vec variants, i.e., p2vc, p2vm, and p2va. As pre-training examples, we use ⟨REF, ASR⟩ utterance pairs from the union of SQuAD and SUBJ datasets (a total of 13,840 pairs). ASR transcriptions contain errors from the specific ASR system adopted in the data generation pipeline. Therefore, the resulting phoneme embeddings should link the phonemes that this particular ASR system often confuses. We denote such embeddings with the subscript \uD835\uDC4E\uD835\uDC60\uD835\uDC5F , e.g., s2s\uD835\uDC4E\uD835\uDC60\uD835\uDC5F or p2vm\uD835\uDC4E\uD835\uDC60\uD835\uDC5F .\n8aws.amazon.com/polly 9aws.amazon.com/transcribe 10We did not transcribe the entire datasets due to budget constraints.\nAdditionally, we also employ CMU Pronouncing Dictionary11 to extract roughly 8000 words having multiple accepted pronunciations. For each word, we couple all its alternative pronunciations and we consider the resulting pairs as ⟨REF, ASR⟩ utterances for training our phoneme embeddings. In this case the data generation pipeline is not used, and the phoneme embeddings we generate express general pronunciation aspects. We denote such embeddings with the subscript \uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 , e.g., s2s\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 or p2vm\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 . Note that the CMU Pronouncing Dictionary was also adopted by Hixon et al. [7] to study phoneme similarities.\nThe context window parameter in phoneme2vec (see Section 3) reflects how many phonemes we take into account when predicting the current phoneme. We set this value to 2 according to some preliminary experiments. For p2va we also use a 0 context window (we refer to the resulting models as p2va0\n\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 and p2va0\uD835\uDC4E\uD835\uDC60\uD835\uDC5F ), to\nforce the model to acquire only pronunciation similarities between phonemes confused by the ASR. In fact, using a 0 window size implies that in the context of a given phoneme from an ASR [REF] utterance, there is only the aligned phoneme in the corresponding REF [ASR] utterance. Overall, we create 10 different pre-trained phoneme embeddings. We set the dimension size of the embedding vector to 20 for all the methods. This setting is reasonable since the total number of phonemes is only 40.\nTo visually assess different phoneme embeddings, Figures 6 and 7 show a 2D projection of s2s\uD835\uDC4E\uD835\uDC60\uD835\uDC5F and p2vc\uD835\uDC4E\uD835\uDC60\uD835\uDC5F obtained by using tSNE [16]. We use different colours to highlight three groups of phonemes. The phonemes in each group are similar in terms of pronunciation. It is clear that the pre-trained embeddings using seq2seq (i.e., s2s\uD835\uDC4E\uD835\uDC60\uD835\uDC5F ) can reasonably cluster these similar phonemes together, such as “ay\", “ey\", “iy\" and “oy\" (in red cycle in Figure 6). We observe similar outcomes by using p2va and p2vm. However, as shown in Figure 7 these similar phonemes are not relatively close when the training model is p2vc\uD835\uDC4E\uD835\uDC60\uD835\uDC5F . This suggests that p2vc, i.e., the adaptation of the classic word2vec to phonemes, is not suited for learning pronunciation aspects, while the other proposed models more effectively capture these desired properties."},{"heading":"6 EXPERIMENTAL EVALUATION ON GENERATED DATA","text":"In this section, we evaluate the impact the proposed phonemebased representations in classification tasks. In addition to the 10 different embedding spaces introduced in the previous section, we also use randomly initialized vectors (denoted as rnd) to explore the un-pretrained case."},{"heading":"6.1 Neural Models on Phonemes","text":"We integrate the proposed phoneme embeddings in standard neural network models for sentence classification, i.e., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). For the CNN-based model, we extend the CNN proposed in Kim [10] to operate on multiple inputs, as shown in Figure 8. We consider different types of inputs, i.e., the sequences of word, character and phoneme embeddings; for each input sequence a convolutional layer and a max pooling layer are applied to create a sentence\n11speech.cs.cmu.edu/cgi-bin/cmudict\nTable 2: Datasets. “C\" is the number of classes. “V\" is the vocabulary size. “L\" is the average sentence length.\nC V L Train Dev Test WER CER PER SST 5 17,836 17.38 8,534 1,099 2,210 0.244 0.113 0.113 TQ 6/50 9,492 8.89 5,452 - 500 0.319 0.187 0.148 SQuAD - 13,240 12.28 8,840 - - 0.241 0.118 0.107 SUBJ 2 14,410 18.66 5,000 - - 0.229 0.100 0.097\nFigure 6: 2D visualization of s2s\uD835\uDC4E\uD835\uDC60\uD835\uDC5F embeddings.\nFigure 7: 2D visualization of p2vc\uD835\uDC4E\uD835\uDC60\uD835\uDC5F embeddings.\nvector; finally, the sentence vectors from different inputs are concatenated and classified using a fully connected layer with the softmax activation function.\nIn addition, we employ a multi-input variant of the characterlevel neural network proposed in Kim et al. [11]. As shown in Figure 9, this is an LSTM-basedmodel, where the information extracted from different input types (i.e., word, character and phoneme) is aggregated at the word level. This architecture consists of four main steps: (i) the sequences of character and/or phoneme embeddings are passed to the corresponding inputs in groups of words; (ii) each group12 is processed by a convolution layer followed by a max pooling layer that creates word vectors; (iii) these word vectors are 12Basically each group corresponds to a word and it is represented as a matrix whose rows are character/phoneme embeddings.\nconcatenated with the word embeddings provided by the word input to create a sequence of enriched word embeddings; (iv) an LSTM operates on this sequence, and its final hidden state is classified by a dense layer with the softmax activation function.\nFor the sake of simplicity, we denote the CNN-based and the LSTM-based models as cnn and lstm, respectively. To prevent confusion, lowercase is used for referring to the entire architectures and the uppercase for the CNN and LSTM layers.\nWe test the proposed Neural Models with various combinations of the three available inputs, i.e., words (w), characters (c) and phonemes (p). We specify which input is used in the model prefix, e.g., wp-lstm means that the model is the LSTM-based architecture operating on words and phonemes, while c-cnn is the CNN-based model using only the character input."},{"heading":"6.2 Experimental Settings","text":"We run an extensive experimental evaluation on the SST and TQ datasets. On the TQ dataset we performed experiments with both the 6-class and 50-class settings. We use word embeddings (300 dimensions) trained on Wikipedia from GloVe [24]. The character embeddings (20-dimensional) are randomly initialized and trainable, whereas the pre-trained phoneme embeddings are not trainable13, except the randomly initialized ones (rnd model).\nFor all the convolutional layers used in the cnn and lstm models, we set the filter windows (\uD835\uDC64 ) to {1, 2, 3, 4} with 256 as the filter size. The pooling size is \uD835\uDC59 −\uD835\uDC64 , where \uD835\uDC59 is the length of the sentence\n13In our preliminary experiments, we verified that making them trainable hurts the performance.\nsequence. In the lstm model, we set 128 as the number of units. We set the dropout rate using a tuning stage on the development set14. Each experiment is repeated 5 times for conducting significance tests (i.e., Student’s t-test)."},{"heading":"6.3 Results and Analysis","text":"Upper- and Lower-bounds: As a first experiment, we assess the upper-bound and the baseline (lower-bound) for the evaluated datasets. The upper-bound, indicated as ↑, is defined as the accuracy of the models on clean data, i.e., REF utterances. Models are also trained and tuned on REF utterances. The baseline (indicated as ↓ ) is the accuracy that the model achieves on noisy data, i.e., ASR utterances. In this case, ASR utterances are used during both the training and tuning stages. We also tried to use REF utterances for training the model to be tested on ASR utterances, however, we observed that this was generally hurting the performance, due to the data distribution mismatch between the train and test data. Table 3 reports the upper- and lower-bound performance of the two types of neural models in the three tasks. It is clear that there are rather large margins between the baseline and the upper-bound accuracy. The margin is 3.8% for the SST dataset, while it is much higher for the TQ-50 and TQ-6, i.e., 19.6% and 16.1%, respectively. The larger margin in the question type classification tasks can be explained by the higher WER that affects the TQ dataset (see\n14In TQ, the dev set is a random sample of the train set without replacement.\nTable 2). Using our phoneme-based approach we aim to reduce this margin.\nFinally, we also compare with a recent approach of Lugosch et al. [15], which is an end-to-end speech model that directly operates on the audio signals. We use their pre-trained model with no unfreezing setting, as the authors reported that it achieves high performance. We fine tune such model on our training sets. As shown in Table 3, the results achieved by this model are fairly low. A possible explanation is that it was trained on simple instructions and very clean speeches, while our datasets include more elaborated utterances and the audios contain ambient noise.\nImpact of Phoneme Embeddings: In the following experiments, we adopt the word embedding models as baselines (i.e., w-cnn and w-lstm). For each individual task we select the neural architecture achieving the best accuracy in Table 3, i.e., lstm for SST and TQ-6, and cnn for TQ-50. Table 4 reports the results of these models after activating the phoneme embedding input.\nWe observe that phoneme embeddings (rows 2-12 in Table 4), even without pre-training (row 2 in Table 4), generally lead to a better accuracy. The positive impact is more pronounced in TQ-50/6, because, as reported in Table 3, there is more margin for improvement. As the results suggest, using phoneme representations is beneficial across different neural architectures and tasks (i.e., cnn for TQ-50 and lstm for SST and TQ-6); we argue that the proposed approach is useful for any task operating on noisy ASR input.\nSeveral pre-training models (rows 3-12 in Table 4) lead to better accuracy. p2vc\uD835\uDC4E\uD835\uDC60\uD835\uDC5F and p2vc\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 achieve the worst results. As discussed in Section 5, this is expected, since p2vc is not designed to capture pronunciation similarities of phonemes. In contrast, the other phoneme pre-training models focus on acoustic aspects, and achieve more competitive results. In particular, the neural network models with p2va0\n\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 embeddings perform significantly better than\nthe baseline models (i.e., None) in the three tasks. This indicates\nthat the acoustic information captured by our models helps neural models to deal with ASR transcription errors. The results for pre-training the phoneme embeddings on the CMU dictionary data, rather then on the ASR generated utterances, are not conclusive as they alternate across different models and tasks.\nImpact of Different Input Types: Next, we examine the impact of using different combinations of inputs (i.e., words, characters and phonemes). For the phoneme embeddings, we use p2va0\n\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 . Table 5\nreports the results of this set of experiments. The bestmodels always include phoneme embeddings, indicating that, when dealing with ASR errors, using phoneme representations is more effective than the sole use of word and character representations. Although there is usually high correlation between representing text in terms of its character sequence or its phoneme sequence, our results show that phoneme-based representations are significantly more effective compared to character-based representations.\nFinally, we run a complete grid search which explores all 6 input combinations, all 11 phoneme embeddings models and both neural architectures. The models that provided the best accuracy on the development sets are wp-lstm using s2s\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 phoneme embeddings, wp-lstm using s2s\uD835\uDC4E\uD835\uDC60\uD835\uDC5F and wp-cnn using p2va0\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61 for the SST, TQ-50 and TQ-6 tasks, respectively. Such models achieve 41.7%, 66.7% and 75.9% accuracy on the test sets. This means that they provide 0.3%, 1.9% and 0.7% absolute accuracy improvement w.r.t. the best models not using phoneme embeddings.\nTable 6 reports some examples in which the baseline model (i.e., w-cnn) cannot predict the correct class due to ASR errors, while the model using phoneme embeddings (i.e., wp-cnn) can actually produce the expected output."},{"heading":"7 EXPERIMENTAL EVALUATION ON AMAZON ALEXA DATA","text":"In this section, we evaluate our models using a real-world dataset from the Amazon Alexa virtual assistant. Virtual assistants, e.g., Cortana, Alexa, or Google Assistant, perform a wide range of tasks in different domains, such as Music,Weather, Calendar, etc. Typically, a sentence is first classified into one of the supported domains, then domain dependent intent analysis and slot filling (i.e., entity extraction) are carried out. In our evaluation, we consider live utterances spoken to the top 11 domains of Alexa and we experiment on the domain classification task. To create a complex scenario and effectively test the model robustness to ASR errors, in our datasets we upsample utterances affected by ASR errors. In particular, we\napply a stratified sampling procedure with four different strata, each one corresponding to a different average WER. In addition to collecting utterances not affected by ASR errors (i.e., 0 WER), we use a stratum corresponding to 0.33 WER, a value similar to the one observed in the TQ dataset (see Table 2). For the other two strata we select a lower WER, namely 0.24 and a higher WER, namely 0.45. From each stratum we select the same number of samples, therefore the average WER is 0.2515. The training and development dataset portions contain 60k and 20k utterances, respectively. In testing, to better study the impact of the ASR errors to the model accuracy we keep separated the data sampled from the four different strata: we have four different test sets with increasing WER, i.e., {0.00, 0.24, 0.33, 0.45}. Each of the four test sets has 5k utterances. The overall dataset statistics are reported in Table 7, where \uD835\uDC47\uD835\uDC52\uD835\uDC60\uD835\uDC610.24 indicates a test set with an average WER of 0.24. For all these datasets (except\uD835\uDC47\uD835\uDC52\uD835\uDC60\uD835\uDC610.00), their PER is lower than CER, which is consistent with what we observe in our generated datasets shown in Table 2.\nWe compared the CNN model which uses only the word embedding channel, with the same model where the phoneme embedding channel is also active (we used the \uD835\uDC602\uD835\uDC60\uD835\uDC4E\uD835\uDC60\uD835\uDC5F phoneme embedding space, as it was performing effectively in the experiments reported in Section 6). We selected the hyperparameters of the two models on the development set (i.e., DEV in Table 7). Table 8 reports the results. All the experiments are repeated five times in order to conduct the statistical significance test and the average accuracy is reported.\nResults are consistent with what we observed on the generated data: the phoneme embeddings allow the model to improve its robustness to ASR errors. As expected, the gap between the baseline model and the proposed one increases when ASR errors are more frequent and the accuracy difference becomes significant when the word error rate increases."},{"heading":"8 CONCLUSIONS","text":"In this paper, we explored the usage of phoneme-based representations to make classification models more robust to ASR errors. We proposed several approaches to learn phoneme embeddings capturing pronunciation similarities of phonemes. We then showed how to integrate phoneme embeddings into existing Neural Network architectures, and we demonstrated that their adoptions can improve classification models when dealing with data containing ASR errors.\nTo support our experiments, we introduced a data generation pipeline that is able to automatically produce noisy ASR transcriptions of textual data. Using this pipeline we created noisy versions of four NLU datasets that we plan to make available to the community.\nWe experimented the proposedmodels on these generated datasets, as well as on a real-world dataset from the Alexa virtual assistant. Results are consistent and we show that our models can increase the robustness of NLU models to ASR errors.\nAs future work, we plan to (i) experiment on other tasks such as slot filling or intent classification and (ii) extend other state-of-theart models, e.g., ELMo [25] or BERT [4], in order to allow them to take advantage of phoneme representations.\n15Note that 0.25 WER does not reflect the real WER of Alexa."}],"references":[{"title":"The festival speech synthesis system","author":["Alan W Black","Paul Taylor","Richard Caley","Rob Clark"],"venue":"University of Edinburgh","citeRegEx":"1","shortCiteRegEx":"1","year":1999},{"title":"Spoken language understandingwithout speech recognition","author":["Yuan-Ping Chen","Ryan Price","Srinivas Bangalore"],"venue":"In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing","citeRegEx":"2","shortCiteRegEx":"2","year":2018},{"title":"The TDT-2 text and speech corpus","author":["Chris Cieri","David Graff","Mark Liberman","Nii Martey","Stephanie Strassel"],"venue":"In Proceedings of the DARPA Broadcast News workshop","citeRegEx":"3","shortCiteRegEx":"3","year":1999},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"venue":"In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","citeRegEx":"4","shortCiteRegEx":"4","year":2019},{"title":"Automatic correction of ASR outputs by using machine translation","author":["Luis Fernando D’Haro","Rafael E Banchs"],"venue":"In Proceedings of the Annual Conference of the International Speech Communication Association","citeRegEx":"5","shortCiteRegEx":"5","year":2016},{"title":"Towards end-to-end speech recognition with recurrent neural networks","author":["Alex Graves","Navdeep Jaitly"],"venue":"In Proceedings of the International Conference on Machine Learning","citeRegEx":"6","shortCiteRegEx":"6","year":2014},{"title":"Phonemic similaritymetrics to compare pronunciation methods","author":["BenHixon","Eric Schneider","Susan L Epstein"],"venue":"In Proceedings of the Annual Conference of the International Speech Communication Association","citeRegEx":"7","shortCiteRegEx":"7","year":2011},{"title":"Long short-termmemory","author":["Sepp Hochreiter","Jürgen Schmidhuber"],"venue":"Neural computation 9,","citeRegEx":"8","shortCiteRegEx":"8","year":1997},{"title":"Recurrent continuous translation models","author":["Nal Kalchbrenner","Phil Blunsom"],"venue":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing","citeRegEx":"9","shortCiteRegEx":"9","year":2013},{"title":"Convolutional neural networks for sentence classification","author":["Yoon Kim"],"venue":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing","citeRegEx":"10","shortCiteRegEx":"10","year":2014},{"title":"Character- Aware Neural Language Models","author":["Yoon Kim","Yacine Jernite","David Sontag","Alexander M Rush"],"venue":"In Proceedings of the Association for the Advancement of Artificial Intelligence","citeRegEx":"11","shortCiteRegEx":"11","year":2016},{"title":"Learning question classifiers","author":["Xin Li","Dan Roth"],"venue":"In Proceedings of the International Conference on Computational linguistics","citeRegEx":"12","shortCiteRegEx":"12","year":2002},{"title":"Phoneme Embedding and its Application to Speech Driven Talking Avatar Synthesis","author":["Xu Li","Zhiyong Wu","Helen M Meng","Jia Jia","Xiaoyan Lou","Lianhong Cai"],"venue":"In Proceedings of the International Speech Communication Association","citeRegEx":"13","shortCiteRegEx":"13","year":2016},{"title":"Research Challenges in Building a Voice-based Artificial Personal Shopper-Position Paper","author":["Nut Limsopatham","Oleg Rokhlenko","David Carmel"],"venue":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing Workshop SCAI: The 2nd InternationalWorkshop on Search-Oriented Conversational AI","citeRegEx":"14","shortCiteRegEx":"14","year":2018},{"title":"Speech Model Pre-Training for End-to-End Spoken Language Understanding","author":["Loren Lugosch","Mirco Ravanelli","Patrick Ignoto","Vikrant Singh Tomar","Yoshua Bengio"],"venue":"In Proceedings of the Annual Conference of the International Speech Communication Association","citeRegEx":"15","shortCiteRegEx":"15","year":2019},{"title":"Efficient estimation of word representations in vector space","author":["Tomas Mikolov","Kai Chen","Greg Corrado","Jeffrey Dean"],"venue":null,"citeRegEx":"17","shortCiteRegEx":"17","year":2013},{"title":"Distributed representations of words and phrases and their compositionality","author":["Tomas Mikolov","Ilya Sutskever","Kai Chen","Greg S Corrado","Jeff Dean"],"venue":"In Proceedings of the Advances in neural information processing systems","citeRegEx":"18","shortCiteRegEx":"18","year":2013},{"title":"A general method applicable to the search for similarities in the amino acid sequence of two proteins","author":["Saul B Needleman","Christian D Wunsch"],"venue":"Journal of molecular biology 48,","citeRegEx":"19","shortCiteRegEx":"19","year":1970},{"title":"Error detection and accuracy estimation in automatic speech recognition using deep bidirectional recurrent neural networks","author":["Atsunori Ogawa","Takaaki Hori"],"venue":"Speech Communication","citeRegEx":"20","shortCiteRegEx":"20","year":2017},{"title":"Librispeech: an ASR corpus based on public domain audio books","author":["Vassil Panayotov","Guoguo Chen","Daniel Povey","Sanjeev Khudanpur"],"venue":"In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing","citeRegEx":"21","shortCiteRegEx":"21","year":2015},{"title":"A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts","author":["Bo Pang","Lillian Lee"],"venue":"In Proceedings of the annual meeting on Association for Computational Linguistics","citeRegEx":"22","shortCiteRegEx":"22","year":2004},{"title":"Improving ASR error detection with non-decoder based features","author":["Thomas Pellegrini","Isabel Trancoso"],"venue":"In Proceedings of the Annual Conference of the International Speech Communication Association","citeRegEx":"23","shortCiteRegEx":"23","year":2010},{"title":"GloVe: Global Vectors for Word Representation","author":["Jeffrey Pennington","Richard Socher","Christopher D. Manning"],"venue":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing","citeRegEx":"24","shortCiteRegEx":"24","year":2014},{"title":"Deep Contextualized Word Representations","author":["Matthew Peters","Mark Neumann","Mohit Iyyer","Matt Gardner","Christopher Clark","Kenton Lee","Luke Zettlemoyer"],"venue":"In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","citeRegEx":"25","shortCiteRegEx":"25","year":2018},{"title":"KnowWhat You Don’t Know: Unanswerable Questions for SQuAD","author":["Pranav Rajpurkar","Robin Jia","Percy Liang"],"venue":"In Proceedings of the Annual Meeting of the Association for Computational Linguistics","citeRegEx":"26","shortCiteRegEx":"26","year":2018},{"title":"Context-based speech recognition error detection and correction","author":["Arup Sarma","David D Palmer"],"venue":"In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics","citeRegEx":"27","shortCiteRegEx":"27","year":2004},{"title":"Incorporating ASR Errors with Attention-Based, Jointly Trained RNN for Intent Detection and Slot Filling","author":["Raphael Schumann","Pongtep Angkititrakul"],"venue":"In Proceedings of International Conference on Acoustics, Speech and Signal Processing","citeRegEx":"28","shortCiteRegEx":"28","year":2018},{"title":"Learning from Past Mistakes: Improving Automatic Speech Recognition Output via Noisy-Clean Phrase Context Modeling","author":["Prashanth Gurunath Shivakumar","Haoqi Li","Kevin Knight","Panayiotis Georgiou"],"venue":null,"citeRegEx":"29","shortCiteRegEx":"29","year":2018},{"title":"Sound Analogies with Phoneme Embeddings","author":["Miikka P Silfverberg","Lingshuang Mao","Mans Hulden"],"venue":"Proceedings of the Society for Computation in Linguistics,","citeRegEx":"30","shortCiteRegEx":"30","year":2018},{"title":"Recursive deep models for semantic compositionality over a sentiment treebank","author":["Richard Socher","Alex Perelygin","Jean Wu","Jason Chuang","Christopher D Manning","Andrew Ng","Christopher Potts"],"venue":"In Proceedings of the Conference on Empirical Methods in Natural Language Processing","citeRegEx":"31","shortCiteRegEx":"31","year":2013},{"title":"Sequence to sequence learning with neural networks. In Proceedings of the Advances in neural information Session 4C: Neural Networks and Embedding SIGIR ’20","author":["Ilya Sutskever","Oriol Vinyals","Quoc V Le"],"venue":"July 25–30,","citeRegEx":"32","shortCiteRegEx":"32","year":2014},{"title":"ASR error detection using recurrent neural network language model and complementary ASR","author":["Yik-Cheung Tam","Yun Lei","Jing Zheng","Wen Wang"],"venue":"In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing","citeRegEx":"33","shortCiteRegEx":"33","year":2014},{"title":"Jointly learning to align and convert graphemes to phonemes with neural attention models","author":["Shubham Toshniwal","Karen Livescu"],"venue":"In Proceedings of the IEEE Spoken Language Technology","citeRegEx":"34","shortCiteRegEx":"34","year":2016},{"title":"Bridging Screen Readers and Voice Assistants for Enhanced Eyes-Free Web Search","author":["Alexandra Vtyurina","Adam Fourney","Meredith Ringel Morris","Leah Findlater","RyenWWhite"],"venue":"In Proceedings of The World Wide Web Conference","citeRegEx":"35","shortCiteRegEx":"35","year":2019},{"title":"Recent trends in deep learning based natural language processing","author":["Tom Young","Devamanyu Hazarika","Soujanya Poria","Erik Cambria"],"venue":"ieee Computational intelligenCe magazine 13,","citeRegEx":"36","shortCiteRegEx":"36","year":2018}],"referenceMentions":[{"referenceID":33,"context":"voice search engine [35] and voice shopping [14].","startOffset":20,"endOffset":24},{"referenceID":13,"context":"voice search engine [35] and voice shopping [14].","startOffset":44,"endOffset":48},{"referenceID":5,"context":"Different from traditional approaches, where NLU is applied on the original text, applying it on ASR transcriptions poses new challenges, as ASR systems often generate transcriptions with errors [6, 20].","startOffset":195,"endOffset":202},{"referenceID":18,"context":"Different from traditional approaches, where NLU is applied on the original text, applying it on ASR transcriptions poses new challenges, as ASR systems often generate transcriptions with errors [6, 20].","startOffset":195,"endOffset":202},{"referenceID":26,"context":"These ASR errors can cause failures in downstream applications of virtual assistants, such as intention classification or slot filling [28], affecting the end-user experience.","startOffset":135,"endOffset":139},{"referenceID":31,"context":"[33] tackled the error detection task with a Recurrent Neural Network, while Pellegrini and Trancoso [23] used features obtained from different knowledge sources to complement an encoderdecoder model.","startOffset":0,"endOffset":4},{"referenceID":21,"context":"[33] tackled the error detection task with a Recurrent Neural Network, while Pellegrini and Trancoso [23] used features obtained from different knowledge sources to complement an encoderdecoder model.","startOffset":101,"endOffset":105},{"referenceID":25,"context":"Sarma and Palmer [27] proposed an unsupervised method based on lexical co-occurrence statistics for detecting and correcting ASR errors.","startOffset":17,"endOffset":21},{"referenceID":27,"context":"[29] designed a noisy channel model for error correction that can learn from past ASR errors.","startOffset":0,"endOffset":4},{"referenceID":4,"context":"D’Haro and Banchs [5] proposed an correction procedure using a phrase-based machine translation system.","startOffset":18,"endOffset":21},{"referenceID":16,"context":", word embeddings [18, 24, 25], or character embeddings [11].","startOffset":18,"endOffset":30},{"referenceID":22,"context":", word embeddings [18, 24, 25], or character embeddings [11].","startOffset":18,"endOffset":30},{"referenceID":23,"context":", word embeddings [18, 24, 25], or character embeddings [11].","startOffset":18,"endOffset":30},{"referenceID":10,"context":", word embeddings [18, 24, 25], or character embeddings [11].","startOffset":56,"endOffset":60},{"referenceID":12,"context":"[13] explored the application of phoneme embeddings for the task of speech-driven talking avatar synthesis to create more realistic and","startOffset":0,"endOffset":4},{"referenceID":28,"context":"[30] proposed an approach to learn phoneme embeddings that can be used to perform phonological analogies.","startOffset":0,"endOffset":4},{"referenceID":32,"context":"Toshniwal and Livescu [34] discussed the usage of phoneme embeddings for the task of grapheme-tophoneme conversion.","startOffset":22,"endOffset":26},{"referenceID":14,"context":"Lattices can be either at the word level or at the phoneme level [15].","startOffset":65,"endOffset":69},{"referenceID":1,"context":"Other solutions consist of developing end-to-end models for SLU [2].","startOffset":64,"endOffset":67},{"referenceID":9,"context":"This approach is proven to be effective for written text [10, 11, 36], however, it is not inherently robust to ASR errors.","startOffset":57,"endOffset":69},{"referenceID":10,"context":"This approach is proven to be effective for written text [10, 11, 36], however, it is not inherently robust to ASR errors.","startOffset":57,"endOffset":69},{"referenceID":34,"context":"This approach is proven to be effective for written text [10, 11, 36], however, it is not inherently robust to ASR errors.","startOffset":57,"endOffset":69},{"referenceID":0,"context":"com/bootphon/ phonemizer), which is based on the speech synthesis system Festival [1].","startOffset":82,"endOffset":85},{"referenceID":15,"context":"Word2vec [17, 18] is widely used to learn word embeddings using a shallow neural network trained on language modeling tasks.","startOffset":9,"endOffset":17},{"referenceID":16,"context":"Word2vec [17, 18] is widely used to learn word embeddings using a shallow neural network trained on language modeling tasks.","startOffset":9,"endOffset":17},{"referenceID":17,"context":"We directly pair phonemes in a REF utterance with their aligned phonemes in the ASR utterance using the Needleman-Wunsch alignment algorithm [19], as shown in Figure 3.","startOffset":141,"endOffset":145},{"referenceID":8,"context":"A very intuitive way of training phoneme embeddings is to use a seq2seq model [9, 32], since it can map the entire REF utterance (i.","startOffset":78,"endOffset":85},{"referenceID":30,"context":"A very intuitive way of training phoneme embeddings is to use a seq2seq model [9, 32], since it can map the entire REF utterance (i.","startOffset":78,"endOffset":85},{"referenceID":7,"context":"where f represents LSTM operations [8], and p t indicates the current phoneme in the REF utterance.","startOffset":35,"endOffset":38},{"referenceID":29,"context":"The Stanford Sentiment Treebank (SST) dataset [31] contains sentences with their labels from a five-point sentiment scale.","startOffset":46,"endOffset":50},{"referenceID":11,"context":"The TREC Question classification dataset contains questions with 6 (TQ-6) or 50 (fine-grained, TQ-50) question types [12].","startOffset":117,"endOffset":121},{"referenceID":24,"context":"This dataset contains approximately 150k crowdsourced questions regarding a number of Wikipedia articles [26].","startOffset":105,"endOffset":109},{"referenceID":20,"context":"This is the subjectivity dataset (10k sentences) fromPang and Lee [22].","startOffset":66,"endOffset":70},{"referenceID":9,"context":"For the CNN-based model, we extend the CNN proposed in Kim [10] to operate on multiple inputs, as shown in Figure 8.","startOffset":59,"endOffset":63},{"referenceID":22,"context":"We use word embeddings (300 dimensions) trained on Wikipedia from GloVe [24].","startOffset":72,"endOffset":76},{"referenceID":14,"context":"[15], which is an end-to-end speech model that directly operates on the audio signals.","startOffset":0,"endOffset":4},{"referenceID":23,"context":", ELMo [25] or BERT [4], in order to allow them to take advantage of phoneme representations.","startOffset":7,"endOffset":11},{"referenceID":3,"context":", ELMo [25] or BERT [4], in order to allow them to take advantage of phoneme representations.","startOffset":20,"endOffset":23}],"year":2020,"abstractText":"Even though Automatic Speech Recognition (ASR) systems significantly improved over the last decade, they still introduce a lot of errors when they transcribe voice to text. One of the most common reasons for these errors is phonetic confusion between similar-sounding expressions. As a result, ASR transcriptions often contain “quasi-oronyms\", i.e., words or phrases that sound similar to the source ones, but that have completely different semantics (e.g., win instead of when or accessible on defecting instead of accessible and affecting). These errors significantly affect the performance of downstream Natural Language Understanding (NLU) models (e.g., intent classification, slot filling, etc.) and impair user experience. To make NLU models more robust to such errors, we propose novel phonetic-aware text representations. Specifically, we represent ASR transcriptions at the phoneme level, aiming to capture pronunciation similarities, which are typically neglected in word-level representations (e.g., word embeddings). To train and evaluate our phoneme representations, we generate noisy ASR transcriptions of four existing datasets Stanford Sentiment Treebank, SQuAD, TREC Question Classification and Subjectivity Analysis and show that common neural network architectures exploiting the proposed phoneme representations can effectively handle noisy transcriptions and significantly outperform state-of-the-art baselines. Finally, we confirm these results by testing our models on real utterances spoken to the Alexa virtual assistant.","creator":"LaTeX with acmart 2019/08/24 v1.64 Typesetting articles for the Association for Computing Machinery and hyperref 2020/01/14 v7.00d Hypertext links for LaTeX"}}
{"name":"3397271.3401330.pdf","metadata":{"source":"META","title":"Sentiment-guided Sequential Recommendation","authors":["Lin Zheng","Naicheng Guo","Weihao Chen","Jin Yu","Dazhi Jiang"],"emails":["lzheng@stu.edu.cn","19ncguo@stu.edu.cn","19whchen@stu.edu.cn","jinyu@stu.edu.cn","dzjiang@stu.edu.cn"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems→Recommender systems; Personalization; Sentiment analysis; Collaborative filtering.\nKEYWORDS sequential behavior; sequential recommendation; human temporal sentiment; sentiment-guided attention; sparse sentiment attention\nACM Reference Format: Lin Zheng, NaichengGuo,Weihao Chen, Jin Yu, Dazhi Jiang. 2020. Sentimentguided Sequential Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401330"},{"heading":"1 INTRODUCTION","text":"In modern recommender systems such as Amazon and YouTube, users generate large amounts of sequential behavior and related information. Therefore, processing a large number of sequential behaviors is not only required to meet the actual scenario but also presents a new challenge to the traditional recommendation techniques. Fortunately, research on sequential recommendations that\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401330\nspecifically model sequential behavior has yielded many fruitful results [9].\nAs an initial attempt at applying deep learning to sequential recommendation, GRU4Rec [2] concatenates the behavior sequences of different users in a reasonable manner, thereby solving the difficult problem of uniformly modeling user behavior sequences of different lengths. This method illustrates the important role of recurrent neural networks (RNNs) [7] in sequential recommendation. Taking this research as a starting point, subsequent advanced approaches, such as memory-based RNNs [3, 12] and self-attention-based RNNs [5, 11], have taken sequential recommendation technology to a new level. These advanced technologies not only complete the modeling of pure behavior (item) sequences but also fully characterize additional information such as knowledge [3] and features [11] related to items to improve the sequence prediction performance. Some recent models based on the self-attention mechanism [8] perform particularly well in sequential recommendation. For example, SASRec [5] effectively captures users’ long-term preferences from both sparse and dense data sets, and in FDSA [11], two separated self-attention blocks (one for item sequences and one for features) were designed that achieved remarkable predictive effects.\nDespite their enhanced performances,most existing methods consider only the objective information of sequential items; they rarely consider the subjective influence of human sentiments on sequential recommendation. Specifically, there is a potential correlation between the temporal changes in a user’s sentiments and that user’s sequential behavior. However, most of the existing models based on sentiment factors [4, 10] focus only on non-sequential recommendations. In other words, they ignore the influence of the temporal sentiment change pattern on the sequence of user behavior; however, this temporal influence sometimes plays a decisive role in generating the final preference. For example, under the influence of continuous positive sentiments, a user may review a series of purchased items positively; in contrast, under the influence of continuous negative sentiments, the same user is more likely to consume some items that can usher in a good mood.\nDifferent from the existing approaches, in this work, we focus on how to capitalize on sequential sentiments to improve the sequential recommendation task. To this end, we set up a new preference prediction mechanism from the perspective of users’ subjective sentiments and propose a sequential recommendation method that employs temporal sentiments to facilitate generating users’ future preferences. Our work makes the following contributions:\nNovelty: We design two types of fused sequential sentiment modeling channels: one uses temporal sentiment to guide user behavior; the other generates pure sentiment preferences based on sparse attention. To the best of our knowledge, this is the first work on human sentiment-guided sequential recommendation.\nEffectiveness: The dual-channel modeling structure can be easily transplanted into existing RNNs. We report the results of an ablation study to prove experimentally that either or both of the two sentiment channels can effectively improve the performance.\nPerformance:We report the results of a method comparison to demonstrate that our model outperforms other competing state-ofthe-art sequential methods."},{"heading":"2 SENTIMENT-GUIDED SEQUENTIAL RECOMMENDATION","text":"In this section, we formalize the sequential recommendation task based on the sentiment state, introduce the network architecture of the Sentiment-Guided Sequential (SGS) recommender, and then describe the model implementation details."},{"heading":"2.1 Problem Formalization","text":"In the traditional sequential recommendation setting, the goal is to recommend the next item given a user’s historical sequence of behaviorsIu = (iu1 , iu2 , · · · , iu|Iu |), whereI\nu represents a sequence of items in chronological order that useru has previously interacted with. When considering sentiment factors in sequential recommendation, however, the situation becomes somewhat more complicated. Specifically, each item in the sequence uniquely corresponds to a sentiment state Sut = (s1t , s2t , · · · , sMt )u , where Sut corresponds to the item iut at time t and M denotes the number of sentiment factors. In a simple case, a sentiment state consists of three factors: positive, neutral, and negative; in this case,M = 3, and each factor appears with a certain probability. For instance, at time t , the positive sentiment may account for a probability of 0.6 (represented as s1t = 0.6), and the neutral (represented by s 2 t ) and negative (represented as s3t ) sentiments may account for 0.3 and 0.1, respectively. Therefore, the sequential recommendation task based on the sentiment state can be formalized as follows: Given a sequence of (item, sentiment_state) tuples ( (iu1 ,Su1 ), (iu2 ,Su2 ), · · · , (iu|Iu |,S u |Iu |) ) , the task is to recommend the next item that user u is most likely to interact with. In the following sections, we omit the identity of user u and use a fixed-length sequence ( (i1,S1), (i2,S2), · · · , (in,Sn )\n) suitable for training to describe our approach."},{"heading":"2.2 The Network Architecture","text":"The network architecture of the sentiment-guided sequential recommender is shown in Figure 1. We use subscripts t1, t2, · · · to represent a sequence in chronological order. SGS models users’\nbehavior-sentiment sequences in two channels. In the first channel, to reflect the guiding role of sentiment on the behavior sequence, we design a special attention mechanism based on sentiment queries and then perform sequential preference mining on both self-attention and sentiment-guided attention to obtain the preference of the first channel, PGu . The second channel emphasizes capturing the sequential preference, PGe , which is independently generated by sentiments. To this end, we incorporate a variant sparse sentiment attention [1] to model the sentiment factors in the sequence separately because a certain behavior in the sequence corresponds toM sentiment factors, and sparse attention can improve the calculation efficiency by reducing the number of attention interactions in the sequence. Similar to PGu , we employ a layernormalization RNN to complete sequential preference modeling to obtain PGe . Finally, the two preferences are aggregated into a fully connected layer based on layer normalization to generate the final preference, PF ."},{"heading":"2.3 The Implementation Details","text":"2.3.1 Embedding Layer. We divide the fixed-length training sequence ( (i1,S1), (i2,S2), · · · , (in,Sn ) ) into one item sequence and M sentiment sequences to construct embedding matrices. Because the elements of these M + 1 sequences are aligned individually in chronological order, we add zero padding to the left of these sequences to ensure that they have a fixed length of n. Then, we created an item embedding matrix E ∈ R |I |×d andM sentiment embedding matrices {S1, S2, · · · , SM }, where S1, S2, · · · , SM ∈ R |S |×d and d is the number of latent dimensions. 2.3.2 Two types of sentiment attention. The self-attention mechanism has been shown to be highly effective at sequential recommendation [5, 11]. It is based on the scaled dot-product attention [8], which is defined as follows:\nAtt (Q,K,V) = softmax (QKT√\nd\n) V , (1)\nwhere Q, K, and V represent queries, keys, and values, respectively, and √ d is a scale factor that changes with the input dimension. Given the current item embedding E as input, the self-attention operation transforms E into queries, keys, and values through linear projection and feeds them to the attention layer:\nAE = SelfAtt (E) = Att (EWQ, EWK , EWV ) , (2) whereWQ,WK ,WV ∈ Rd×d are projectionmatrices. The self-attention mechanism considers only items as queries; it ignores the role of sentiment factors. To emphasize the guiding role of sentiment factors, we define the sentiment-guided attention of the m-th sentiment factor to items as follows:\nAmS = SentiAtt (Sm, E) = Att (SmW Q, EWK , EWV ) . (3)\nBecause there are M sentiment factors, we perform a linear projection transformation on item self-attention AE and sentimentguided attentions {A1S ,A 2 S , · · · ,A M S } and then concatenate them according to the last dimension as follows:\nGAES = ConCat ([AEW E,A1SW S1 ,A2SW S2 , · · · ,AMS W SM ]) , (4) where WE,WS 1 , · · · ,WSM ∈ Ro×o and o is the size of output dimension of the attention layer. GAES is the aggregation of attention\nto items and sentiments. To model the sentiment-generated preference, we applied a simplified sparse attention mechanism [1] to model pure sentiments and specifically designed its inputs to facilitate interactions between sentiment factors. In particular, we concatenate theM sentiment embedding matrices to obtain SC as the input for sparse sentiment attention:\nACP = SparseAtt (SC, r ) = Att (SCW Q, SCWK , SCWV ) | | r , (5)\nwhere the | | operator indicates that only two behaviors with a distance less than or equal to r are used to calculate the attention. We term r as the “sparsity rate” and adjust its value according to the different datasets in the experiments.\n2.3.3 Sequential Aggregation to Obtain a Final Preference. We first apply layer normalization to GAES and then use an RNN to model the attention on behavior sequence to obtain the user preferences guided by sentiment as follows.\nPGu = RNN ( LayerNorm (GAES ) ) . (6)\nHere, using a simple RNN for sequential modeling to obtain preference PGu involves excluding other factors to directly study the guiding effect of sentiment on behavior sequences.We capitalize on LSTM to instantiate RNN throughout the experiments.\nSimilar to sentiment-guided user preferences, the preference generated by pure sentiment is calculated as follows:\nPGe = RNN ( LayerNorm (ACPW P ) ) . (7)\nHere, WP ∈ Ro×o and o is the size of output dimension of the attention layer. The two types of preferences generated thus far by the two sequential channels are aggregated into the final preference as follows:\nPF =WF LayerNorm ( ConCat ([PGu , PGe ]) ) + BF , (8)\nThus, PGu and PGe are concatenated and then layer normalized as the input to the fully connected layer, which takes the weight WF and the bias BF as parameters. Finally, we employ binary cross entropy loss and the Adam optimizer to train the SGS network."},{"heading":"3 EXPERIMENTS","text":"In this section, we describe experiments intended to answer the following research questions:\nRQ1: Do temporal sentiments have a positive guiding effect on sequential recommendation?\nRQ2: Is pure sequential sentiment modeling helpful in generating user preferences?\nRQ3: Does SGS outperform current state-of-the-art models, including RNN-based methods?"},{"heading":"3.1 Datasets","text":"We conducted experiments on four publicly available datasets. The first two datasets, Restaurant and Bars, are from the Yelp Challenge Dataset1. The other two datasets, Baby and Videogames, were selected from the Amazon dataset2. We first extracted temporal sentiments from reviews corresponding to sequential behaviors by using the Stanford NLP tools [6] and obtained five categories\n1https://www.yelp.com/dataset/challenge 2http://jmcauley.ucsd.edu/data/amazon/\nof sentiment factors: Very Positive, Positive, Neutral, Negative, and Very Negative, which constitute a sentiment state. Then, to facilitate training, we adopted the data augmentation technique reported in [7] to generate subsequences as input data. The maximum length of the subsequences was set to 50 for all four datasets. Finally, the input data were randomly split into a training set, a validation set, and a test set (8 : 1 : 1) for five-fold cross validation. The dataset statistics after these preprocessing operations are listed in Table 1, where Avg.B/user and Avg.B/item denote “average behavior per user” and “average behavior per item”, respectively."},{"heading":"3.2 Baselines and Metrics","text":"In these experiments, we compare SGS with the following relevant RNN-based sequential recommendation models:\n• RNN - an LSTM-based recurrent neural network for sequential recommendation, • GRU4Rec[2] - a representative GRU-based sequential recommender, • GRU4Senti[2] - an improved GRU4Rec model by concatenating user sentiment embeddings with item embeddings, • SASRec[5] - a state-of-the-art self-attention-based sequential recommender, • CFSA[11] - a state-of-the-art self-attention-based sequential recommender that considers sentiments as its features,\n• SGS - our Sentiment-Guided Sequential recommender. We fixed {batch_size, embeddinд_dimension} of all models to {128, 20} on the two smaller datasets (Restaurant,Bars) and {256, 30} on the two larger datasets (Baby,Videogames), respectively. The initial learning rate was uniformly set to 0.001 and cooperated with the Adam optimizer to complete the training. For the multihead selfattention-based methods (SASRec, CFSA, SGS), we adopted a grid search to find the optimal values for the attention-based parameters. Subsequently, the {head_number , size_per_head} of attention were set to {5, 4} and {6, 5} for the small and large datasets, respectively. A hyperparameter specific to SGS is the sparsity rate r , whose optimal values are 3 and 5 for the small and large datasets, respectively. As evaluation metrics, we adopted HitRate (Hit for short) and NDCG to evaluate the performances of all the approaches, and we choose K = {10, 20} to report the different results for Hit@K and NDCG@K ."},{"heading":"3.3 Sentiment Ablation Study","text":"The goal of the sentiment ablation study is to answer both RQ1 and RQ2. We designed three variants of SGS and analyzed their performances:\n• 1) Remove Sentiment Attention: a version of SGS without sentiment-guided attention. • 2) Remove Sparse Attention: a version of SGS without sparse sentiment attention. • 3) Remove All Sentiments: a version of SGS without any sentiment factors.\nFigure 2 illustrates that the two SGS versions with sentiment factors outperformed the SGS version without sentiments, which indicates the effectiveness of temporal sentiments in sequential recommendations. On the one hand, adding a sparse attention of pure sentiments to the behavior sequence modeling channel is helpful for improving the preference prediction performance. On the other hand, the performance of the sentiment-guided attention mechanism is much better than that of the self-attention of only behaviors on several metrics, which means that it has a positive influence on the self-attention mechanism. Fusing two types of sentiment attention further improves the performance of SGS and provides positive answers to RQ1 and RQ2."},{"heading":"3.4 Method Comparisons","text":"Table 2 shows that SGS outperforms all baselines on the four datasets; gaining average improvements of {18.38%, 20.95%, 17.79%, and 20.125%} on {Hit@10,NDCG@10,Hit@20, and NDCG@20}, respectively, compared to the strongest baseline. In particular, SGS\nperforms relatively better on the datasets Restaurant andBarswhich include more average user behaviors. Because each user behavior corresponds to 5 sentiment factors, more behaviors means that the model can learn more sentiment information. To answer RQ3, we first observed two types of comparative models: {GRU4Rec, GRU4Senti} and {SASRec, CFSA}. As a representative model of sequential recommendation, GRU4Rec becomes GRU4Senti after concatenating the user sentiment embeddings with the item embeddings, and the latter achieves better results. Similarly, the selfattention network based on sentiment features (CFSA) performs relatively better than that without sentiments (SASRec). SGS is superior to both types of approaches because it does not simply organize sentiment factors but instead promotes sentiment factors as preference guides modeled in two channels. Thus, the resulting preferences are more human-centric and more meaningful to users."},{"heading":"4 CONCLUSION AND FUTUREWORK","text":"The intent of this work is to model the subjective preferences of users in sequential recommendation. In future work, we will further consider additional subjective factors (such as emotions) to improve sequential preference mining based on human study."},{"heading":"ACKNOWLEDGMENTS","text":"This work is supported by the National Natural Science Foundation of China under Grant No. 61902231, the Guangdong Basic and Applied Basic Research Foundation under Grant No. 2020A1515010531, and the Research Startup Fund Project of Shantou University under Grant No. NTF18017."}],"references":[{"title":"Generating Long Sequences with Sparse Transformers","author":["Rewon Child","Scott Gray","Alec Radford","Ilya Sutskever"],"venue":null,"citeRegEx":"1","shortCiteRegEx":"1","year":2019},{"title":"Session-based Recommendations with Recurrent Neural Networks","author":["Balázs Hidasi","Alexandros Karatzoglou","Linas Baltrunas","Domonkos Tikk"],"venue":"In Proceedings of ICLR 2016","citeRegEx":"2","shortCiteRegEx":"2","year":2016},{"title":"Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks","author":["Jin Huang","Wayne Xin Zhao","Hongjian Dou","Ji-Rong Wen","Edward Y. Chang"],"venue":"In The 41st ACM SIGIR Conference (SIGIR","citeRegEx":"3","shortCiteRegEx":"3","year":2018},{"title":"Review Sentiment-Guided Scalable Deep Recommender System","author":["Dongmin Hyun","Chanyoung Park","Min-Chul Yang","Ilhyeon Song","Jung-Tae Lee","Hwanjo Yu"],"venue":"In The 41st ACM SIGIR Conference (SIGIR","citeRegEx":"4","shortCiteRegEx":"4","year":2018},{"title":"Self-Attentive Sequential Recommendation","author":["W. Kang","J. McAuley"],"venue":"IEEE International Conference on Data Mining (ICDM","citeRegEx":"5","shortCiteRegEx":"5","year":2018},{"title":"The Stanford CoreNLP Natural Language Processing Toolkit","author":["Christopher Manning","Mihai Surdeanu","John Bauer","Jenny Finkel","Steven Bethard","David McClosky"],"venue":"In Proceedings of the 52nd ACL","citeRegEx":"6","shortCiteRegEx":"6","year":2014},{"title":"Improved recurrent neural networks for session-based recommendations","author":["Yong Kiam Tan","Xinxing Xu","Yong Liu"],"venue":"In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems","citeRegEx":"7","shortCiteRegEx":"7","year":2016},{"title":"Attention is All you Need","author":["Ashish Vaswani","Noam Shazeer","Niki Parmar","Jakob Uszkoreit","Llion Jones","Aidan N Gomez","Ł ukasz Kaiser","Illia Polosukhin"],"venue":"In NeurIPS. Curran Associates,","citeRegEx":"8","shortCiteRegEx":"8","year":2017},{"title":"Sequential Recommender Systems: Challenges, Progress and Prospects","author":["Shoujin Wang","Liang Hu","Yan Wang","Longbing Cao","Quan Z. Sheng","Mehmet Orgun"],"venue":"In Proceedings of IJCAI","citeRegEx":"9","shortCiteRegEx":"9","year":2019},{"title":"NeuO: Exploiting the sentimental bias between ratings and reviews with neural networks","author":["Yuanbo Xu","Yongjian Yang","Jiayu Han","En Wang","Fuzhen Zhuang","Jingyuan Yang","Hui Xiong"],"venue":"Neural Networks","citeRegEx":"10","shortCiteRegEx":"10","year":2019},{"title":"Feature-level Deeper Self-Attention Network for Sequential Recommendation","author":["Tingting Zhang","Pengpeng Zhao","Yanchi Liu","Victor S. Sheng","Jiajie Xu","Deqing Wang","Guanfeng Liu","Xiaofang Zhou"],"venue":"In Proceedings of IJCAI","citeRegEx":"11","shortCiteRegEx":"11","year":2019},{"title":"Memory Reorganization: A Symmetric Memory Network for Reorganizing Neighbors and Topics to Complete Rating Prediction","author":["Lin Zheng","Naicheng Guo","Jin Yu","Dazhi Jiang"],"venue":"IEEE Access","citeRegEx":"12","shortCiteRegEx":"12","year":2020}],"referenceMentions":[{"referenceID":8,"context":"3401330 specifically model sequential behavior has yielded many fruitful results [9].","startOffset":81,"endOffset":84},{"referenceID":1,"context":"As an initial attempt at applying deep learning to sequential recommendation, GRU4Rec [2] concatenates the behavior sequences of different users in a reasonable manner, thereby solving the difficult problem of uniformly modeling user behavior sequences of different lengths.","startOffset":86,"endOffset":89},{"referenceID":6,"context":"This method illustrates the important role of recurrent neural networks (RNNs) [7] in sequential recommendation.","startOffset":79,"endOffset":82},{"referenceID":2,"context":"Taking this research as a starting point, subsequent advanced approaches, such as memory-based RNNs [3, 12] and self-attention-based RNNs [5, 11], have taken sequential recommendation technology to a new level.","startOffset":100,"endOffset":107},{"referenceID":11,"context":"Taking this research as a starting point, subsequent advanced approaches, such as memory-based RNNs [3, 12] and self-attention-based RNNs [5, 11], have taken sequential recommendation technology to a new level.","startOffset":100,"endOffset":107},{"referenceID":4,"context":"Taking this research as a starting point, subsequent advanced approaches, such as memory-based RNNs [3, 12] and self-attention-based RNNs [5, 11], have taken sequential recommendation technology to a new level.","startOffset":138,"endOffset":145},{"referenceID":10,"context":"Taking this research as a starting point, subsequent advanced approaches, such as memory-based RNNs [3, 12] and self-attention-based RNNs [5, 11], have taken sequential recommendation technology to a new level.","startOffset":138,"endOffset":145},{"referenceID":2,"context":"These advanced technologies not only complete the modeling of pure behavior (item) sequences but also fully characterize additional information such as knowledge [3] and features [11] related to items to improve the sequence prediction performance.","startOffset":162,"endOffset":165},{"referenceID":10,"context":"These advanced technologies not only complete the modeling of pure behavior (item) sequences but also fully characterize additional information such as knowledge [3] and features [11] related to items to improve the sequence prediction performance.","startOffset":179,"endOffset":183},{"referenceID":7,"context":"Some recent models based on the self-attention mechanism [8] perform particularly well in sequential recommendation.","startOffset":57,"endOffset":60},{"referenceID":4,"context":"For example, SASRec [5] effectively captures users’ long-term preferences from both sparse and dense data sets, and in FDSA [11], two separated self-attention blocks (one for item sequences and one for features) were designed that achieved remarkable predictive effects.","startOffset":20,"endOffset":23},{"referenceID":10,"context":"For example, SASRec [5] effectively captures users’ long-term preferences from both sparse and dense data sets, and in FDSA [11], two separated self-attention blocks (one for item sequences and one for features) were designed that achieved remarkable predictive effects.","startOffset":124,"endOffset":128},{"referenceID":3,"context":"However, most of the existing models based on sentiment factors [4, 10] focus only on non-sequential recommendations.","startOffset":64,"endOffset":71},{"referenceID":9,"context":"However, most of the existing models based on sentiment factors [4, 10] focus only on non-sequential recommendations.","startOffset":64,"endOffset":71},{"referenceID":0,"context":"To this end, we incorporate a variant sparse sentiment attention [1] to model the sentiment factors in the sequence separately because a certain behavior in the sequence corresponds toM sentiment factors, and sparse attention can improve the calculation efficiency by reducing the number of attention interactions in the sequence.","startOffset":65,"endOffset":68},{"referenceID":4,"context":"The self-attention mechanism has been shown to be highly effective at sequential recommendation [5, 11].","startOffset":96,"endOffset":103},{"referenceID":10,"context":"The self-attention mechanism has been shown to be highly effective at sequential recommendation [5, 11].","startOffset":96,"endOffset":103},{"referenceID":7,"context":"It is based on the scaled dot-product attention [8], which is defined as follows:","startOffset":48,"endOffset":51},{"referenceID":0,"context":"To model the sentiment-generated preference, we applied a simplified sparse attention mechanism [1] to model pure sentiments and specifically designed its inputs to facilitate interactions between sentiment factors.","startOffset":96,"endOffset":99},{"referenceID":5,"context":"We first extracted temporal sentiments from reviews corresponding to sequential behaviors by using the Stanford NLP tools [6] and obtained five categories","startOffset":122,"endOffset":125},{"referenceID":6,"context":"Then, to facilitate training, we adopted the data augmentation technique reported in [7] to generate subsequences as input data.","startOffset":85,"endOffset":88},{"referenceID":1,"context":"• RNN - an LSTM-based recurrent neural network for sequential recommendation, • GRU4Rec[2] - a representative GRU-based sequential recommender, • GRU4Senti[2] - an improved GRU4Rec model by concatenating user sentiment embeddings with item embeddings, • SASRec[5] - a state-of-the-art self-attention-based sequential recommender, • CFSA[11] - a state-of-the-art self-attention-based sequential recommender that considers sentiments as its features, • SGS - our Sentiment-Guided Sequential recommender.","startOffset":87,"endOffset":90},{"referenceID":1,"context":"• RNN - an LSTM-based recurrent neural network for sequential recommendation, • GRU4Rec[2] - a representative GRU-based sequential recommender, • GRU4Senti[2] - an improved GRU4Rec model by concatenating user sentiment embeddings with item embeddings, • SASRec[5] - a state-of-the-art self-attention-based sequential recommender, • CFSA[11] - a state-of-the-art self-attention-based sequential recommender that considers sentiments as its features, • SGS - our Sentiment-Guided Sequential recommender.","startOffset":155,"endOffset":158},{"referenceID":4,"context":"• RNN - an LSTM-based recurrent neural network for sequential recommendation, • GRU4Rec[2] - a representative GRU-based sequential recommender, • GRU4Senti[2] - an improved GRU4Rec model by concatenating user sentiment embeddings with item embeddings, • SASRec[5] - a state-of-the-art self-attention-based sequential recommender, • CFSA[11] - a state-of-the-art self-attention-based sequential recommender that considers sentiments as its features, • SGS - our Sentiment-Guided Sequential recommender.","startOffset":260,"endOffset":263},{"referenceID":10,"context":"• RNN - an LSTM-based recurrent neural network for sequential recommendation, • GRU4Rec[2] - a representative GRU-based sequential recommender, • GRU4Senti[2] - an improved GRU4Rec model by concatenating user sentiment embeddings with item embeddings, • SASRec[5] - a state-of-the-art self-attention-based sequential recommender, • CFSA[11] - a state-of-the-art self-attention-based sequential recommender that considers sentiments as its features, • SGS - our Sentiment-Guided Sequential recommender.","startOffset":336,"endOffset":340}],"year":2020,"abstractText":"The existing sequential recommendation methods focus on modeling the temporal relationships of user behaviors and are good at using additional item information to improve performance. However, these methods rarely consider the influences of users’ sequential subjective sentiments on their behaviors—and sometimes the temporal changes in human sentiment patterns plays a decisive role in users’ final preferences. To investigate the influence of temporal sentiments on user preferences, we propose generating preferences by guiding user behavior through sequential sentiments. Specifically, we design a dual-channel fusion mechanism. The main channel consists of sentiment-guided attention to match and guide sequential user behavior, and the secondary channel consists of sparse sentiment attention to assist in preference generation. In the experiments, we demonstrate the effectiveness of these two sentiment modelingmechanisms through ablation studies. Our approach outperforms current state-of-the-art sequential recommendation methods that incorporate sentiment factors.","creator":"LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2018/11/30 v6.88e Hypertext links for LaTeX"}}
{"name":"3397271.3401430.pdf","metadata":{"source":"CRF","title":"FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval","authors":["Dehong Gao","Linbo Jin","Ben Chen","Minghui Qiu","Peng Li","Yi Wei","Yi Hu","Hao Wang"],"emails":["longran.wh}@alibaba-inc.com"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information System • Information Retrieval • Specialized Information Retrieval • Multimedia and Multimodal Retrieval\nKEYWORDS FashionBERT, Text and Image matching, Cross-modal retrieval\nACM Reference format:\nDehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu and Hao Wang. 2020. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’20), July 25-30, Virtual Event, China. ACM, New York, NY, USA, 10 pages. hps://doi.org/10.1145/3397271.3401430"},{"heading":"1 Introduction","text":"Over the last decade, a great number of multimedia data (including image, video, audio, and text) have been emerged on the internet. To search important information from these multimodal data efficiently, multimedia retrieval is becoming an essential technique and widely researched by world-wide researchers. Recently, it has been witnessed a soar increase of the research interest in cross-modal retrieval, which takes one type of data as the query and retrieves relevant data of another type. The pivot of cross-modal retrieval is to learn a meaningful cross-modal matching [40].\nThere exists a long research line in cross-modal matching, especially in text and image matching. The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41]. Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46]. Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34]. The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2]. However, these studies are centered on text and image matching of the general domain. In this paper, we focus on the text and image matching of the fashion industry1, which is mainly referred to clothing, footwear, accessories, makeup and etc.\n1 https://en.wikipedia.org/wiki/Fashion\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR’20, July 25-30, 2020, Virtual Event, China. © 2020 Association of Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07 $15.00. DOI: https://doi.org/10.1145/3397271.3401430\nThe main challenge of these pioneer matching approaches is how to extract the semantic information from images, and integrate this information into the BERT model. All current approaches detect RoIs2 (i.e., Region of Interest [13]) from images as seen in Figure 1(a) and treat these RoIs as “image tokens”. But this RoI method does not work well in the fashion domain since relatively-rare RoIs can be detected from fashion images. As seen in Figure 1(b), we show the detected RoIs of Fashion-Gen images of different categories, where the minimum number of detected RoIs is set to one from an image. We found on average 19.8 RoIs can be detected from one MSCOCO3 image, but only 6.4 can be detected from one Fashion-Gen4 image. This is because in general a fashion image contains only one or two objects (e.g., a coat and/or a pant) with a flat background. We can set the minimum RoI number to detect, but under this setting lots of detected RoIs are repeated since they only focus on the same object(s) as seen in Figure 1(e). These repeated RoIs will produce similar features and contribute little to the later modeling. Meanwhile, we find some RoIs from fashion images are useless for text and image matching, for example, RoIs about the body parts (head, hair, hands etc.) of the models in fashion images as seen in Figure 1(f). These RoIs are irrelated to the fashion products and cannot build connection with the descriptions. On the contrary, most of the fashion texts describe the fine-grained information about the products (e.g., “crew neck”, “off-shoulder”, “high collar”). Occasionally, some of descriptions contain abstract styles, e.g., “artsy” and “bohemian” as seen in Figure 1(d). The RoIs in fashion images can indicate main fashion object(s), but fail to distinguish these fine-grained attributes or styles. Thus, it is more difficult for fashion text and\n2 https://github.com/peteanderson80/bottom-up-attention 3 http://cocodataset.org\nimage matching with such “object-level” RoIs and fine-grained descriptions.\nIn this paper, we propose FashionBERT to solve the above problems. Inspired by the selfie idea [38], we first introduce the patch method to extract image tokens. Each fashion image is split to small patches with the same pixels and we assume these patches as image tokens. The patches show more rawer pixel information, and thus contain more detained information compared with object-level RoIs. Besides, the split patches are non-repeated and ordered in nature, which are well suitable as the sequence inputs of the BERT model. The training procedure of FashionBERT is a standard multitask learning procedure (i.e., Masked Language Modeling, Masked Patch Modeling and Text&Image Alignment, which will be depicted in the later section). We propose an adaptive algorithm to balance the learning of each task. The adaptive algorithm treats the determination of loss weights of each task as a new optimal problem and will estimate the loss weights in each batch step.\nWe evaluate FashionBERT with two tasks, Text&Image alignment classification and cross-modal retrieval (including Image-to-Text and Text-to-Image retrieval). Experiments are conducted on the public fashion product dataset (Fashion-Gen). The results show that FashionBERT significantly outperforms the SOTA and other pioneer approaches. We also apply FashionBERT in our E-commercial website. The main contributions of this paper are summarized as follows: 1) We show the difficulties of text and image matching in the fashion domain and propose FashionBERT to address these issues. 2) We present the patch method to extract image tokens, and the adaptive algorithm to balance the multitask learning of\n4 https://fashion-gen.com/\nFashionBERT. The patch method and the adaptive algorithm are task-agnostic, which can be directly applied in others tasks. 3) We extensively experiment FashionBERT on the public dataset. Experiments show the powerful ability of FashionBERT in text and image matching of the fashion domain. 4) FashionBERT currently has been applied in practice. We present the concrete application of FashionBERT in cross-modal retrieval. Meanwhile, we analyze both matching performances and inference efficiencies in detail."},{"heading":"2 Methodology","text":"In this section, we will briefly revisit the BERT language model and then describe how we extract the image features and how FashionBERT jointly models the image and text data."},{"heading":"2.1 BERT","text":"The BERT model introduced by [8] is an attention-based bidirectional language model. Taking tokens (i.e., word pieces) as inputs, BERT processes the embeddings of tokens with a multi-\nlayer Transformer encoder [39]. When pre-trained on a large language corpus, BERT has proven to be very effective for transfer learning in variants of natural language processing tasks. The original BERT model focuses on encoding of the singlemodality text data. In the cross-modal scenario, the extended BERT model takes multi-modality data as input and allows them to interact within the Transformer blocks."},{"heading":"2.2 FashionBERT","text":"The overview of FashionBERT is illustrated in Figure 2. It is composed of four parts, text representation, image representation, matching backbone and FashionBERT training with adaptive loss. Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42]. The same BERT vocabulary is adopted in our FashionBERT model. We use the standard BERT pre-process method to process the input text. Finally, the sum of the word-piece embedding, position embedding and segmentation embedding is regarded as the text representation. The segmentation (i.e., “T” and “I” in Figure 2) is used to differentiate text and image inputs.\nImage Representation: Different from the RoI method, we cut each image into patches with the same pixels as illustrated in Figure 2. We regard each patch as an “image token”. For each patch, the outputs of the patch network are regarded as the patch features. It is possible to select any pre-trained image model, (e.g., InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network. These patches are ordered in nature. The spatial positions of the patches are used in the position embedding. The sum of the patch features, the position embedding and segmentation embedding are regarded as patch representations. Matching Backbone: The concatenation of the text token sequence and image patch sequence consists of the FashionBERT inputs. Similar to BERT, the special token [CLS] and separate token [SEP] are added in the first position and between the text token sequence and the image patch sequence, respectively.\nThe pre-trained standard BERT is adopted as the matching backbone network of FashionBERT. The information of text tokens and image patches thus interact freely in multiple selfattention layers. FashionBERT outputs the final representations of each token or patch. FashionBERT Training with Adaptive Loss: We exploit three tasks to train FashionBERT.\nMasked Language Modeling (MLM): This task is very similar to the MLM task utilized in BERT pre-training. We apply the Whole Word Masking (WWM) strategy to mask out all the text tokens corresponding to a word at once [6]. For example, in Figure 2, “long sleeve hoodie in black” is masked as “long sleeve [MSK] [MSK] in black”, rather than “long sleeve [MSK] #ie in [MSK]” occasionally. The input tokens are masked out with the probability of 15%. Given a text token sequence \uD835\uDC61 = {\uD835\uDC61 , \uD835\uDC61 , … , \uD835\uDC61 }, the masked-out sequence is denoted by \uD835\uDC61\\ = {\uD835\uDC61 , \uD835\uDC61 , … , [\uD835\uDC40\uD835\uDC46\uD835\uDC3E] , … , \uD835\uDC61 }, which denotes token i is masked out. The operator “\\” means removing. The last-layer hidden outputs of the masked-out tokens are fed into a classifier over the standard BERT vocabularies. Finally, the MLM task is to minimize the cross-entropy loss, written as\n~ \\( ) log ( , )MLM t D i il E P t t   (1) where \uD835\uDF03 is the FashionBERT parameters and \uD835\uDC37 is the whole training set. \uD835\uDC43(\uD835\uDC61 |\uD835\uDC61\\ , \uD835\uDF03) denotes the probability of the maskedout token \uD835\uDC61 predicted by FashionBERT, given surrounding tokens \uD835\uDC61\\ .\nMasked Patch Modeling (MPM): Similar to MLM, we mask out certain patches in a patch sequence in the MPM task. Given an image patch sequence \uD835\uDC5D = {\uD835\uDC5D , \uD835\uDC5D , … , \uD835\uDC5D }, we randomly mask out patches with the probability of 10%, denoting as \uD835\uDC5D\\ = {\uD835\uDC5D , \uD835\uDC5D , … , [\uD835\uDC40\uD835\uDC46\uD835\uDC3E] , … , \uD835\uDC5D }. The patch features of masked-out patches are set to zero. MPM is to predict the distribution over the masked-out patch features. The MPM training is supervised by minimizing the KL-divergence between the distributions of patch features.\n~ \\ ( ) ( .( , ) | .( )) p DMPM KL i i i l E Distr p p Distr p   (2) Text and Image Alignment (TIA): In the TIA task, the hidden output of the special token [CLS] is fed into a binary classifier to indicate whether the text and image data are matched. For one\npositive example in the train dataset, the text and image are extracted from one same fashion product, while for one negative sample, the text and image are randomly selected from different fashion products. TIA objects to optimize the binary crossentropy loss.\n, ~ ˆ( ) [ log ( | , , )\nˆ(1 ) log(1 ( | , , ))]\nTIA t p Dl E y P y t p\ny P y t p\n               (3)\nwhere y and \uD835\uDC66 denote the true and predicted labels, respectively. In sum, FashionBERT attempts to optimize the aggregated loss function as seen in Equation (4), which is well acknowledged as an multitask learning problem.\n1\n( ) ( ) L\ni i i L l     (4) \uD835\uDC3F is the task number and in FashionBERT and \uD835\uDC3F equals to three. ω are the loss weights to balance the learning of each task. We treat the determination of the loss weight \uD835\uDF14 as a new optimal problem:\n22\n1 , 1\n1\n1 1 arg min\n2 2\n. . 1 0\nL L\ni i i j i i j\nL\ni i i\nl\ns t and\n  \n \n \n\n   \n  \n \n (5)\nIn Equation (5), on one hand we aim to minimum the total weighted loss, and on the other hand we expect FashionBERT fairly treats the learning of all tasks. Considering the KKT conditions (Karush-Kuhn-Tucher Conditions) [3], we can obtain the solution to \uD835\uDF14 as\n2 1\n2 1\n1\n( )\n( )\ni i L\nii\nL l\nL l \n \n \n \n (6)\nThe proof to this solution is in the appendix. We illustrate the training procedure of FashionBERT in Algorithm.1."},{"heading":"3 Experiments","text":"In this section, we describe our experimental settings and show the main results.\nAlgorithm 1 FashionBERT Training with Adaptive Loss Input: the aggregated loss functions ℒ(\uD835\uDF03) = ∑ \uD835\uDC59 (\uD835\uDF03) and training dataset D Parameter: the model parameters \uD835\uDF03 Output: \uD835\uDF03 1: Let \uD835\uDF14 = 1 \uD835\uDC3F⁄ . 2: for each batch of train data do 3: Feed the train batch into FashionBERT to get all \uD835\uDC59 . 4: Obtain weight losses according to Equation (6) 5: Aggregate loss function according to Equation (4) 6: Update FashionBERT by optimizing ℒ(\uD835\uDF03) with ADAM \uD835\uDF03 = \uD835\uDF03 − \uD835\uDF02 \uD835\uDF15ℒ(\uD835\uDF03) \uD835\uDF15\uD835\uDF03⁄ 7: end for 8: return model parameters \uD835\uDF03"},{"heading":"3.1 Experimental Settings","text":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions. In our experiments, we adopt the Fashion-Gen dataset, which contains 67,666 fashion products. The Fashion products are associated with text descriptions provided by professional stylists, which are high-quality data for our FashionBERT pre-training. Each product is photographed from 1 to 6 different angles. We collect 293,008 image (256*256 pixels) and description pairs, among which 260,480 pairs are used for training, and 32,528 for testing. FashionBERT is pre-trained on the training dataset. We directly evaluate it on the testing dataset. Evaluation Tasks and Metrics: We introduce two tasks (i.e., Text and Image Matching, and Cross-modal Retrieval) to test the FashionBERT performances. For Text and Image Matching evaluation, all the test data are adopted. Given a text and image pair, the output of the TIA classifier is regarded as the matching similarity. We use Accuracy to assess the FashionBERT performance on this matching task.\nFor Cross-modal Retrieval evaluation, 1,000 unique description queries and 1,000 unique image queries are randomly selected from the test data. Given one description (or image) query, the ground-truth image (or description) from the same product and randomly-sampled 100 images (or description) from other products consist of the candidate rank set. Then we acquire the evaluation datasets for text-to-image and image-totext retrieval. Same with [23], we score each description and image pair in the test dataset and then sort the candidate rank set. We use three metrics (i.e. Rank@K (K=1, 5, 10)) to evaluate FashionBERT on Cross-modal Retrieval. Rank@K is the percentage of ground-truth matchings appearing in the top-K ranked list.\nImplementation Details: We reuse the pre-trained parameters from the 12-layer BERT-base. Each block has 768 hidden units and 12 self-attention heads. The text representation follows the same processing of BERT. The maximum sequence length is set to 512, among which the patch sequence is set to 64 (8*8 patches) and the maximum text sequence length is set to 448 (=512-64, including the special tokens). For each patch, ResNeXt101 [43] is first adopted to extract the patch features and the dimensions of patch features are 2048.\nFashionBERT is implemented with Tensorflow5 and trained on 8*Tesla V100 GPUs with early stopping to avoid overfitting (it usually had run about 10 epochs when stopping). In each training batch, 64 shuffled <Text, Image> pairs are utilized. Adam optimizer is applied with the learning rating of 2e-5, \uD835\uDEFD = 0.95, \uD835\uDEFD = 0.999, weight decay of 1e-4, learning rate warmed up at the first 5,000 steps, and then linear decay."},{"heading":"3.2 Evaluation of the SOTA and","text":"Pionner Approaches\nIn this section, we conduct the experiments to response two questions:  Does our model perform well comparing with the baseline\napproaches? We implement the following baseline approaches. VSE [11]: VSE directly projects image features and text features into visual semantic embedding space in an end-toend manner, which is regarded as our first baseline approach. VSE++ [10]: VSE++ extends the VSE approach with modification of the rank-based loss function which pays more attention to the hard negatives. SCAN [18]: SCAN performs the cross-modal match on the image RoIs and text tokens. Meanwhile, the aention mechanism [39] is leveraged to enhance matching ability.\n5 https://www.tensorflow.org/\nPFAN [41]: The position information of each RoI is incorporated in the PFAN model. By this way, the matching can better understand the position information in both texts and images.  Does our model perform well comparing with the state-ofthe-art (SOTA) pre-trained approaches? We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34]. ViLBERT-ZeroShot [23]: In ViLBERT, the authors fine-tune and evaluate ViLBERT with cross-modal retrieval. They release the fine-tuned cross-modal retrieval model as well. Thus, in this experiment, we evaluate the Fashion-Gen testing data with the released ViLBERT model. We follow the same RoI extraction method in ViLBERT. For each image, we keep 10 to 36 high-scoring regions with Faster R-CNN [13] pre-trained on the Visual Genome dataset [27]. ViLBERT-Finetune: In this experiment, based on the pretrained ViLBERT, we fine-tune a new cross-modal retrieval model with the Fashion-Gen training data. VLBERT-Finetune [34]: The pre-trained VLBERT model is not evaluated with cross-modal retrieval. We thus fine-tune a new cross-modal retrieval model with the pre-trained VLBERT. In VLBERT, we follow its RoI extraction setting, where the minimum number of RoIs is set to 10, and at most 100 RoIs with detection scores higher than 0.5 are selected for each image. All these approaches have the same settings, e.g., hidden layers, embedding size, patch features. Each experiment runs three times, and the average performances are shown in Table 1. we observe that FashionBERT with patch and adaptive loss achieves the significant improvement on Rank@K metrices (statistically significant difference over the other baselines with \uD835\uDC5D<0.1). This shows the excellent ability of FashionBERT in fashion text and image matching. In the ViLBERT-ZeroShot experiment, though ViLBERT is pre-trained with the general domain dataset, it does not perform well in matching of the fashion domain, which hints that there may exist a large gap between the fashion domain and the general domain. Thus, the pre-training model with general domain dataset contributes little to the matching of the fashion domain. This also explains that after finetuning with the Fashion-Gen dataset, a soar increase is achieved in the ViLBERT-Finetune experiment. Furthermore, it is found that FashionBERT benefits more from the patch method than from the RoI method compared with the RoI-based ViLBERT/VLBERT experiments and our Patch-based FashionBERT. As mentioned in section 1, more useless and repeated RoIs are detected from fashion images. The selfsupervised learning of FashionBERT was expected to mask out some of these RoIs and to predict them with surrounding ones. However, surrounding tokens may either provide the irrelevant information (the useless RoIs, e.g., the head, hands of the models) or almost the same information (the repeated RoIs which generate the similar patch features as well). It is hard for the\n6 https://github.com/jiasenlu/vilbert_beta 7 https://github.com/jackroos/VL-BERT\nmodel to learn useful information from these noise regions. On the contrary, the patches provide non-repeated and reasonablyrelated information, which is more suitable for self-supervised learning and enhances the performance of the fashion text and image matching."},{"heading":"3.3 Ablation Studies","text":"In this section, we conduct ablation experiments in order to incrementally exam the influences of adaptive loss, pre-trained image models and model size of BERT. Effect of Adaptive Loss: We first investigate the contribution of the task-agnostic adaptive loss algorithm. We test FashionBERT with and without adaptive loss. When without adaptive loss, \uD835\uDF14 is set to 1 \uD835\uDC3F⁄ .\nThe evaluation results are showed in Figure 3, which illustrates the adaptive loss weight produces positive influence on the FashionBERT performance. As shown in Equation (6), \uD835\uDF14 is in direct ratio to \uD835\uDC59 . This means that when the task \uD835\uDC56 gets a larger loss, the adaptive loss weight \uD835\uDF14 will be larger than the others. In consequence, FashionBERT will pay more attention to the learning of the task \uD835\uDC56. We show the adaptive loss weights during the training batches in Figure 4. It is found that at the\nbeginning FashionBERT pays more attention to MLM and TIA since these two tasks are newly introduced in BERT. Later on, the weight of TIA and MPM decay. At the same time, we find that when FashionBERT well matches the fashion texts and images (as seen ACC_TIA in Figure 4), it shifts its attention on the MPM and MLM tasks. These two tasks are relatively harder than the TIA task. This may also hint that there is still room for further improvements if more difficult matching tasks can be introduced in, for example, the token-level and patch-level alignment. Effect of pre-trained image models: We compare different pre-trained image models when extracting patch features. In this section, we compare ResNeXt-101 with InceptionV3. For the sake of fair comparison, we use their pre-trained parameters in Inception V3 and ResNeXt-101, respectively. The dimensions of the patch features are set to 2048 and the same hyperparameters are used in FashionBERT.\nThe evaluation results are illustrated in Table 2, where FashionBERT with ResNeXt-101 shows clearly better results than that with Inception V3. Compared with Inception V3, ResNeXt101 contains a series of residual network blocks. This residual structure brings more raw-pixel information into the Transformer encoders, which helps the modeling. We will explore more pre-trained tasks and extract more representative information. Effect of Model Size: In order to test the influence of the model size, we vary the number of the transformer encoder layers. FashionBERT is experimented with 4-layer, 6-layer and 12-layer transformer encoders. The 4-layer FashionBERT means that we load the first four layers of the pre-trained BERT. The other hyperparameters are consistent among all the experiments.\nThe evaluation results are shown in Table 3, which demonstrates that FashionBERT benefits from the deeper BERT encoders over all the metrices except Rank@1 (Rank@1 tends to be more sensitive than the other two). Due to limited resources, we did not experiment with the pre-trained BERT-Large. It is possible to achieve further improvements with the 24-layer BERT model."},{"heading":"3.4 Industry Applications","text":"FashionBERT is a general text and image encoder in the fashion domain, which can be widely applied in varieties of text and image matching tasks. The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform\nretrieval and ranking with nearest-neighbor search [15, 45]. The main framework of cross-modal retrieval is shown in Figure 5.\nIn practice cross-modal retrieval, FashionBERT is pretrained and fine-tuned with private datasets from scratch. The pre-train <Text, Image> pairs are collected from fashion products in our Alibaba.com8 website, where the titles of products act as the text information. The fine-tune dataset of cross-modal retrieval is extracted from logs of the search engine. From search logs, the queries and their clicked products first compose of the click dataset. In consequence, <Query, Title, Image> triples are chosen as the fine-tune dataset, where “Title” and “Image” are from the same clicked products. Totally, ten million <Title, Image> pairs are collected for pre-training. Two million <Query, Title, Image> triples are collected for fine-tuning, of which ninety percent are used in training and the rest are used in testing. Like the downstream task of Visual Question Answer (VQA) in VLBERT [34], “Q”, “T” and “I” are utilized in the segmentation to distinguish the three input types of “Query”, “Title” and “Image” in fine-tuning. We show the fine-tune model of cross-modal retrieval in Figure 6.\nThe previous experiments already prove the advantages of FashionBERT in the public dataset. In this set of experiments, we focus on the comparison of cross-modal retrieval and singlemodal retrieval. To certain degree, the fine-tune task can be regarded as the click prediction in information retrieval. Thus, we employ the accuracy and AUC metrics to evaluate the performance.\n8 www.alibaba.com - the global wholesale online trading market\noriBERT: This approach is the online baseline approach. The image information is not incorporated either in pre-training or in fine-tuning. In pre-training, we adopt the original pre-trained BERT model [8]. In fine-tuning, <Query, Title> pairs from <Query, Title, Image> triples compose of the training and testing datasets. Thus, this approach can be regarded as the singlemodel retrieval approach.\nBERT+IMG: In this experiment, image information is not interacted with text information in BERT. Rather than feeding image features into BERT, we concatenate them with the BERT outputs of the final layer for click prediction in fine-tuning.\nFurthermore, the inference speed is pivotal for online deployment. The computational and memory complexity of selfattention in BERT is Ο(\uD835\uDC3E \uD835\uDC37) , highly related to the input sequence length \uD835\uDC3E (\uD835\uDC3E = \uD835\uDC5A + \uD835\uDC5B in our FashionBERT) and the hidden dimension \uD835\uDC37[39]. The performance can be improved with a shorter sequence length. To speed up the online inference, we attempt the Variable Sequence Length (VSL) strategy, which directly concatenates the text and patch sequences, rather than first pads either of them to the max length.\nThe evaluation results are shown in Table 4. First, both the BERT+IMG and FashionBERT approaches benefit a lot from image features compared with oriBERT. Meanwhile, image features further enhance the performances after fully interacting with text features in FashionBERT. Second, though better performances are obtained with six-layer BERT or FashionBERT, the inference latencies vastly excel the online requirements. To trade off inference speeds and matching performances, model reduction has to be introduced in and only the first two FashionBERT layer in our final model. Meanwhile, we observe that the VSL strategy is quite effective in accelerating the speed and has very little influence on performances.\nOnline inference is still one of the most challenging issues for BERT-like models in deployment. More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26]. We are now attempting to incorporate these approaches in FashionBERT to further accelerate the online performance."},{"heading":"4 Related Work","text":""},{"heading":"4.1 Pre-training","text":"The pre-training technique recently has been widely adopted in Machine Learning, which allows the learning model to leverage information from other related tasks.\nThe pre-training technique becomes popular first in CV. Krizhevsky et al. propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7]. Later on, the researchers found that these CNN blocks in AlexNet pretrained on ImageNet or the other large-scale image corpus can be treated as general feature extractors and perform well in various of downstream tasks [9]. Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43]. These pre-trained models are widely adopted in the\nCV tasks, like object detection [12] and semantic segmentation [22]. The applications of the pre-training technique in NLP is behind in CV. The early studies on pre-training in NLP can be traced back to word embedding, such as word2vec, GloVe [30]. Transformer is proposed to leverage the self-attention mechanism to draw global dependencies between inputs and outputs [39]. After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks. Very recently, the selfsupervised learning draws increasing attention of researchers, which allow the model to pre-train with large-scale unlabeled data and learn a more general representation across tasks."},{"heading":"4.2 Text and Image Matching","text":"There is a long research history on text and image matching. These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].\nHardoon et al., overview the applications of Canonical Correlation Analysis (CCA), where CCA is introduced to project text and image features into a shared vector space by maximizing the cross relation [14]. Researchers then propose variants of CCA-based approaches [25, 32, 44]. Visual-semantic embedding (VSE) present by Frome et al., learns semantic relationships between labels and explicitly maps images into the semantic embedding space [11]. Faghri et al., extend VSE and propose VSE++ by introducing the hard-negative mining technique [10]. SCAN performs the cross-modal match on the image RoIs and text tokens. Meanwhile, the attention mechanism is leveraged to enhance matching ability [18]. The position information of each RoI is incorporated in the PFAN model [41]. By this way, the matching can better understand the position information in both texts and images. Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks. VideoBERT [35] lever-ages off-the-shelf networks for action recognition from video clips. The clusters of video clips are regarded as visual words. VideoBERT learns the high-level video representation by using BERT as the backbone network. ViLBERT [23] focuses on the representation learning of the general domain. It extracts RoIs from images and regards these\nRoIs as image tokens. Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.\nIn this paper, we mainly follow this research line. We argue that this RoI method used in these BERT-based approaches does not work well in the fashion domain since the detected RoIs from fashion images are not fine-grained enough for fashion text and image matching."},{"heading":"5 Conclusions","text":"In this paper, we focus on the text and image matching in crossmodal retrieval of the fashion domain. We propose FashionBERT to address the matching issues in the fashion domain. FashionBERT splits images into patches. The images patches and the text tokens are as the inputs of the BERT backbone. To trade off the learning of each task, we present the adaptive loss algorithm which automatically determines the loss weights. Two tasks are incorporated to evaluate FashionBERT and extensive experiments are conducted on the Fashion-Gen dataset. The main conclusions are 1) the patch method shows its advantages in matching fashion texts and images, compared with the objectlevel RoI method; 2) through the adaptive loss, FashionBERT shifts its attention on different tasks during the training procedure.\nCompared with the matching of the general domain, there is still room for further improvements in the fashion domain. In the future, 1) To better understand the semantic of the fashion images, we attempt to construct more fine-grained training task (for example, token-level and patch-level alignment) to force FashionBERT to learn more detail information. 2) We attempt to visualize the FashionBERT matching secrets. This would help to understand how FashionBERT work inside and make further improvement. 3) We are attempting the model reduction, knowledge distillation approaches to further speed up the online inference.\nAPPENDIX PROOF. The adaptive loss weights learning can be written as\n22\n1 . 1\n1\n1 1 arg min\n2 2\n. . 1 0\nL L\ni i i j i i j\nL\ni i i\nl\ns t and\n   \n \n  \n\n   \n  \n \n (1)\nWe first omit the non-negative constraint and apply the Lagrange multipliers and get the Lagrange:\n2\n1 . 1\n1\n1 1 ( , )\n2 2\n(1 )\nL L\ni i i j i i j\nL\ni i\nL a l   \n \n \n\n    \n \n \n (2)\nThe solution is obtained by\n( , ) 0L a   (3.1) ( , ) 0L a   (3.2)\nFrom Equation (3.1), we get\n2\n, 1 , 1\n2\n0\n1\nL L\ni i i j i j i j\ni i\nl\nL l\n   \n\n \n    \n  \n\n  (4)\nFrom Equation (3.2), we get\n1\n1 L\ni i    (5) By taking Equation (4) into Equation (5), the solution can be\nachieved by 2 1\n2 1\n1\n( )\n( )\ni i L\nii\nL l\nL l \n \n \n \n (6)\nwhen ∇\uD835\uDC59 ∈ [0,1) , \uD835\uDF14∗ is non-negative and satisfy the nonnegative constrain in Equation (1)."},{"heading":"ACKNOWLEDGMENTS","text":"e authors would like to thank Alibaba PAI team for providing experimental environments and their constructive comments. is work was partially supported by the China Postdoctoral Science Foundation (No. 2019M652038). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}],"references":[{"title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering","author":["Peter Anderson","Xiaodong He","Chris Buehler","Damien Teney","Mark Johnson","Stephen Gould","Lei Zhang"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"1","shortCiteRegEx":"1","year":2018},{"title":"VQA: Visual Question Answering","author":["Stanislaw Antol","Aishwarya Agrawal","Jiasen Lu","Margaret Mitchell","Dhruv Batra","C. Lawrence Zitnick","Devi Parikh"],"venue":"In Proceedings of International Conference on Computer Vision","citeRegEx":"2","shortCiteRegEx":"2","year":2015},{"title":"Convex Optimization","author":["Stephen Boyd","Lieven Vandenberghe"],"venue":null,"citeRegEx":"3","shortCiteRegEx":"3","year":2004},{"title":"Describing clothing by semantic attributes","author":["Huizhong Chen","Andrew Gallagher","Bernd Girod"],"venue":"In Proceedings of European Conference on Computer Vision,","citeRegEx":"4","shortCiteRegEx":"4","year":2012},{"title":"AdaBERT: Task- Adaptive BERT Compression with Differentiable Neural Architecture Search","author":["Daoyuan Chen","Yaliang Li","Minghui Qiu","Zhen Wang","Bofang Li","Bolin Ding","Hongbo Deng","Jun Huang","Wei Lin","Jingren Zhou"],"venue":"arXiv preprint. arXiv:2001.04246","citeRegEx":"5","shortCiteRegEx":"5","year":2001},{"title":"Pre-Training with Whole Word Masking for Chinese BERT","author":["Yiming Cui","Wanxiang Che","Ting Liu","Bing Qin","Ziqing Yang","Shijin Wang","Guoping Hu"],"venue":"arXiv. preprint arXiv:1906.08101","citeRegEx":"6","shortCiteRegEx":"6","year":2019},{"title":"ImageNet: A large-scale hierarchical image database","author":["Jia Deng","Wei Dong","Richard Socher","Li-Jia Li","Kai Li","Li Fei-Fei"],"venue":"IEEE Conference on Computer Vision and Pattern Recognition","citeRegEx":"7","shortCiteRegEx":"7","year":2009},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"venue":"arXiv. preprint arXiv:1810.04805","citeRegEx":"8","shortCiteRegEx":"8","year":2018},{"title":"DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition","author":["Jeff Donahue","Yangqing Jia","Oriol Vinyals","Judy Hoffman","Ning Zhang","Eric Tzeng","Trevor Darrell"],"venue":null,"citeRegEx":"9","shortCiteRegEx":"9","year":2013},{"title":"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives","author":["Fartash Faghri","David J. Fleet","Jamie Ryan Kiros","Sanja Fidler"],"venue":"arXiv preprint arXiv:1707.05612. Industry (SIRIP) Papers I SIGIR","citeRegEx":"10","shortCiteRegEx":"10","year":2017},{"title":"DeViSE: A Deep Visual- Semantic Embedding Model","author":["Andrea Frome","Greg S Corrado","Jon Shlens","Samy Bengio","Jeff Dean","Marc’Aurelio Ranzato","Tomas Mikolov"],"venue":"In Proceedings of Advances in Neural Information Processing Systems","citeRegEx":"11","shortCiteRegEx":"11","year":2013},{"title":"Rich feature hierarchies for accurate object detection and semantic segmentation","author":["Ross Girshick","Jeff Donahue","Trevor Darrell","Jitendra Malik"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"12","shortCiteRegEx":"12","year":2014},{"title":"Canonical Correlation Analysis: An overview with Application to Learning Methods","author":["David R. Hardoon","Sandor Szedmak","John Shawe-Taylor"],"venue":"Neural Computation","citeRegEx":"14","shortCiteRegEx":"14","year":2004},{"title":"Product Quantization for Nearest Neighbor Search","author":["Herve Jégou","Matthijs Douze","Cordelia Schmid"],"venue":"In IEEE Transactions on Pattern Analysis and Machine Intelligence,","citeRegEx":"15","shortCiteRegEx":"15","year":2011},{"title":"TinyBERT: Distilling BERT for Natural Language Understanding","author":["Xiaoqi Jiao","Yichun Yin","Lifeng Shang","Xin Jiang","Xiao Chen","Linlin Li","Fang Wang","Qun Liu"],"venue":"arXiv preprint arXiv:1909.10351","citeRegEx":"16","shortCiteRegEx":"16","year":2019},{"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","author":["Zhenzhong Lan","Mingda Chen","Sebastian Goodman","Kevin Gimpel","Piyush Sharma","Radu Soricut"],"venue":"arXiv preprint arXiv:1909.11942","citeRegEx":"17","shortCiteRegEx":"17","year":1942},{"title":"Stacked Cross Attention for Image-Text Matching","author":["Kuang-Huei Lee","Xi Chen","Gang Hua","Houdong Hu","Xiaodong He"],"venue":"In Proceedings of European Conference on Computer Vision","citeRegEx":"18","shortCiteRegEx":"18","year":2018},{"title":"Unicoder-VL: A Universal Encoder for Vision and Language by Crossmodal Pre-training","author":["Gen Li","Nan Duan","Yuejian Fang","Ming Gong","Daxin Jiang","Ming Zhou"],"venue":"In Proceedings of Association for the Advancement of Artificial Intelligence","citeRegEx":"19","shortCiteRegEx":"19","year":2019},{"title":"Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set","author":["Si Liu","Zheng Song","Guangcan Liu","Changsheng Xu","Hanqing Lu","Shuicheng Yan"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"20","shortCiteRegEx":"20","year":2012},{"title":"Deepfashion: Powering robust clothes recognition and retrieval with rich annotations","author":["Ziwei Liu","Ping Luo","Shi Qiu","Xiaogang Wang","Xiaoou Tang"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"21","shortCiteRegEx":"21","year":2016},{"title":"Fully Convolutional Networks for Semantic Segmentation","author":["Jonathan Long","Evan Shelhamer","Trevor Darrell"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"22","shortCiteRegEx":"22","year":2015},{"title":"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265","author":["Jiasen Lu","Dhruv Batra","Devi Parikh","Stefan Lee"],"venue":null,"citeRegEx":"23","shortCiteRegEx":"23","year":2019},{"title":"Hipster wars: Discovering elements of fashion styles","author":["M Hadi Kiapour","Kota Yamaguchi","Alexander C Berg","Tamara L Berg"],"venue":"In Proceedings of European Conference on Computer","citeRegEx":"24","shortCiteRegEx":"24","year":2014},{"title":"Discriminative Learning and Recognition of Image Set Classes Using Canonical Correlations","author":["Tae-Kyun Kim","Josef Kitter","Roberto Cipolla"],"venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence,","citeRegEx":"25","shortCiteRegEx":"25","year":2007},{"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations","author":["Ranjay Krishna","Yuke Zhu","Oliver Groth","Justin Johnson","Kenji Hata","Joshua Kravitz","Stephanie Chen","Yannis Kalantidis","Li-Jia Li","David A. Shamma","Michael S. Bernstein","Fei-Fei Li"],"venue":"arXiv preprint arXiv:1602.07332","citeRegEx":"27","shortCiteRegEx":"27","year":2016},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","author":["Alex Krizhevsky","Ilya Sutskever","Geoffrey E. Hinton"],"venue":"In Proceedings of Annual Conference on Neural Information Processing Systems","citeRegEx":"28","shortCiteRegEx":"28","year":2012},{"title":"Shuffle and learn: unsupervised learning using temporal order verification","author":["Ishan Misra","C Lawrence Zitnick","Martial Hebert"],"venue":"In Proceedings of European Conference on Computer Vision,","citeRegEx":"29","shortCiteRegEx":"29","year":2016},{"title":"Glove: Global vectors for word representation","author":["Jeffrey Pennington","Richard Socher","Christopher Manning"],"venue":"In Proceedings of conference on Empirical Methods in Natural Language Processing","citeRegEx":"30","shortCiteRegEx":"30","year":2014},{"title":"Improving language understanding by generative pre-training","author":["Alec Radford","Karthik Narasimhan","Tim Salimans","Ilya Sutskever"],"venue":null,"citeRegEx":"31","shortCiteRegEx":"31","year":2018},{"title":"Deep canonical correlation analysis with progressive and hypergraph learning for cross-modal retrieval","author":["Jie Shao","Leiquan Wang","Zhicheng Zhao Fei Su","Anni Cai"],"venue":null,"citeRegEx":"32","shortCiteRegEx":"32","year":2016},{"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition","author":["Karen Simonyan","Andrew Zisserman"],"venue":null,"citeRegEx":"33","shortCiteRegEx":"33","year":2014},{"title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations","author":["Weijie Su","Xizhou Zhu","Yue Cao","Bin Li","Lewei Lu","Furu Wei","Jifeng Dai"],"venue":"arXiv. preprint arXiv:1908.08530","citeRegEx":"34","shortCiteRegEx":"34","year":1908},{"title":"Videobert: A joint model for video and language representation learning","author":["Chen Sun","Austin Myers","Carl Vondrick","Kevin Murphy","Cordelia Schmid"],"venue":"arXiv preprint arXiv:1904.01766","citeRegEx":"35","shortCiteRegEx":"35","year":2019},{"title":"Rethinking the Inception Architecture for Computer Vision. arXiv preprint arXiv:1512.00567","author":["Christian Szegedy","Vincent Vanhoucke","Sergey Ioffe","Jonathon Shlens","Zbigniew Wojna"],"venue":null,"citeRegEx":"36","shortCiteRegEx":"36","year":2016},{"title":"Efficient object category recognition using classemes","author":["Lorenzo Torresani","Martin Szummer","Andrew Fitzgibbon"],"venue":null,"citeRegEx":"37","shortCiteRegEx":"37","year":2010},{"title":"Selfie: Selfsupervised Pretraining for Image Embedding","author":["Trieu H. Trinh","Minh-Thang Luong","Quoc V. Le"],"venue":"arXiv preprint arXiv:1906.02940","citeRegEx":"38","shortCiteRegEx":"38","year":2019},{"title":"Composing Text and Image for Image Retrieval – An Empirical Odyssey","author":["Nam Vo","Lu Jiang","Chen Sun","Kevin Murphy","Li-Jia Li","Li Fei-Fei","James Hays"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"40","shortCiteRegEx":"40","year":2019},{"title":"Position Focused Attention Network for Image-Text Matching","author":["Yaxiong Wang","Hao Yang","Xueming Qian","Lin Ma","Jing Lu","Biao Li","Xin Fan"],"venue":"In Proceedings of International Joint Conference on Artificial Intelligence","citeRegEx":"41","shortCiteRegEx":"41","year":2019},{"title":"Google’s neural machine translation system: Bridging the gap between human and machine translation","author":["Yonghui Wu","Mike Schuster","Zhifeng Chen","Quoc V Le","Mohammad Norouzi"],"venue":"arXiv preprint arXiv:1609.08144","citeRegEx":"42","shortCiteRegEx":"42","year":2016},{"title":"Aggregated Residual Transformations for Deep Neural Networks. arXiv preprint arXiv:1611.05431","author":["Saining Xie","Ross Girshick","Piotr Dollár","Zhuowen Tu","Kaiming He"],"venue":null,"citeRegEx":"43","shortCiteRegEx":"43","year":2016},{"title":"Deep Correlation for Matching Images and Text","author":["Fei Yan","Krystian Mikolajczyk"],"venue":"In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition","citeRegEx":"44","shortCiteRegEx":"44","year":2015},{"title":"Efficient Indexing of Billion-Scale Datasets of Deep Descriptors","author":["Artem Babenko Yandex","Victor Lempitsky"],"venue":"In IEEE Conference on Computer Vision and Pattern Recognition","citeRegEx":"45","shortCiteRegEx":"45","year":2016}],"referenceMentions":[{"referenceID":36,"context":"The pivot of cross-modal retrieval is to learn a meaningful cross-modal matching [40].","startOffset":81,"endOffset":85},{"referenceID":12,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":234,"endOffset":246},{"referenceID":23,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":234,"endOffset":246},{"referenceID":40,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":234,"endOffset":246},{"referenceID":9,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":275,"endOffset":291},{"referenceID":10,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":275,"endOffset":291},{"referenceID":16,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":275,"endOffset":291},{"referenceID":37,"context":"The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].","startOffset":275,"endOffset":291},{"referenceID":0,"context":"Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].","startOffset":96,"endOffset":102},{"referenceID":1,"context":"Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].","startOffset":96,"endOffset":102},{"referenceID":7,"context":"Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].","startOffset":140,"endOffset":147},{"referenceID":17,"context":"Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34].","startOffset":145,"endOffset":153},{"referenceID":31,"context":"Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34].","startOffset":145,"endOffset":153},{"referenceID":36,"context":"The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":121,"endOffset":125},{"referenceID":0,"context":"The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":144,"endOffset":147},{"referenceID":1,"context":"The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":178,"endOffset":181},{"referenceID":35,"context":"Inspired by the selfie idea [38], we first introduce the patch method to extract image tokens.","startOffset":28,"endOffset":32},{"referenceID":7,"context":"The BERT model introduced by [8] is an attention-based bidirectional language model.","startOffset":29,"endOffset":32},{"referenceID":7,"context":"Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42].","startOffset":32,"endOffset":35},{"referenceID":38,"context":"Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42].","startOffset":117,"endOffset":121},{"referenceID":33,"context":", InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network.","startOffset":14,"endOffset":18},{"referenceID":39,"context":", InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network.","startOffset":35,"endOffset":39},{"referenceID":5,"context":"We apply the Whole Word Masking (WWM) strategy to mask out all the text tokens corresponding to a word at once [6].","startOffset":111,"endOffset":114},{"referenceID":2,"context":"Considering the KKT conditions (Karush-Kuhn-Tucher Conditions) [3], we can obtain","startOffset":63,"endOffset":66},{"referenceID":3,"context":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.","startOffset":56,"endOffset":75},{"referenceID":18,"context":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.","startOffset":56,"endOffset":75},{"referenceID":19,"context":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.","startOffset":56,"endOffset":75},{"referenceID":22,"context":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.","startOffset":56,"endOffset":75},{"referenceID":34,"context":"Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.","startOffset":56,"endOffset":75},{"referenceID":21,"context":"Same with [23], we score each description and image pair in the test dataset and then sort the candidate rank set.","startOffset":10,"endOffset":14},{"referenceID":39,"context":"For each patch, ResNeXt101 [43] is first adopted to extract the patch features and the dimensions of patch features are 2048.","startOffset":27,"endOffset":31},{"referenceID":10,"context":"VSE [11]: VSE directly projects image features and text features into visual semantic embedding space in an end-toend manner, which is regarded as our first baseline approach.","startOffset":4,"endOffset":8},{"referenceID":9,"context":"VSE++ [10]: VSE++ extends the VSE approach with modification of the rank-based loss function which pays more attention to the hard negatives.","startOffset":6,"endOffset":10},{"referenceID":16,"context":"SCAN [18]: SCAN performs the cross-modal match on the image RoIs and text tokens.","startOffset":5,"endOffset":9},{"referenceID":37,"context":"PFAN [41]: The position information of each RoI is incorporated in the PFAN model.","startOffset":5,"endOffset":9},{"referenceID":21,"context":"We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34].","startOffset":71,"endOffset":75},{"referenceID":31,"context":"We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34].","startOffset":88,"endOffset":92},{"referenceID":21,"context":"ViLBERT-ZeroShot [23]: In ViLBERT, the authors fine-tune and evaluate ViLBERT with cross-modal retrieval.","startOffset":17,"endOffset":21},{"referenceID":24,"context":"For each image, we keep 10 to 36 high-scoring regions with Faster R-CNN [13] pre-trained on the Visual Genome dataset [27].","startOffset":118,"endOffset":122},{"referenceID":31,"context":"VLBERT-Finetune [34]: The pre-trained VLBERT model is not evaluated with cross-modal retrieval.","startOffset":16,"endOffset":20},{"referenceID":13,"context":"The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform retrieval and ranking with nearest-neighbor search [15, 45].","startOffset":213,"endOffset":221},{"referenceID":41,"context":"The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform retrieval and ranking with nearest-neighbor search [15, 45].","startOffset":213,"endOffset":221},{"referenceID":31,"context":"Like the downstream task of Visual Question Answer (VQA) in VLBERT [34], “Q”, “T” and “I” are utilized in the segmentation to distinguish the three input types of “Query”, “Title” and “Image” in fine-tuning.","startOffset":67,"endOffset":71},{"referenceID":7,"context":"In pre-training, we adopt the original pre-trained BERT model [8].","startOffset":62,"endOffset":65},{"referenceID":14,"context":"More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].","startOffset":96,"endOffset":100},{"referenceID":15,"context":"More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].","startOffset":109,"endOffset":113},{"referenceID":4,"context":"More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].","startOffset":123,"endOffset":126},{"referenceID":25,"context":"propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7].","startOffset":24,"endOffset":28},{"referenceID":6,"context":"propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7].","startOffset":98,"endOffset":101},{"referenceID":8,"context":"Later on, the researchers found that these CNN blocks in AlexNet pretrained on ImageNet or the other large-scale image corpus can be treated as general feature extractors and perform well in various of downstream tasks [9].","startOffset":219,"endOffset":222},{"referenceID":30,"context":"Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].","startOffset":118,"endOffset":122},{"referenceID":33,"context":"Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].","startOffset":141,"endOffset":145},{"referenceID":39,"context":"Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].","startOffset":154,"endOffset":158},{"referenceID":11,"context":"These pre-trained models are widely adopted in the CV tasks, like object detection [12] and semantic segmentation [22].","startOffset":83,"endOffset":87},{"referenceID":20,"context":"These pre-trained models are widely adopted in the CV tasks, like object detection [12] and semantic segmentation [22].","startOffset":114,"endOffset":118},{"referenceID":27,"context":"The early studies on pre-training in NLP can be traced back to word embedding, such as word2vec, GloVe [30].","startOffset":103,"endOffset":107},{"referenceID":28,"context":"After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks.","startOffset":72,"endOffset":76},{"referenceID":7,"context":"After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks.","startOffset":83,"endOffset":86},{"referenceID":36,"context":"These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":119,"endOffset":123},{"referenceID":0,"context":"These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":142,"endOffset":145},{"referenceID":1,"context":"These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].","startOffset":176,"endOffset":179},{"referenceID":12,"context":", overview the applications of Canonical Correlation Analysis (CCA), where CCA is introduced to project text and image features into a shared vector space by maximizing the cross relation [14].","startOffset":188,"endOffset":192},{"referenceID":23,"context":"Researchers then propose variants of CCA-based approaches [25, 32, 44].","startOffset":58,"endOffset":70},{"referenceID":29,"context":"Researchers then propose variants of CCA-based approaches [25, 32, 44].","startOffset":58,"endOffset":70},{"referenceID":40,"context":"Researchers then propose variants of CCA-based approaches [25, 32, 44].","startOffset":58,"endOffset":70},{"referenceID":10,"context":", learns semantic relationships between labels and explicitly maps images into the semantic embedding space [11].","startOffset":108,"endOffset":112},{"referenceID":9,"context":", extend VSE and propose VSE++ by introducing the hard-negative mining technique [10].","startOffset":81,"endOffset":85},{"referenceID":16,"context":"Meanwhile, the attention mechanism is leveraged to enhance matching ability [18].","startOffset":76,"endOffset":80},{"referenceID":37,"context":"The position information of each RoI is incorporated in the PFAN model [41].","startOffset":71,"endOffset":75},{"referenceID":26,"context":"Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks.","startOffset":72,"endOffset":80},{"referenceID":39,"context":"Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks.","startOffset":72,"endOffset":80},{"referenceID":32,"context":"VideoBERT [35] lever-ages off-the-shelf networks for action recognition from video clips.","startOffset":10,"endOffset":14},{"referenceID":21,"context":"ViLBERT [23] focuses on the representation learning of the general domain.","startOffset":8,"endOffset":12},{"referenceID":17,"context":"Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.","startOffset":12,"endOffset":16},{"referenceID":31,"context":"Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.","startOffset":29,"endOffset":33}],"year":2020,"abstractText":"In this paper, we address the text and image matching in crossmodal retrieval of the fashion industry. Different from the matching in the general domain, the fashion matching is required to pay much more aention to the fine-grained information in the fashion images and texts. Pioneer approaches detect the region of interests (i.e., RoIs) from images and use the RoI embeddings as image representations. In general, RoIs tend to represent the “object-level” information in the fashion images, while fashion texts are prone to describe more detailed information, e.g. styles, aributes. RoIs are thus not fine-grained enough for fashion text and image matching. To this end, we propose FashionBERT, which leverages patches as image features. With the pre-trained BERT model as the backbone network, FashionBERT learns high level representations of texts and images. Meanwhile, we propose an adaptive loss to trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text and image matching and cross-modal retrieval) are incorporated to evaluate FashionBERT. On the public dataset, experiments demonstrate FashionBERT achieves significant improvements in performances than the baseline and state-ofthe-art approaches. In practice, FashionBERT is applied in a concrete cross-modal retrieval application. We provide the detailed matching performance and inference efficiency analysis.","creator":null}}
{"name":"3397271.3401266.pdf","metadata":{"source":"META","title":"Reranking for Efficient Transformer-based Answer Selection","authors":["Yoshitomo Matsubara","Thuy Vu","Alessandro Moschitti"],"emails":["yoshitom@uci.edu","thuyvu@amazon.com","amosch@amazon.com"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems→Retrievalmodels and ranking;Question answering.\nKEYWORDS QuestionAnswering, TransformerModels, Neural Networks, Reranking, Information Retrieval, Natural Language Processing\nACM Reference Format: Yoshitomo Matsubara, Thuy Vu, and Alessandro Moschitti. 2020. Reranking for Efficient Transformer-based Answer Selection. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China.ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401266"},{"heading":"1 INTRODUCTION","text":"QA research has received a renewed attention in recent years thanks to advent of virtual assistants in industrial sectors. For example, Alexa, Google Home, and Siri provide general information inquiry services. One key task for QA, is the Answer Sentence Selection (AS2), which has been widely studied by the TREC challenges. AS2, given a question and a set of answer sentence candidates, consists in selecting sentences (e.g., retrieved by a search engine) that correctly answer the question. Neural models have significantly contributed with new techniques, e.g., [8, 11] to AS2. More recently, neural language models, e.g., ELMO [13], GPT [14], BERT [5], RoBERTa [10],\n∗This work was done while the author was an intern at Amazon Alexa.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401266\nXLNet [3] have led to major advancements in NLP. These methods capture dependencies between words and their compounds by pre-training neural networks on large amounts of data. Unfortunately, the Transformer-based architectures use a huge number of parameters (e.g., 340 million for BERT Large). This poses important challenges for their deployment in production: first, the latency of the approach highly increases with the number of candidates. For instance, to target the required Recall, it might be necessary to classify of hundreds of candidates, which can take several seconds even when using powerful GPUs. Secondly, although the classification of candidates can be scaled horizontally, the number of GPUs required to fulfill a target transaction-per-second will prohibitively increase the operational cost. Finally, their high energy consumption is an environmental threat, as pointed out by [18] and the NeurIPS workshop, Tackling Climate Change with ML.\nIn this paper, we study and propose solutions to improve the efficiency and cost of modern QA systems based on search engines and Transformer models. Though we mainly focus on AS2, the proposed solution is general, and can be applied to other QA paradigms, including machine reading tasks. Our main idea follows the successful cascade approach for ad-hoc document retrieval [19], which considers fast but less accurate rerankers together with more accurate but slower models. In particular, we use (i) simple models, e.g., Jaccard similarity, as well as light neural models such as CompareAggregate [22], for reranking answer sentence candidates; and (ii) BERT models as our final AS2 step.\nTo carry out our experiments, we created three different AS2 datasets, all including a large number of answer sentence candidates. Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended. We tested the combinations of fast rerankers, Jaccard similarity, Rel-CNN1 and CA with an accurate BERT selector, and compared them with the upper bound, obtained with the expensive approach of classifying all candidates with the BERT model. The key finding of our paper is to show that the inference cost of Transformer models, e.g., BERT, can be reduced by selecting only a promising candidate subset to be processed, still preserving the original Accuracy. That is, the candidates selected by shallow neural rerankers are compatible with those selected by Transformer models. This enables the design of accurate, fast and cost-effective QA systems, based on sequential rerankers."},{"heading":"2 RELATEDWORK","text":"Neural models for AS2 typically apply a series of non-linear transformations to the input question and answer, represented as compositions of word or character embeddings and then measure the 1For lack of space we do not report its result, but it is between Jaccard and CA models.\nsimilarity between the obtained representations. For example, the Rel-CNN [16] has two separate embedding layers for the question and answer, and relational embedding, which aims at connecting them. Recent work has shown that Transformer-based models, e.g., BERT [5], can highly improve inference. Yang et al. [23] applied it to Ad Hoc Document Retrieval, obtaining significant improvement. Garg et al. [8] fine-tuned BERT for AS2, achieving the state of the art. However, BERT’s high computational cost prevents its use in most real-word applications. Some solutions rely on leveraging knowledge distillation in the pre-training step, e.g., [15]. In contrast, we propose an alternative (and compatible with the initiatives above) approach following previous work in document retrieval, e.g., the use of sequential rerankers [19]. Wang et al. [21] focused on quickly identifying a set of good candidate documents to be passed to the second and further rerankers of the cascade. Dang et al. [4] proposed two stage approaches using a limited set of textual features and a final model trained using a larger set of queryand document-dependent features. Gallagher et al. [7] presented a new general framework for learning an end-to-end cascade of rankers using backpropagation. Asadi and Lin [1] studied effectiveness/efficiency trade-offs with three candidate selection approaches. All the methods above are in line with our study, but they target very different settings: document retrieval, linear models or just basic neural models. In contrast, the main contribution of our paper is to show that simple and efficient neural sentence rerankers are compatible with expensive Transformer models, enabling their efficient use."},{"heading":"3 EFFICIENT PASSAGE RERANKING","text":"In this section, we introduce the most famous neural models for AS2, as well as a state-of-the-art model based on BERT, and then, we present sequential architecture based on fast neural models and more expensive (but more accurate) models."},{"heading":"3.1 Neural rerankers","text":"A reranker can be simply designed as a model for estimating the probability of correctness of a question and answer sentence pair, \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60). Given a set of questions, \uD835\uDC44 , and a set of sentences, \uD835\uDC46 , we define a reranker as \uD835\uDC45 : \uD835\uDC44 × P(\uD835\uDC46) → P(\uD835\uDC46), which takes a subset Σ of the sentences in \uD835\uDC46 , and returns a subset of size \uD835\uDC58 < |Σ|, i.e., \uD835\uDC45(\uD835\uDC5E, Σ) = (\uD835\uDC60\uD835\uDC561, ..., \uD835\uDC60\uD835\uDC56\uD835\uDC58 ), where \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56ℎ) > \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56\uD835\uDC59 ), if ℎ < \uD835\uDC59 . We use the Compare-Aggregate (CA) model [22], which implements an attention mechanism between the question and the answer candidate with two-step operations, i.e., comparison and aggregation.\nFinally, we study the efficient use of Transformer models, which are neural networks designed to capture dependencies between words, i.e., their interdependent context. The input is encoded in embeddings for tokens, segments and their positions. These are given in input to several blocks (up to 24), containing layers for multi-head attention, normalization and feed forward processing. The result of this transformation is an embedding, \uD835\uDC65 , representing a text pair thatmodels the dependencies betweenwords and segments\nof the two sentences. We train these models for AS2 by feeding \uD835\uDC65 to a fully connected layer, and fine-tuning the Transformer as shown by Devlin et al. [5]. For this step, we used question and sentence pairs labeled as positive or negative, whether or not the sentence correctly answers the question."},{"heading":"3.2 Sequential rerankers","text":"Given a sequence of rerankers sorted by their computational efficiency, (\uD835\uDC451, \uD835\uDC452,.., \uD835\uDC45\uD835\uDC41 ), we assume that the ranking Accuracy A (e.g., in terms of MAP and MRR), increases in reverse order of the efficiency, i.e., A(\uD835\uDC45 \uD835\uDC57 ) > A(\uD835\uDC45\uD835\uDC56 ) iff \uD835\uDC57 > \uD835\uDC56 .\nWe define a Sequential Reranker of order \uD835\uDC5B, SR\uD835\uDC5B , as the composition of \uD835\uDC5B rerankers: \uD835\uDF0C (\uD835\uDC46) = \uD835\uDC45\uD835\uDC41 ◦ \uD835\uDC45\uD835\uDC41−1 ◦ .. ◦ \uD835\uDC451 (\uD835\uDC46). Each \uD835\uDC45\uD835\uDC56 is associated with a different \uD835\uDC58\uD835\uDC56 = |\uD835\uDC45\uD835\uDC56 (·) |, i.e., the number of sentences the reranker returns (see Fig. 1). Setting different values for \uD835\uDC58\uD835\uDC56 can produce many valuable SRs with different trade-off between Accuracy and efficiency. The design of an end-to-end neural architecture that learns the optimal parameter set for the target trade-off is an interesting future research. In this paper, we focus on a simple SR2 constituted by just two rerankers, SR2 = \uD835\uDC452 ◦ \uD835\uDC451 (\uD835\uDC46), where the first ranker is either a Jaccard similarity-based model or neural rankers such as CA, whereas the second reranker (selector) is the more expensive BERT model, i.e., Base or Large versions."},{"heading":"4 EXPERIMENTS","text":"We show that fast rerankers can effectively feed more expensive Transformer-based selectors, enabling the trade off between Accuracy and efficiency. We compare with state-of-the-art AS2 baselines on two different test sets, created with different question types from Alexa Virtual Assistant traffic. We also built an academic dataset to enable model replicability and comparison with future work."},{"heading":"4.1 Experimental setup","text":"4.1.1 Metrics. The performance for QA systems is typically measured with Accuracy in providing correct answers, i.e., the percentage of correct responses. Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers. We also measure the system latency using the average endto-end processing time for each question, including preprocessing, loading files, etc.2.\n4.1.2 Alexa Datasets. We built training and test sets using questions sampled from Alexa traffic (see statistics in Table 1). We only considered information inquiry questions. These ask about general knowledge, ranging from “Who is the president of United States?” to “How did Los Angeles Lakers score in the match yesterday?”. We generate candidates by selecting sentences from the documents retrieved using an elastic search system, where the referring index\n2Measured on an EC2-instance p3.2xlarge machine (8 Intel Xeon Scalable (Skylake) vCPUs, 62GB RAM, 1 GPU: NVIDIA Tesla V100)\nis ingested with several web domains, ranging from Wikipedia to reference.com, coolantarctica.com, www.cia.gov/library and so on.\nGeneral Purpose Dataset (GPD):We used the following strategy to retrieve correct candidates with high confidence: (i) retrieve top 100 documents; (ii) automatically extract the top 100 sentences ranked by a BERT model over all sentences of the documents; and (iii) manually annotate the top 100 sentences as correct or incorrect answers. This process does not guarantee that we have all correct answers but the chance to miss them is much smaller than for other datasets. Indeed, Table 1 shows an average of seven correct answers for each question. As some of the correct answers can be missed, we consider the GPD evaluation as a strict lower bound (SLB).\nSentence-Based Training Set (SBTS): A training set requires labels for all answer candidates. We limited the rank to just the top 10 sentence per question, to save annotation costs. SBTS consists of 25, 226 questions for a total of 134, 765 candidates, where 125, 779 were labelled as positive and 8, 986 as negative examples.\n4.1.3 Academic Benchmark dataset. Answer Sentence Natural Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario. Thus, we used ASNQ3 by Garg et al. [8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task. ASNQ contains 57,242 and 2,672 distinct questions in the training and test sets, respectively: this is an order of magnitude larger than most public AS2 datasets. In our study, we only use it for testing. Finally, as the number of candidates is still lower than what can typically appear in an industrial scenario, we created ASNQ++ by simply adding negative candidates associated with different questions to reach 1,300 sentences for each question. Table 1 shows some statistics.\n4.1.4 Models. We used the following models: Jaccard Similarity, CA, BERT-Base and BERT-Large to estimate \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56 ) and used SBTS for training them (except for Jaccard similarity, which is an unsupervised approach). BERT-based models use SBTS for fine-tuning. We trained all the models using the log cross-entropy loss function: L = −∑\uD835\uDC59 ∈{0,1} \uD835\uDC66\uD835\uDC59 × \uD835\uDC59\uD835\uDC5C\uD835\uDC54(\uD835\uDC66\uD835\uDC59 ) on pairs of text. For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, \uD835\uDC59\uD835\uDC5F = 0.0001, and (iii) the batch size of 32. Additionally, we use early stopping to monitor MAP on the validation set. We set the hidden dimensions of the biLSTM in CA to 300. As for BERT, we tuned the learning rate in the interval [1\uD835\uDC52-6, 2\uD835\uDC52-5].\n3ASNQ and ASNQ++ are available at https://github.com/alexa/wqa_tanda\nTable 3: Performance of multi-reranker approach using BERT-Base as the selector derived on GPD\nReranker Jaccard CA BERT #Select. sent. 100 200 250 300 100 all Latency (sec/q) 0.564 0.840 0.977 1.110 0.962 4.590 Prec.@1 0.270 0.376 0.382 0.411 0.450 0.476\nFigure 2: Accuracy of SR using BERT-Large and \uD835\uDC58 sentences selected by the first reranker on ASNQ."},{"heading":"4.2 Results","text":"We first assess our models on public datasets, then test the individual models as well as the combined models in terms of reranking performance on different datasets. In particular, we compute a SLB and the exact measures on Alexa and ASNQ datasets, respectively.\n4.2.1 Model Assessment. As the datasets for testing the efficiency of the rerankers are new or partially new, to assess where our systems collocate in previous work, we compare our models with the best AS2 systems on standard datasets, WikiQA and TRECQA. Table 2 shows the results derived according to the standard clean setting on the official test sets. We can see that our CA implementation performs slightly lower than models from previous work. This depends on the fact that there is a large variation of the results according to different random seeds. In contrast, our BERT implementation is aligned to the state of the art, thus our comparison using Sequential Rerankers (SRs) is very meaningful.\n4.2.2 Evaluation of SR on GPD. We tested SRs constituted by different models on GPD. Table 3 reports the Accuracy, where: (i) the first row indicates the approach for reranking, either Jaccard or CA, (ii) the second row shows several values for the number of sentences selected by Jaccard, while for CA, we selected a competitive number, i.e., 100, (iii) the third row shows the latency of the SR models including, reranker time and selector applied to the \uD835\uDC58 candidates; and finally, (iv) the last row shows the Precision@1 (i.e., the SLB Accuracy) of SR. The BERT column just shows the upper bound Accuracy, i.e., applying BERT to all candidates.\nWe note that: (i) the latency of the Jaccard-based SR is better than the SR based on CA, when we consider the same number of candidates, e.g., 100, since CA execution time is not negligible, while Jaccard latency is tiny; (ii) the Accuracy of the Jaccard-based SR is half the one based on CA; (iii) also when increasing the number of candidates, the Jaccard-based SR cannot catch up with the Accuracy of CA-based SR using 100 candidates, whereas the latency quickly increases; (iv) the BERT upper bound is just 2.6% better, but the latency is 4.5 seconds, which is prohibitively high; and (v) we did\nnot optimize CA in terms of efficiency and a better trade off to select a more appropriate \uD835\uDC58 can be found. 4.2.3 Results on ASNQ. The evaluation on ASNQ is in principle more accurate as all candidates have been annotated4. Figure 2 shows the plot of three models: the blue line is the BERT-Large (Accuracy when it is applied to all sentence candidates), the green curve is the SR Accuracy when using \uD835\uDC58 sentences from CA, and the orange curve is the Accuracy of SR-based on Jaccard similarity. Again, CA highly outperforms the Jaccard-based model. Interestingly, using 100 sentences or fewer allows for reaching the upper bound. Due to limited space, we cannot show a similar plot for BERT-Base, which reflects the same behaviour of BERT-Large (but with lower Accuracy).\nTo better show the potential of the SR approach, we report a configuration with 100 candidates selected by the first reranker in Table 4. We note that (i) SR based on Jaccard is almost 10 points below the upper bound (all candidates are used), for both selectors, BERT Base and Large; (ii) in contrast, SR based on CA is less than 0.3% below the upper bound. (iii) BERT-Large-based models are around three points better than those based on BERT-Base, as expected. (iv) Finally, there is a clear advantage in terms of latency, but the average number of candidates of ASNQ is lower than in a real setting, thus the speedup is just about 2-3 times for BERT-Base and 7-8 times for BERT-Large.\nTo better appreciate the gain in latency, we tested our models on ASNQ++ dataset, on which we added a larger number of negative examples. Table 5 shows that the latency of BERT-Base increases to 4.23 seconds, while the execution time of SR based on CA is still under 1 second. It is interesting to note that the Accuracy of the upper bound and SR models decreases comparing to the one obtained on ASNQ, e.g., BERT-based obtains 0.571 vs. 0.536 on ASNQ and ASNQ++, respectively. However, SR based on CA still reaches the target upper bound. Although, ASNQ++ is partially artificial, it provides results inline with GPD."},{"heading":"5 CONCLUSIONS","text":"Very recent research work has shown that powerful neural models such as Transformer models can highly improve retrieval Accuracy. In particular, we show (confirming the results from ad hoc retrieval and our previous work [8]) that BERT can improve passage/sentence reranking in a QA setting. Unfortunately, in a realworld scenario requiring the processing of hundreds or thousands of candidates, this also leads to a latency of 4-5 seconds or more per question, even using an entire GPU. To tackle this problem, we applied SR models by limiting our study to two types of classifiers fast models such as, Jaccard similarity and CA, and slower but more accurate models, i.e., the Transformer (BERT Base and Large). We showed two important findings: (i) Simple neural rerankers such as CA can select candidates containing correct answers BERT would 4Considering that annotation process is always subject to annotator agreement.\nselect. This is important as the data representation of the former is very different from the one of the latter. (ii) SR can be used to parameterize the trade off between efficiency and Accuracy, enabling the use of transformer models for real-world scenarios. Future ideas connected to our work are already in progress, e.g., [2, 17]."}],"references":[{"title":"Effectiveness/Efficiency Tradeoffs for Candidate Generation in Multi-stage Retrieval Architectures","author":["Nima Asadi","Jimmy Lin"],"venue":null,"citeRegEx":"1","shortCiteRegEx":"1","year":2013},{"title":"A Study on Efficiency, Accuracy and Document Structure for Answer Sentence Selection","author":["Daniele Bonadiman","Alessandro Moschitti"],"venue":"CoRR abs/2003.02349 (2020)","citeRegEx":"2","shortCiteRegEx":"2","year":2020},{"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context","author":["Zihang Dai","Zhilin Yang","Yiming Yang","Jaime G Carbonell","Quoc Le","Ruslan Salakhutdinov"],"venue":null,"citeRegEx":"3","shortCiteRegEx":"3","year":2019},{"title":"Two-Stage Learning to Rank for Information Retrieval","author":["Van Dang","Michael Bendersky","W. Bruce Croft"],"venue":"ECIR","citeRegEx":"4","shortCiteRegEx":"4","year":2013},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"venue":"In Proceedings of NAACL","citeRegEx":"5","shortCiteRegEx":"5","year":2019},{"title":"Natural Questions: A Benchmark for Question Answering Research","author":["Tom Kwiatkowski"],"venue":"TACL","citeRegEx":"6","shortCiteRegEx":"6","year":2019},{"title":"Joint Optimization of Cascade Ranking Models","author":["Luke Gallagher","Ruey-Cheng Chen","Roi Blanco","J. Shane Culpepper"],"venue":"InWSDM","citeRegEx":"7","shortCiteRegEx":"7","year":2019},{"title":"TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection","author":["Siddhant Garg","Thuy Vu","Alessandro Moschitti"],"venue":null,"citeRegEx":"8","shortCiteRegEx":"8","year":2019},{"title":"Adam: A Method for Stochastic Optimization","author":["Diederik P. Kingma","Jimmy Ba"],"venue":"CoRR abs/1412.6980","citeRegEx":"9","shortCiteRegEx":"9","year":2014},{"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","author":["Yinhan Liu","Myle Ott","Naman Goyal","Jingfei Du","Mandar Joshi","Danqi Chen","Omer Levy","Mike Lewis","Luke Zettlemoyer","Veselin Stoyanov"],"venue":"CoRR abs/1907.11692","citeRegEx":"10","shortCiteRegEx":"10","year":2019},{"title":"Passage Re-ranking with BERT","author":["Rodrigo Nogueira","Kyunghyun Cho"],"venue":"CoRR abs/1901.04085 (2019)","citeRegEx":"11","shortCiteRegEx":"11","year":2019},{"title":"Glove: Global vectors for word representation","author":["Jeffrey Pennington","Richard Socher","Christopher Manning"],"venue":"In EMNLP’14","citeRegEx":"12","shortCiteRegEx":"12","year":2014},{"title":"Language models are unsupervised multitask learners","author":["Alec Radford","Jeffrey Wu","Rewon Child","David Luan","Dario Amodei","Ilya Sutskever"],"venue":"OpenAI Blog 1,","citeRegEx":"14","shortCiteRegEx":"14","year":2019},{"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","author":["Victor Sanh","Lysandre Debut","Julien Chaumond","Thomas Wolf"],"venue":null,"citeRegEx":"15","shortCiteRegEx":"15","year":2019},{"title":"Learning to rank short text pairs with convolutional deep neural networks. In SIGIR’15","author":["Aliaksei Severyn","Alessandro Moschitti"],"venue":null,"citeRegEx":"16","shortCiteRegEx":"16","year":2015},{"title":"The Cascade Transformer: an Application for Efficient Answer Sentence Selection","author":["Luca Soldaini","Alessandro Moschitti"],"venue":"CoRR abs/2005.02534","citeRegEx":"17","shortCiteRegEx":"17","year":2020},{"title":"Energy and Policy Considerations for Deep Learning in NLP","author":["E. Strubell","A. Ganesh","A. McCallum"],"venue":"In ACL","citeRegEx":"18","shortCiteRegEx":"18","year":2019},{"title":"A Cascade Ranking Model for Efficient Ranked Retrieval","author":["Lidan Wang","Jimmy Lin","Donald Metzler"],"venue":"In SIGIR","citeRegEx":"19","shortCiteRegEx":"19","year":2011},{"title":"What is the Jeopardy model? A quasi-synchronous grammar for QA","author":["MengqiuWang","Noah A Smith","Teruko Mitamura"],"venue":"In EMNLP-CoNLL","citeRegEx":"20","shortCiteRegEx":"20","year":2007},{"title":"Fast First-Phase Candidate Generation for Cascading Rankers","author":["Qi Wang","Constantinos Dimopoulos","Torsten Suel"],"venue":"In SIGIR’16","citeRegEx":"21","shortCiteRegEx":"21","year":2016},{"title":"A compare-aggregate model for matching text sequences","author":["Shuohang Wang","Jing Jiang"],"venue":"In Fifth International Conference on Learning Representations","citeRegEx":"22","shortCiteRegEx":"22","year":2017},{"title":"Simple Applications of BERT for Ad Hoc Document Retrieval","author":["Wei Yang","Haotian Zhang","Jimmy Lin"],"venue":"CoRR abs/1903.10972 (2019)","citeRegEx":"23","shortCiteRegEx":"23","year":2019},{"title":"WikiQA: A Challenge Dataset for Open-Domain Question Answering","author":["Yi Yang","Wen-tau Yih","Christopher Meek"],"venue":null,"citeRegEx":"24","shortCiteRegEx":"24","year":2015},{"title":"A compare-aggregate model with latent clustering for answer selection","author":["Seunghyun Yoon","Franck Dernoncourt","Doo Soon Kim","Trung Bui","Kyomin Jung"],"venue":"In CIKM. Short Research Papers I SIGIR","citeRegEx":"25","shortCiteRegEx":"25","year":2019}],"referenceMentions":[{"referenceID":12,"context":", ELMO [13], GPT [14], BERT [5], RoBERTa [10],","startOffset":17,"endOffset":21},{"referenceID":4,"context":", ELMO [13], GPT [14], BERT [5], RoBERTa [10],","startOffset":28,"endOffset":31},{"referenceID":9,"context":", ELMO [13], GPT [14], BERT [5], RoBERTa [10],","startOffset":41,"endOffset":45},{"referenceID":2,"context":"3401266 XLNet [3] have led to major advancements in NLP.","startOffset":14,"endOffset":17},{"referenceID":16,"context":"Finally, their high energy consumption is an environmental threat, as pointed out by [18] and the NeurIPS workshop, Tackling Climate Change with ML.","startOffset":85,"endOffset":89},{"referenceID":17,"context":"Our main idea follows the successful cascade approach for ad-hoc document retrieval [19], which considers fast but less accurate rerankers together with more accurate but slower models.","startOffset":84,"endOffset":88},{"referenceID":20,"context":", Jaccard similarity, as well as light neural models such as CompareAggregate [22], for reranking answer sentence candidates; and (ii) BERT models as our final AS2 step.","startOffset":78,"endOffset":82},{"referenceID":7,"context":"Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended.","startOffset":127,"endOffset":130},{"referenceID":5,"context":"Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended.","startOffset":185,"endOffset":188},{"referenceID":14,"context":"For example, the Rel-CNN [16] has two separate embedding layers for the question and answer, and relational embedding, which aims at connecting them.","startOffset":25,"endOffset":29},{"referenceID":4,"context":", BERT [5], can highly improve inference.","startOffset":7,"endOffset":10},{"referenceID":21,"context":"[23] applied it to Ad Hoc Document Retrieval, obtaining significant improvement.","startOffset":0,"endOffset":4},{"referenceID":7,"context":"[8] fine-tuned BERT for AS2, achieving the state of the art.","startOffset":0,"endOffset":3},{"referenceID":17,"context":", the use of sequential rerankers [19].","startOffset":34,"endOffset":38},{"referenceID":19,"context":"[21] focused on quickly identifying a set of good candidate documents to be passed to the second and further rerankers of the cascade.","startOffset":0,"endOffset":4},{"referenceID":3,"context":"[4] proposed two stage approaches using a limited set of textual features and a final model trained using a larger set of queryand document-dependent features.","startOffset":0,"endOffset":3},{"referenceID":6,"context":"[7] presented a new general framework for learning an end-to-end cascade of rankers using backpropagation.","startOffset":0,"endOffset":3},{"referenceID":0,"context":"Asadi and Lin [1] studied effectiveness/efficiency trade-offs with three candidate selection approaches.","startOffset":14,"endOffset":17},{"referenceID":20,"context":"We use the Compare-Aggregate (CA) model [22], which implements an attention mechanism between the question and the answer candidate with two-step operations, i.","startOffset":40,"endOffset":44},{"referenceID":18,"context":"Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario.","startOffset":59,"endOffset":63},{"referenceID":22,"context":"Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario.","startOffset":74,"endOffset":78},{"referenceID":7,"context":"[8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task.","startOffset":0,"endOffset":3},{"referenceID":5,"context":"[8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task.","startOffset":60,"endOffset":63},{"referenceID":11,"context":"For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, lr = 0.","startOffset":127,"endOffset":131},{"referenceID":8,"context":"For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, lr = 0.","startOffset":157,"endOffset":160},{"referenceID":7,"context":"In particular, we show (confirming the results from ad hoc retrieval and our previous work [8]) that BERT can improve passage/sentence reranking in a QA setting.","startOffset":91,"endOffset":94}],"year":2020,"abstractText":"IR-based Question Answering (QA) systems typically use a sentence selector to extract the answer from retrieved documents. Recent studies have shown that powerful neural models based on the Transformer can provide an accurate solution to Answer Sentence Selection (AS2). Unfortunately, their computation cost prevents their use in real-world applications. In this paper, we show that standard and efficient neural rerankers can be used to reduce the amount of sentence candidates fed to Transformer models without hurting Accuracy, thus improving efficiency up to four times. This is an important finding as the internal representation of shallower neural models is dramatically different from the one used by a Transformer model, e.g., word vs. contextual embeddings.","creator":"LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2019/11/10 v7.00c Hypertext links for LaTeX"}}
{"name":"3397271.3401057.pdf","metadata":{"source":"CRF","title":"Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View","authors":["Jibing Gong","Shen Wang","Jinlong Wang","Wenzheng Feng","Hao Peng","Jie Tang","Philip S. Yu"],"emails":["gongjibing@ysu.edu.cn","swang224@uic.edu","wangjinlong@stumail.ysu.edu.cn","fwz17@mails.tsinghua.edu.cn","penghao@act.buaa.edu.cn","jietang@tsinghua.edu.cn","psyu@uic.edu","permissions@acm.org."],"sections":[{"heading":null,"text":"∗The first two authors contributed equally. †Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/11221.27\ninformation from different meta-paths, in order to capture the different interests of different students. To learn the parameters of the proposedmodel, we propose to utilize extendedmatrix factorization (MF). A series of experiments are conducted, demonstrating the effectiveness of ACKRec across multiple popular metrics compared with state-of-the-art baseline methods. The promising results show that the proposedACKRec is able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs.\nCCS CONCEPTS • Information systems → Recommender systems; Collaborative filtering; Personalization; Social recommendation; • Computing methodologies → Neural networks.\nKEYWORDS Recommender System; Graph Neural Networks; Heterogeneous Information Network\nACM Reference Format: Jibing Gong, Shen Wang, Jinlong Wang, Wenzheng Feng, Hao Peng, Jie Tang, and Philip S. Yu. 2020. Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View. In 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/11221.27"},{"heading":"1 INTRODUCTION","text":"In recent years, massive open online courses (MOOCs) are gradually becoming a mode of alternative education worldwide. For example, Coursera, edX, and Udacity, the three pioneering MOOC platforms, offer millions of user accesses to numerous courses from internationally renowned universities. In China, millions of users study in XuetangX1, which is one of the largest MOOC platforms[20], where thousands of courses are offered on various subjects. Although the number of students in MOOCs is continuously growing, there are still some straits withMOOCs. A challenging problem forMOOCs is how to attract students to study continuously and efficiently on the platforms, where the overall course completion rate is lower than\n1http://www.xuetangx.com\n5% [34]. Therefore, it requires better understanding and capturing of student interests.\nTo understand and capture student interests on MOOCs platforms, multiple efforts have been done, including course recommendation [13, 35], behavior prediction [20], user intentions understanding [34], etc. Among these efforts, recommendation system is applied by MOOCs provider to recommend courses to students. However, a course usually consists of a number of video lectures with each one covering some specific knowledge concepts. Direct course recommendation overlooks students’ interest to specific knowledge concept, e.g., computer vision courses taught by different instructors may be quite different in a microscopic view (cover different sets of knowledge concepts): someone instructor may only cover geometry based methods while other one may only cover deep learning based methods, and thus recommending the computer vision course only covering geometry based methods to the student interested in the deep learning based methods will not be a good match. Therefore, it requires to study students’ online learning interests from a microscopic view and conduct knowledge concept recommendation.\nTraditional recommendation strategy, such as collaborative filtering (CF), which considers user (students) historical interactions and makes recommendations based on potential common preferences from users with similar interests, has achieved great success. However, CF based methods suffer from the sparsity of user-item (student-knowledge concept) relationships, which limits the recommendation performance. To overcome this problem, a number of efforts have been done by leveraging side information, such as social networks [11], user/item attributes [27], images [33], contexts [25], etc. In a MOOCs platform, we observe that in addition to the user and knowledge concept, there exist multiple types of entities (video, course, teacher) and multiple types of relationships between pair of different entities. Table 1 shows the statistics of the real-world XuetangX data collected between January 1st, 2018 and March 31st, 2018. This data consists of 9,986 users, 43,405 videos, 7,020 courses, 5,038 teachers, 1,029 knowledge concepts, and corresponding multiple types of relationships. As shown in Figure 1, the “course: V_9e77179” includes the “knowledge concept: c++”, the “student: 207256” is taking the “course: CaltechX”, the “video: V_1a9aa686” is related to the “knowledge concept: binary tree”, and the “course: CaltechX” is taught by the “teacher: Smith”. Further more, taking users’ behavior history into consideration, we can discover additional relationships. For example, the “user: 207256” clicked the “knowledge concept: c++”, “knowledge concept: binary\ntree”, and “knowledge concept: depth-first search”. Accounting for above multiple types of relationships, we can get much more fruitful facts and interactions between the user and knowledge concepts. If we merely depend on the basic structures, it is difficult to find the significant interaction between “knowledge concept: depth-first search” and “knowledge concept: time complexity ”, which belong to different courses but are clicked by one user. As shown in Figure 1, different knowledge concepts contain different context. Only utilizing single type of interaction may overlook the significant relations between user and knowledge concept. For example, “knowledge concept: c++” and “knowledge concept: binary tree” have dissimilar semantics even though they are included in the same video. These heterogeneous relationships provide rich side information and can benefit the recommendation system in three folds: (1) semantic relatedness among knowledge concepts can be introduced and help to identify the latent interaction; (2) a user’s interests can be reasonably extended and the diversity of recommended knowledge concepts can be increased; and (3) a user’s interest can be interpreted by tracking a user’s historical records along these relationships. Thus, it requires to incorporate these heterogeneous relationships into the representation learning of the entities.\nBased on above observation, we propose Attentional Heterogeneous Graph Convolutional Deep Knowledge Recommender (ACKRec ), an end-to-end framework for knowledge concept recommendation on MOOCs platform. To capture heterogeneous complex relationships, we model the MOOCs platform data as a heterogeneous information network (HIN) [23]. Then, we propose an attentionbased graph convolutional networks (GCNs) to learn the representation of different entities. Traditional GCNs can only capture the homogeneous relationships among homogeneous entities, which overlooks the rich information among heterogeneous relationships. To address this issue, we use meta-paths [25] as the guidance to capture the heterogeneous context information in a HIN via GCN. In this way, the heterogeneous relationships are utilized in a more natural and intuitive way. Moreover, considering that different students may have different interests, we further propose an attention mechanism to adaptively leverage context in multiple meta-paths. In the end, we propose to optimize the parameters of proposed model via an extended matrix factorization and obtain the final recommendation list.\nThe key contributions of this paper can be summarized as follows:• We identify the important problem of knowledge concept\nrecommendation, which is often overlooked by the existing MOOCs recommendation system. Knowledge concept recommendation fills this gap and provides a more microscope level recommendation.\n• We propose ACKRec , a novel end-to-end framework utilizing rich heterogeneous context side information to assist knowledge concept recommendation. • We develop a heterogeneous information network modeling to capture various complex interactions among different types of entities in the MOOCs platform. • We design an attention-based graph convolutional network, which can incorporate both content and heterogeneous context together into the representation learning of different entities. The proposed model can automatically discovers user potential interests by propagating users’ preferences under the guide of meta-path in an attentional way. • We conduct numerous experimental studies using real-world data collected from XuetangX to fully evaluate the performance of the proposed model. We study the parameters, including meta-path combination, representation dimension, number of latent factories, and number of GCNs layers. We synthetically demonstrate the effectiveness of the proposed model compared with a series of strong baselines."},{"heading":"2 PROBLEM STATEMENT AND SYSTEM ARCHITECTURE","text":""},{"heading":"2.1 Problem Statement","text":"Given a target user with corresponding interactive data in MOOCs, the goal is to calculate the interest score about the user and a series of knowledge concepts, and the recommend results – a top \uD835\uDC41 list of knowledge concepts. More formally, given interactive data of a user \uD835\uDC62, a predict function \uD835\uDC53 is learned and used to generate a recommend list of knowledge concepts \uD835\uDC3E (e.g., \"c++\", \"binary tree\", \"linked list\", ect.), such that \uD835\uDC53 : \uD835\uDC62 → {\uD835\uDC58\uD835\uDC56 |\uD835\uDC58\uD835\uDC56 ∈ \uD835\uDC3E, \uD835\uDC56 < \uD835\uDC41 }."},{"heading":"2.2 System Architecture","text":"The architecture of our proposed knowledge concept recommendation system, ACKRec, is shown in Figure 2. It consists of the following components:\n• Feature Extraction. By using the data collected from the MOOCs, we first extract content information as content feature from the knowledge concepts’ name, and then analyze various relationships (e.g., \uD835\uDC50\uD835\uDC5C\uD835\uDC5B\uD835\uDC50\uD835\uDC52\uD835\uDC5D\uD835\uDC61 − \uD835\uDC63\uD835\uDC56\uD835\uDC51\uD835\uDC52\uD835\uDC5C and \uD835\uDC50\uD835\uDC5C\uD835\uDC5B\uD835\uDC50\uD835\uDC52\uD835\uDC5D\uD835\uDC61 − \uD835\uDC50\uD835\uDC5C\uD835\uDC62\uD835\uDC5F\uD835\uDC60\uD835\uDC52 relations) among different types of entities (e.g., knowledge concept, video, course) to describe the knowledge concept. Similarly, we also generate the concept features and context feature for the user. (See Section 3.1 for details regarding feature extraction.) • Meta-path Selection. Based on the features extracted from the data, in this module we construct a structural HIN to model the relationships among different types of entities, and then select different meta-paths from the HIN to depict the relatedness over knowledge concept (i.e., with different meanings). For example, if two different users enrolled in the same course, we brings an edge between two users. (See Section 3.2 for details regarding the meta-path builder on HIN.)\n• Representation Learning ofHeterogeneous Entities.Based on the meta-paths constructed in the previous step, a representation learning model is proposed to learn the lowdimensional representations of the entities in a heterogeneous view. The model is capable of capturing the structural correlations between heterogeneous entities. Specifically, we leverage the selected meta-paths to guide the entity representation learning via graph convolutional networks. Later, we utilize the attention mechanism to adaptively fuse the learned entity representations from different meta-paths. (See Section 3.3 for details regarding our proposed model ACKRec .) • Rating Prediction. After generating the low-dimensional representations of users and knowledge concepts, the dense vectors of entities are fed to an extended matrix factorization to learn the parameters of the model. Moreover, we predict users’ interests in the unclicked knowledge concepts base on the user-item (student-knowledge concept) rating matrix."},{"heading":"3 PROPOSED METHOD","text":"In this section, we introduce the details of how we learn the representation of knowledge concepts and users based on the generated content feature and context feature, and how we perform knowledge concept recommendation based on the learned representations."},{"heading":"3.1 Feature Extraction","text":"3.1.1 Content Feature. In general, names of knowledge concepts are almost a generalization of knowledge concepts (e.g., “c++,” “binary tree,” “linked list”), which contains rich semantic information. Hence, we generate the word embedding of the name of the knowledge concept and use it as content feature for knowledge concept. Specifically, we use Word2vector [18] to generate the word embedding. For the user concept, we generate the content feature in a similar way.\n3.1.2 Context Feature. Content feature such as word embedding of knowledge concept names can be used to represent information of a knowledge concept. Besides, there exist rich context information, such as relationships between different entities in the network structure (e.g., user: 207256 watched video: v_9e77179 and video: v_1a9aa686; this behavior implies a relation between two videos). To include these complex relationships among different types of entities, we further model the context information as the feature. Specifically, we consider the following relationships in a user learning activities.\n• \uD835\uDC45\uD835\uDC621 : Based on the data of users’ online learning behaviors, we build the user-click-knowledge concept matrix A\uD835\uDC621 , where each element \uD835\uDC50\uD835\uDC56, \uD835\uDC57 ∈ {0, 1} implies that a user \uD835\uDC56 clicked a knowledge concept \uD835\uDC57 during his learning activities. • \uD835\uDC45\uD835\uDC622 : To describe the relation between a user and a course, we generate the user-learn-course matrix A\uD835\uDC622 , where each element \uD835\uDC59\uD835\uDC56, \uD835\uDC57 ∈ {0, 1}. It indicates that a user \uD835\uDC56 is taking a course \uD835\uDC57 . • \uD835\uDC45\uD835\uDC623 : Similarly, we generate the user-watch-videomatrixA \uD835\uDC62 3 ,\nwhere each element \uD835\uDC64\uD835\uDC56, \uD835\uDC57 ∈ {0, 1} denotes that a user \uD835\uDC56 has watched a video \uD835\uDC57 .\n• \uD835\uDC45\uD835\uDC624 : To describe the behavior that a user is taking a course, which is taught by a teacher, we generate the user-learncourse-taught by-teacher matrix A\uD835\uDC624 , where element \uD835\uDC61\uD835\uDC56, \uD835\uDC57 ∈ {0, 1} denotes that a user \uD835\uDC56 is taking a course taught by a teacher \uD835\uDC57 .\nWe generate these relationship to describe the user related interactions in the heterogeneous information network. For knowledge concepts, we also discover a number of knowledge concepts related relationships. For example, the relation knowledge conceptincluded by-video denotes that a knowledge concept is included in a video, and the relation knowledge concept-involved-course indicates that a knowledge concepts is covered in a course."},{"heading":"3.2 Meta-path Based Relationship","text":"To model different types of entities and their complex relationships in a proper manner, we first describe how to utilize a HIN to depict users, knowledge concepts, and corresponding heterogeneous relations among them. Before proceeding to our approach, we first introduce some related concepts.\nDefinition 1. Heterogeneous information network (HIN) [23]. A HIN is denoted as G = {V, E} consisting of an object set V and a link set E. A HIN is also associated with an object type mapping function \uD835\uDF19 : V → N and a link type mapping function \uD835\uDF11 : E → R. N and R denote the sets of the predefined object and link types, where |N | + |R| > 2.\nIn this study, wemodel theMOOCs data as the a HIN. Specifically, the constructed HIN includes five entities (i.e., user (U), course (C), video (V), teacher (T), knowledge concept (K) as shown in Figure 2) and a series of relationships among them (e.g., \uD835\uDC45\uD835\uDC621 , \uD835\uDC45 \uD835\uDC62 2 , \uD835\uDC45 \uD835\uDC62 3 , \uD835\uDC45 \uD835\uDC62 4 ). Based on the constructed HIN, we can obtain the network schema, where their definitions are as followed.\nDefinition 2. Network schema [28]. The network schema is denoted as S = (N ,R). It is a meta-template for an information network G = {V, E} with the object type mapping \uD835\uDF19 : V → N\nand the link type mapping \uD835\uDF11 : E → R, which is a directed graph defined over object types N with edges as relations from R.\nWe defined our network schema in Figure 3, which represents semantic and relation information comprehensively in the MOOCs dataset. Based on the the network schema, we can discover the semantic paths between a pair of entities, which is called metapath.\nDefinition 3.Meta-path [5]. Ameta-path is defined on a network\nschema S = (N ,R) and is denoted as a path in the form of \uD835\uDC411 \uD835\uDC451→ \uD835\uDC412 \uD835\uDC452→ · · · \uD835\uDC45\uD835\uDC59→ \uD835\uDC41\uD835\uDC59+1 (abbreviated as\uD835\uDC411, \uD835\uDC412, · · ·\uD835\uDC41\uD835\uDC59+1), which describes a composite relation R = R1 ◦ \uD835\uDC452 · · · ◦ \uD835\uDC45\uD835\uDC59 between object \uD835\uDC411 and \uD835\uDC41\uD835\uDC59+1 , where ◦ denotes the composition operator on relations.\nTypical meta-paths between two users can be defined as follows:\n\uD835\uDC48 click−→ \uD835\uDC3E click −1 −→ \uD835\uDC48 , which means that two different users are related because they click the same knowledge concept; \uD835\uDC48 learn−→ \uD835\uDC36 taught by −→ \uD835\uDC47 taught by −1 −→ \uD835\uDC36 learn −1 −→ \uD835\uDC48 , which denotes that two users are related through paths containing different courses taught by the same teacher. Notice that, the potential meta-paths induced from the HIN can be infinite, but not everyone is relevant and useful for the specific task of interest. Fortunately, there are some algorithms [2] proposed recently for automatically selecting the meta-paths for particular tasks. Given all the concepts about the HIN, we now proceed to our problem of Heterogeneous Information Network Representation Learning. The notations we will use throughout the article are summarized in Table 2."},{"heading":"3.3 Attention-based Graph Convolutional Networks for HIN Representation Learning","text":"After the content features and context features are obtained, we feed the entity content features to the graph convolutional networks to learn the latent entity representation. Given the heterogeneous information network G = (V, E) associated with a set of meta-paths MP = { \uD835\uDC40\uD835\uDC431, \uD835\uDC40\uD835\uDC432, · · ·\uD835\uDC40\uD835\uDC43 ∥\uD835\uDC40\uD835\uDC43 ∥ } and the corresponding adjacency\nTable 2: Notations and explanations.\nNotation Explanation\nG heterogeneous information network V set of entities E set of relations S network schema\nMP set of meta-paths A, D̃ adjacency matrix and degree matrix\nbase different meta-paths X the features matrix of entities h\uD835\uDC59 \uD835\uDC59 − \uD835\uDC61ℎ layer of entity representation W\uD835\uDC59 weights of \uD835\uDC59 − \uD835\uDC61ℎ GCN layer e the representation of entities \uD835\uDEFC the weights of meta-paths x\uD835\uDC62 the latent factors of user \uD835\uDC62 y\uD835\uDC58 the latent factors of knowledge concept \uD835\uDC58 \uD835\uDC5A,\uD835\uDC5B the number of users and knowledge concepts in MOOCs dataset \uD835\uDC51 the representation dimension \uD835\uDC37 the number of latent factors r\uD835\uDC62,\uD835\uDC58 the true rating of user \uD835\uDC62 to knowledge concept \uD835\uDC58 r̂\uD835\uDC62,\uD835\uDC58 predicted rating of user \uD835\uDC62 to knowledge concept \uD835\uDC58\nt the matrix to integrate the e \uD835\uDC62 and e\uD835\uDC58 be in the same space \uD835\uDEFD the tuning parameter\nFigure 3: Network schema for HINs in MOOCs. The MOOCs include user, course, video, teacher and knowledge concept. Different types line indicate different type of relationships among different types entities.\nmatrixA = { A1,A2, · · ·A |\uD835\uDC40\uD835\uDC43 | } . |\uD835\uDC40\uD835\uDC43 | denotes the number of metapaths. We adopt a multiple-layer graph convolutional network (GCN) with the following layer-wise propagation rule:\nh(\uD835\uDC59+1) = \uD835\uDF0E ( Ph\uD835\uDC59W\uD835\uDC59 ) , (1)\nHere we remove the subscripts of meta-path indicator, user indicator and knowledge concept indicator for all the graph related symbols for simplification. h(\uD835\uDC59+1) denotes the new representation of an entity. In particular, h0 is the content feature we have extracted at the first step. P = D̃−1/2ÃD̃−1/2, Ã = A+I is the adjacency matrix corresponds to a specific meta-path with self-connections and I is the identity matrix, D̃ = diag(Ã1), and 1 is the all-ones vector. Here \uD835\uDF0E (·) is defined as \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 (·), where \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 (\uD835\uDC4E) = max {0, \uD835\uDC4E} is an entry-wise rectified linear activation function. \uD835\uDC59 is the layer number indicator.W\uD835\uDC59 is the shared trainable weight matrix for all the entities at layer \uD835\uDC59 . Weight sharing is beneficial since it is statistically and computationally more efficient than the traditional embedding\nmethods. With the help of the weight sharing, the model can be well regularized and the number of parameters is significantly reduced.\nThe information propagation process of content or context can be regarded as a Markov process converges to a stationary distribution P ∈ R\uD835\uDC5B×\uD835\uDC5B and the row \uD835\uDC56 indicating a likelihood of spreading from the knowledge concept \uD835\uDC56 . This stationary distribution of the diffusion process is proven to have a closed form solution. When considering the 1-step truncation of the diffusion process, the propagation layer computes the weighted sum of the contexts’ current representation. We set P0 = P1 = P2 = D̃−1/2ÃD̃−1/2, and the three propagation layers are defined as follows:\nh1 = \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 ((P0X)W0), (2)\nh2 = \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 ((P1H1)W1), (3)\nh3 = \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 ((P2H2)W2), (4)\ne\uD835\uDC40\uD835\uDC43 = h3, (5)\nwhere e\uD835\uDC40\uD835\uDC43 ∈ R\uD835\uDC51 is the final representations of an entity. Going through the three propagation layers, we learn the rep-\nresentations for each meta-path. However, different meta-paths should not be considered equally. To address this problem, we utilize the attention mechanism to fuse the representation of entities learned under the guide of different meta-paths and generate the attentional joint representation. Specifically, we learn the attention weights for different meta-paths as follows:\ne = |\uD835\uDC40\uD835\uDC43 |∑ \uD835\uDC56=1 att ( e\uD835\uDC40\uD835\uDC43\uD835\uDC56 ) e\uD835\uDC40\uD835\uDC43\uD835\uDC56 , (6)\nHere, att(·) indicates the attention function. e indicates final representation of an entity, which has integrated the attention weights of different meta-paths. Since in this problem, we mainly focus on the user and knowledge concept. The target entity is user or knowledge concept. Formally, given the corresponding representation e\uD835\uDC40\uD835\uDC43\uD835\uDC56 for each meta-path\uD835\uDC40\uD835\uDC43\uD835\uDC56 ∈ {\uD835\uDC40\uD835\uDC431, \uD835\uDC40\uD835\uDC432, · · · , \uD835\uDC40\uD835\uDC43 |MP |}, we define the attention weights as follows:\n\uD835\uDEFC\uD835\uDC40\uD835\uDC43\uD835\uDC56 = exp\n( \uD835\uDF0E (ae\uD835\uDC40\uD835\uDC43\uD835\uDC56 ) )∑ \uD835\uDC57 ∈ |\uD835\uDC40\uD835\uDC43 | exp ( \uD835\uDF0E (ae\uD835\uDC40\uD835\uDC43 \uD835\uDC57 )\n) , (7) where e\uD835\uDC40\uD835\uDC43\uD835\uDC56 is the representation of an entity based on the target meta-path, and e\uD835\uDC40\uD835\uDC43 \uD835\uDC57 denotes the representation based on the other meta-paths. a denotes a trainable attention vector, and \uD835\uDF0E denotes the nonlinear gating function. We formulate a feed-forward neural network to compute the correlation between one meta-path and the other meta-paths. This correlation is normalized by a softmax function. The attentional joint representation can be represented as follows:\ne = |\uD835\uDC40\uD835\uDC43 |∑ \uD835\uDC56=1 \uD835\uDEFC\uD835\uDC40\uD835\uDC43\uD835\uDC56 e\uD835\uDC40\uD835\uDC43\uD835\uDC56 , (8)\nwhere \uD835\uDEFC = ∑ |\uD835\uDC40\uD835\uDC43 | \uD835\uDC56=1 \uD835\uDEFC\uD835\uDC40\uD835\uDC43\uD835\uDC56 , and e denotes the final representation ofknowledge concepts. The meta-path attention allows us to better infer the importance of different meta-paths by leveraging their correlations and learning the entities representations.The algorithm framework is shown in Algorithm 1.\nAlgorithm 1 Generating the representations of entities. Input:\nthe given meta-paths set\uD835\uDC40\uD835\uDC43 ; the corresponding adjacency matrix set A; the features matrix x of target entities; the dimension of representations \uD835\uDC51 .\nOutput: The representations of target entities e.\n1: Initialize e; 2: \uD835\uDC5D\uD835\uDC4E\uD835\uDC61ℎ\uD835\uDC60 ⇐ []; 3: for each\uD835\uDC40\uD835\uDC43\uD835\uDC56 ∈ \uD835\uDC40\uD835\uDC43 do 4: Calculate Ã, D̃ and P according to Eq.1; 5: h0 ⇐ x; 6: Calculate h1 by Eq.2; 7: Calculate h2 by Eq.3; 8: Calculate h3 by Eq.4; 9: Add h3 to \uD835\uDC5D\uD835\uDC4E\uD835\uDC61ℎ\uD835\uDC60 ; 10: end for 11: Generate e by Eq.8; 12: return e."},{"heading":"3.4 Matrix Factorization for Knowledge Concept Recommendation","text":"So far, we have studied how to extract content feature and context feature of users and knowledge concepts, respectively. Using attention-based GCNs for representation learning, we can obtain the representation of knowledge concepts e\uD835\uDC58 , and the representation of users e\uD835\uDC62 . In this part, we propose to utilize an extended matrix factorization (MF) based method to perform knowledge concept recommendation for the users. We consider the number of times users click on the knowledge concepts as a rating matrix. The rating of a user on a knowledge concept can be defined as follows:\nr̂\uD835\uDC62,\uD835\uDC58 = x ⊤ \uD835\uDC62 y\uD835\uDC58 , (9)\nwhere x\uD835\uDC62 ∈ R\uD835\uDC37×\uD835\uDC5A indicates latent factors of the user and y\uD835\uDC58 ∈ R\uD835\uDC37×\uD835\uDC5B denotes latent factors of the knowledge concept. \uD835\uDC37 is the number of latent factors. And\uD835\uDC5A and \uD835\uDC5B are the number of the user entities and knowledge concept entities. Because we have also obtained the representations for user u and knowledge concept k, we further feed them into the rating predictor as follows:\nr̂\uD835\uDC62,\uD835\uDC58 = x ⊤ \uD835\uDC62 y\uD835\uDC58 + \uD835\uDEFD\uD835\uDC62 · e\uD835\uDC62⊤t\uD835\uDC58 + \uD835\uDEFD\uD835\uDC58 · t\uD835\uDC62⊤e\uD835\uDC58 , (10)\nwhere e\uD835\uDC62 and e\uD835\uDC58 are the representation of users and knowledge concepts. The trainable parameters t\uD835\uDC62 and t\uD835\uDC58 are introduced to make sure e\uD835\uDC62 and e\uD835\uDC58 be in the same space. \uD835\uDEFD\uD835\uDC62 and \uD835\uDEFD\uD835\uDC58 are the tuning parameters. The purpose is to achieve a suitable ratings prediction, so the object function of MF is defined as follows:\nmin \uD835\uDC48 ,\uD835\uDC3E 1 \uD835\uDC5A × \uD835\uDC5B \uD835\uDC5B∑ \uD835\uDC62=1 \uD835\uDC5A∑ \uD835\uDC58=1 ( r\uD835\uDC62,\uD835\uDC58 − r̂\uD835\uDC62,\uD835\uDC58 )2 . (11)\nWe further add regularization terms to the function. Therefore, the final objective function is formulated as follows:\nmin \uD835\uDC48 ,\uD835\uDC3E 1 \uD835\uDC5A × \uD835\uDC5B \uD835\uDC5B∑ \uD835\uDC62=1 \uD835\uDC5A∑ \uD835\uDC58=1 ( r\uD835\uDC62,\uD835\uDC58 − r̂\uD835\uDC62,\uD835\uDC58 )2 + \uD835\uDF06( | |x\uD835\uDC62 | |2 +||y\uD835\uDC58 | |2 + ||t\uD835\uDC62 | |2 + ||t\uD835\uDC58 | |2),\n(12)\nwhere \uD835\uDF06 is the regularization parameter. We then utilize the sto-\nchastic gradient descent algorithm to optimize the local minimum of the final objective function."},{"heading":"4 EXPERIMENTS","text":""},{"heading":"4.1 Datasets","text":"We collected real world data from \uD835\uDC4B\uD835\uDC62\uD835\uDC52\uD835\uDC61\uD835\uDC4E\uD835\uDC5B\uD835\uDC54\uD835\uDC4B MOOC platform. We select enrollment behaviors occurring between October 1st, 2016 and December 30th, 2017 as the training set, and those occurring between January 1st, 2018 and March 31st, 2018 as the test set. Each instance in the training set or test set is a sequence representing a user’s history of click behaviors. In the training process, for each sequence in the training data, we treat the last clicked knowledge concept as the target and the remainders as the past behaviors. Moreover, for each positive instance, we randomly generate one negative instance to replace the target knowledge concept. In the testing process, we treat each enrolled knowledge concept in the test set as the target knowledge concept; the corresponding knowledge concepts of the same user in the training set are treated as the sequence representing the history of clicked knowledge concepts. To evaluate the recommendation performance, each positive instance in the test set is paired with 99 randomly sampled negative instances, and outputs prediction scores for the 100 instances (1 positive and 99 negatives)[7]."},{"heading":"4.2 Evaluation Metrics","text":"We evaluate all the methods in terms of the widely used metrics, including Hit Ratio of top-K items (HR@K) and Normalized Discounted Cumulative Gain of top-K items (NDCG@K) [12]. HR@K is a recall-based metric that measures the percentage of ground truth instances that are successfully recommended in the top K, and NDCG@K is a precision-based metric that accounts for the predicted position of the ground truth instance. We set K to 5, 10, and 20, and calculate all metrics for every 100 instances (1 positive plus 99 negatives). The final recommendation list for user \uD835\uDC62 is \uD835\uDC45\uD835\uDC62 = { \uD835\uDC5F1\uD835\uDC62 , \uD835\uDC5F 2 \uD835\uDC62 , · · · , \uD835\uDC5F\uD835\uDC3E\uD835\uDC62 } . \uD835\uDC5F \uD835\uDC56\uD835\uDC62 denotes rank at the \uD835\uDC56 − \uD835\uDC61ℎ position in \uD835\uDC45\uD835\uDC62 based on the predict score. \uD835\uDC47\uD835\uDC62 is the interacted items set of user \uD835\uDC62 in the test data, and \uD835\uDC41 is the total number of users in our test data.\n\uD835\uDC3B\uD835\uDC45@\uD835\uDC3E = 1 \uD835\uDC41 ∑ \uD835\uDC62 \uD835\uDC3C ( |\uD835\uDC45\uD835\uDC62 ∩\uD835\uDC47\uD835\uDC56 |), (13)\nwhere, \uD835\uDC3C (\uD835\uDC65) is an indicator function whose value is 1 when \uD835\uDC65 > 0 and 0 otherwise. The large the value of \uD835\uDC3B\uD835\uDC45@\uD835\uDC3E the better the performance of the model.\n\uD835\uDC41\uD835\uDC37\uD835\uDC36\uD835\uDC3A@\uD835\uDC3E = 1 \uD835\uDC4D \uD835\uDC37\uD835\uDC36\uD835\uDC3A@\uD835\uDC3E = 1 \uD835\uDC4D \uD835\uDC3E∑ \uD835\uDC57=1 2\uD835\uDC3C ( | {\uD835\uDC5F \uD835\uDC57 \uD835\uDC62 }∩\uD835\uDC47\uD835\uDC62 |) − 1 \uD835\uDC59\uD835\uDC5C\uD835\uDC542 ( \uD835\uDC57 + 1) , (14)\nwhere\uD835\uDC4D is a normalization constant which is themaximumpossible value of \uD835\uDC37\uD835\uDC36\uD835\uDC3A@\uD835\uDC3E . We also use the mean reciprocal rank (MRR). From the definition, we can see that a larger MRR value indicates a better performance of the model[7].\nMRR = 1 \uD835\uDC41 \uD835\uDC41∑ \uD835\uDC56=1 1 rank\uD835\uDC56 , (15)\nwhere \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC58\uD835\uDC56 refers to the rank position of the one positive instance for the \uD835\uDC56 − \uD835\uDC61ℎ user in 100 instances. In addition, we also add the area under the curve of ROC (AUC) as a metric."},{"heading":"4.3 Detailed Analysis of the Proposed Approach","text":"4.3.1 Evaluation of Different Meta-paths Combination. In this part of the experiments, we analyze how selection of meta-path combinations affect the performance of ACKRec , since a small number of high-quality meta-paths can lead to considerable performance [23]. We consider both single meta-path and their combinations. Specifically, We select four types of meta-paths to characterize the relatedness between pair of users, including \uD835\uDC40\uD835\uDC431: \uD835\uDC48 → \uD835\uDC3E\n-1−→ \uD835\uDC48 , \uD835\uDC40\uD835\uDC432: \uD835\uDC48 → \uD835\uDC36 -1−→ \uD835\uDC48 , \uD835\uDC40\uD835\uDC433: \uD835\uDC48 → \uD835\uDC49 -1−→ \uD835\uDC48 , and \uD835\uDC40\uD835\uDC434: \uD835\uDC48 → \uD835\uDC36 → \uD835\uDC47 -1−→ \uD835\uDC36 -1−→ \uD835\uDC48 . We also select three types of meta-path to characterize the relatedness between pair of knowledge concepts, including \uD835\uDC3E ↔ \uD835\uDC3E , \uD835\uDC3E → \uD835\uDC48 -1−→ \uD835\uDC58 and \uD835\uDC3E → \uD835\uDC36 -1−→ \uD835\uDC3E . To analyze the impact of different combinations in a small number of meta-paths, we use all three meta-paths to model the knowledge concept and study the performance with single user related meta-path and their combinations. The experiments results are shown in Table 3. From Table 3, we can find that each single meta-path (i.e., \uD835\uDC40\uD835\uDC431, \uD835\uDC40\uD835\uDC432, \uD835\uDC40\uD835\uDC433,\uD835\uDC40\uD835\uDC434) exhibits different performance, where the performance ranking is\uD835\uDC40\uD835\uDC433 >\uD835\uDC40\uD835\uDC431 >\uD835\uDC40\uD835\uDC432 >\uD835\uDC40\uD835\uDC434, and the combinations of single meta-paths follow the same tendency (e.g., the performance of \uD835\uDC40\uD835\uDC431&\uD835\uDC40\uD835\uDC433 > \uD835\uDC40\uD835\uDC431&\uD835\uDC40\uD835\uDC432, \uD835\uDC40\uD835\uDC431&\uD835\uDC40\uD835\uDC432&\uD835\uDC40\uD835\uDC433 > \uD835\uDC40\uD835\uDC431&\uD835\uDC40\uD835\uDC432&\uD835\uDC40\uD835\uDC434). This illustrates that different meta-paths indicate different relations Further, although the growth of performance is not quite obvious, we can observe that the combination including more metapaths will exhibit better performance, and the best performance is achieved by combining all four meta-paths.\n4.3.2 Evaluation of Model Parameters. In a matrix factorizationbased method, the number of latent factors is an important parameter. Therefore, we present a comparison of the performance obtained with different numbers of latent factors. As shown in Figure 4, we select the metrics \uD835\uDC3B\uD835\uDC45@\uD835\uDC3E , \uD835\uDC41\uD835\uDC37\uD835\uDC36\uD835\uDC3A@\uD835\uDC3E ,\uD835\uDC40\uD835\uDC45\uD835\uDC45, and \uD835\uDC34\uD835\uDC48\uD835\uDC36 to show how the performance of ACKRec changes with changes in the number of latent factors. We tune the number from 10 to 40 in increments of 10. We can see the increase in performance becomes\nflat as the number of latent factors increases. We find that using 30 latent factors can produce optimal performance.\nAfter setting the number of latent factors as 30, we study the dimension settings of the entities representation. The experiment results are shown in Figure 5. We conducted the experiments using different numbers of dimensions (i.e., 20, 50, 100, 150, 200), and found that optimal performance was achieved with 100. Therefore, both the user and the knowledge concepts are represented as 100- dimensional vectors. The results also show that the representation of user and knowledge concepts in the heterogeneous information network is an important factor in improving the performance of the recommendation task.\nWe also examine how the number of GCN layers influence the performance of the model. As shown in Figure 6, we can clearly see that the performance of the proposed model changes with different numbers of layers (i.e., 1, 2, 3, and 4). It illustrates that the optimal number of GCN layers is around 3."},{"heading":"4.4 Baseline Methods","text":"To evaluate the performance of the proposed approach, we consider various of baseline methods as follows:\n• \uD835\uDC35\uD835\uDC43\uD835\uDC45 [22]: It optimizes a pairwise ranking loss for the recommendation task in a Bayesian manner. • \uD835\uDC40\uD835\uDC3F\uD835\uDC43 [8]: It applies amulti-layer perceptron (MLP) to a pair of user representations and corresponding knowledge concept representations to learn the probability of recommending a knowledge concept to the user. • \uD835\uDC39\uD835\uDC40 [21]: This is a principled approach that can easily incorporate any heuristic features. However, to ensure a fair comparison to other methods, we only use the representations of users and knowledge concepts. • \uD835\uDC39\uD835\uDC3C\uD835\uDC46\uD835\uDC40 [14]: This is an item-to-item collaborative filtering algorithm that conducts recommendations based on the average embeddings of all behavior histories and the embeddings of the target knowledge concept. • \uD835\uDC41\uD835\uDC34\uD835\uDC3C\uD835\uDC46 [7]: This is also an item-to-item collaborative filtering algorithm, but distinguishes the weights of different online learning behaviors using an attention mechanism method. • \uD835\uDC41\uD835\uDC34\uD835\uDC46\uD835\uDC45 [16]: This is an improved GRU [9] model that estimates an attention coefficient for each behavior history based on the corresponding hidden vector output by GRU. And GRU is a gated recurrent unit model which receives a list of historical behavior as input. • \uD835\uDC5A\uD835\uDC52\uD835\uDC61\uD835\uDC4E\uD835\uDC5D\uD835\uDC4E\uD835\uDC61ℎ2\uD835\uDC63\uD835\uDC52\uD835\uDC50 [3]: This is a meta-paths based representation method in heterogeneous information network by random walk and skip-gram. With the same meta-path of user and knowledge concept inACKRecmodel, we usemetapath2vec to generate the representations of users and knowledge concepts with the same dimension of ACKRec. • \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50ℎ : A variant of ACKRec , which ignores the heterogeneity of entities in the heterogeneous information network. We regenerate the adjacency matrix to represent the relationship between different entities. • \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC50 : This can be viewed as a variant of ACKRec without the attention mechanism method, and concatenates different meta-paths together.\nTable 4: Results obtained with different models using the MOOC dataset.\nHR@1 HR@5 HR@10 HR@20 NDCG@5 NDCG@10 NDCG@20 MRR AUC\n\uD835\uDC35\uD835\uDC43\uD835\uDC45 0.1699 0.4633 0.6246 0.7966 0.3217 0.3736 0.4151 0.3156 0.8610 \uD835\uDC40\uD835\uDC3F\uD835\uDC43 0.0660 0.3680 0.5899 0.7237 0.2231 0.2926 0.3441 0.2146 0.8595 \uD835\uDC39\uD835\uDC40 0.2272 0.4057 0.5867 0.7644 0.3655 0.3968 0.3930 0.3067 0.8574 \uD835\uDC39\uD835\uDC3C\uD835\uDC46\uD835\uDC40 0.1410 0.5849 0.7489 0.7610 0.3760 0.4203 0.4279 0.3293 0.8532 \uD835\uDC41\uD835\uDC34\uD835\uDC3C\uD835\uDC46 0.078 0.4112 0.6624 0.8649 0.2392 0.3201 0.3793 0.2392 0.8863 \uD835\uDC41\uD835\uDC34\uD835\uDC46\uD835\uDC45 0.1382 0.4437 0.6215 0.7475 0.2364 0.3172 0.3821 0.2117 0.8215 \uD835\uDC5A\uD835\uDC52\uD835\uDC61\uD835\uDC4E\uD835\uDC5D\uD835\uDC4E\uD835\uDC61ℎ2\uD835\uDC63\uD835\uDC52\uD835\uDC50 0.2476 0.5983 0.7598 0.8689 0.4194 0.4422 0.4602 0.3873 0.8909 \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50ℎ 0.2092 0.5388 0.7139 0.8665 0.3783 0.4348 0.4738 0.3634 0.8927 \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC50 0.2457 0.5917 0.7542 0.8778 0.4216 0.4763 0.5079 0.4026 0.8974 \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60 0.2195 0.5917 0.7476 0.8553 0.4154 0.4659 0.4933 0.3891 0.8848 \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC5F 0.2588 0.6427 0.7911 0.8909 0.4591 0.5074 0.5329 0.4285 0.9035 \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60+\uD835\uDC5F 0.2645 0.6470 0.8122 0.9255 0.4635 0.5170 0.5459 0.4352 0.9232\nFigure 6: Performance of different number of layers. • \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60 : Content feature-based ACKRec . The input of this model is just the content feature of entities.\n• \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC5F : Context feature-based ACKRec . Similar to the model \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60 , the context feature of entities in MOOCs are fed into this model. • \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60+\uD835\uDC5F : The proposed method, which combines heterogeneous context feature and content feature of entities to maximally depict the entities in the HIN. For the MOOCs dataset, we split the user history behavior data into a training set and a test set. The training time for our method and the baseline methods are followed: BPR 2hrs,MLP 2hrs, FM 2.5hrs, FISM 4hrs, NAIS 3.5 hrs, NASR 4.5 hrs, metapath2vec 5.5 hrs, ACKRec (our method) 4.5 hrs. As shown in Table 4, we compare ACKRec with other machine learning methods. For the HIN based methods, We select meta-paths combination given the best performance in Section 3.1.2. As shown in Table 4, HIN based methods outperform all the other methods. It indicates the importance of the heterogeneity in MOOCs data Furthermore, the results show that\uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60+\uD835\uDC5F , which integrates content feature and context\nfeature, gives the best performance. Different from metapath2vec [3] generating representations based on random walk strategy and skip-gram method to generate representations of nodes, our model utilizes the graph convolutional networks to learn representations and adaptive attention mechanism to learn the different meta-paths weights and can better capture the heterogeneity within the data.\nIn addition, compared with \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50ℎ , it obviously demonstrates that ACKRec utilizing the meta-paths based method on the HIN can more effectively capture the heterogeneous relationships. Compared with\uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC50 , we can see that adaptively fuse representation learned in different meta-path is better than simple concatenation, since different meta-path has different importance corrsponds to the task. Moreover, compared with \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60 and \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC5F , it demonstrates that methods utilizing either content feature or context feature alone will lose information needed for the representation of entities; thus, they cannot comprehensively depict the feature of users and knowledge concepts in MOOCs. The final method \uD835\uDC34\uD835\uDC36\uD835\uDC3E\uD835\uDC45\uD835\uDC52\uD835\uDC50\uD835\uDC60+\uD835\uDC5F , which uses adaptive weights and the meta-path based approach over the HIN, can integrate rich content feature of entities and structural relations between different types of entities in a more comprehensive and effective manner, and achieves the best performance."},{"heading":"4.5 Case Study","text":"In this part, we conduct one case to demonstrate the effectiveness of our proposed method ACKRec . We randomly select a student:2481307 and obtain two top 10 recommend lists based on single meta-path\uD835\uDC40\uD835\uDC432 and combined meta-paths\uD835\uDC40\uD835\uDC431−\uD835\uDC40\uD835\uDC432−\uD835\uDC40\uD835\uDC433−\uD835\uDC40\uD835\uDC434, respectively. As shown in Figure 7, we can intuitively observe that the proposed model generates different results with different reality conditions. With more meaningful relationships among user and knowledge concept, the recommend list will contain more related knowledge concepts for the corresponding user. For example, from the click history list of \uD835\uDC60\uD835\uDC61\uD835\uDC62\uD835\uDC51\uD835\uDC52\uD835\uDC5B\uD835\uDC61 : 2481307, we know that the student is especially interested in the field of computer network, the result of meta-paths\uD835\uDC40\uD835\uDC431−\uD835\uDC40\uD835\uDC432−\uD835\uDC40\uD835\uDC433−\uD835\uDC40\uD835\uDC434 shows that theACKRec can capture the student’s interests of knowledge concept, matching his real next click, asynchronous transfer mode, shown in blue. The other recommendations such as protocol data unit, switched telephone network, switch, and IPv4 are also highly related. In particularly, the recommended results will be significantly different for different meta-paths."},{"heading":"5 RELATEDWORK","text":""},{"heading":"5.1 Graph Neural Network in Heterogeneous Information Network","text":"Graphs play a crucial role in modern machine learning[5, 6]. Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability. However, in the real world, the graphs are usually heterogeneous. There are a few attempts heterogeneous information network setting. Wang et al. [28] proposed DeepHGNN, an attentional heterogeneous graph neural network model to learn from the heterogeneous program behavior graph to guide the reidentification process. Wang et al. [29] presented HAGNN, a Hierarchical Attentional Graph Neural Encoder and used it for program behavior\ngraph analysis. Additionally, the GEM[17] model, a heterogeneous graph neural network approach for detecting malicious accounts at Alipay, has been presented. Unlike these approaches, our proposed model utilizes attentional graph convolutional networks for the representations of users and knowledge concepts in heterogeneous information networks."},{"heading":"5.2 Recommendation System in","text":"Heterogeneous Information Network Some information recommendation models are based on heterogeneous information networks. [19] proposed Heaters, a graph-based model, to solve the general recommendation problem in heterogeneous networks. Yu et al.[32] proposed to use meta-paths based latent features to represent the connectivity between users and items along with different types of paths. Additionally, Follow precious work, Shi et al. [10, 23, 24] proposed to use meta-path concept to mode the heterogeneous information in HIN. Different from previous methods, this study focuses on capture the representations of different types of entities on the heterogeneous information network and fuses themselves content feature of different types of entities and the structure features of entities in MOOCs data together for the recommendation task of the knowledge concept."},{"heading":"6 CONCLUSION","text":"In this work, we investigate the problem of the knowledge concept recommendation in MOOCs system, which is often overlooked by MOOCs recommendation system.We proposeACKRec , an end-toend graph neural network based approach that naturally incorporates rich heterogeneous context side information into knowledge concept recommendation. To make use of rich context information in a more natural and intuitive way, we model the MOOCs as a heterogeneous information network. We design an attention-based graph convolutional network to learn the representation of different entities via propagate context information under the guide of meta-path in an attentional way. With the help of proposed attention-based graph convolutional network, the users’ potential interests can be effectively explored and aggregated. Comprehensive experimental study on real data collected from XuetangX is\nconducted. The proposed approaches outperform the strong baseline. The promising experimental results illustrate the effectiveness of the proposed method."},{"heading":"7 ACKNOWLEDGMENTS","text":"This work is supported by NSF under grants III-1526499, III-1763325, III-1909323, CNS-1930941, by Science and Technology Project of the Headquarters of State Grid co., LTD under Grant No. 5700- 202055267A-0-0-0, and by NKPs under grants 2018YFC0830804. The authors also would like to thank XuetangX for data collection and supports."}],"references":[{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","author":["Jie Chen","Tengfei Ma","Cao Xiao"],"venue":null,"citeRegEx":"1","shortCiteRegEx":"1","year":2018},{"title":"Task-guided and path-augmented heterogeneous network embedding for author identification","author":["Ting Chen","Yizhou Sun"],"venue":"In Proceedings of the Tenth ACM International Conference on Web Search and Data","citeRegEx":"2","shortCiteRegEx":"2","year":2017},{"title":"metapath2vec: Scalable representation learning for heterogeneous networks","author":["Yuxiao Dong","Nitesh V Chawla","Ananthram Swami"],"venue":"In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining","citeRegEx":"3","shortCiteRegEx":"3","year":2017},{"title":"Convolutional Neural Network Architectures for Signals Supported on Graphs","author":["F. Gama","A.G. Marques","G. Leus","A. Ribeiro"],"venue":"IEEE Transactions on Signal Processing 67,","citeRegEx":"4","shortCiteRegEx":"4","year":2019},{"title":"A new model for learning in graph domains","author":["M. Gori","G. Monfardini","F. Scarselli"],"venue":"In Proceedings","citeRegEx":"5","shortCiteRegEx":"5","year":2005},{"title":"Representation Learning on Graphs: Methods and Applications","author":["William L. Hamilton","Rex Ying","Jure Leskovec"],"venue":null,"citeRegEx":"6","shortCiteRegEx":"6","year":2017},{"title":"NAIS: Neural Attentive Item Similarity Model for Recommendation","author":["Xiangnan He","Zhankui He","Jingkuan Song","Zhenguang Liu","Yu-Gang Jiang","Tat-Seng Chua"],"venue":"IEEE Transactions on Knowledge and Data Engineering","citeRegEx":"7","shortCiteRegEx":"7","year":2018},{"title":"Session-based recommendations with recurrent neural networks","author":["Balázs Hidasi","Alexandros Karatzoglou","Linas Baltrunas","Domonkos Tikk"],"venue":"arXiv preprint arXiv:1511.06939","citeRegEx":"9","shortCiteRegEx":"9","year":2015},{"title":"Leveraging metapath based context for top-n recommendation with a neural co-attention model","author":["Binbin Hu","Chuan Shi","Wayne Xin Zhao","Philip S Yu"],"venue":"In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","citeRegEx":"10","shortCiteRegEx":"10","year":2018},{"title":"A matrix factorization technique with trust propagation for recommendation in social networks","author":["Mohsen Jamali","Martin Ester"],"venue":"In Proceedings of the fourth ACM conference on Recommender systems","citeRegEx":"11","shortCiteRegEx":"11","year":2010},{"title":"IR evaluation methods for retrieving highly relevant documents","author":["Kalervo Järvelin","Jaana Kekäläinen"],"venue":"In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval","citeRegEx":"12","shortCiteRegEx":"12","year":2000},{"title":"Guess You Like: Course Recommendation in MOOCs","author":["Xia Jing","Jie Tang"],"venue":"In Proceedings of the International Conference on Web Intelligence (WI ’17)","citeRegEx":"13","shortCiteRegEx":"13","year":2017},{"title":"FISM: Factored Item Similarity Models for top-N Recommender Systems","author":["Santosh Kabbur","Xia Ning","George Karypis"],"venue":"In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’13)","citeRegEx":"14","shortCiteRegEx":"14","year":2013},{"title":"Kipf and MaxWelling","author":["N Thomas"],"venue":"arXiv preprint arXiv:1609.02907","citeRegEx":"15","shortCiteRegEx":"15","year":2016},{"title":"Neural Attentive Session-based Recommendation","author":["Jing Li","Pengjie Ren","Zhumin Chen","Zhaochun Ren","Tao Lian","Jun Ma"],"venue":"In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017,","citeRegEx":"16","shortCiteRegEx":"16","year":2017},{"title":"Heterogeneous Graph Neural Networks for Malicious Account Detection","author":["Ziqi Liu","Chaochao Chen","Xinxing Yang","Jun Zhou","Xiaolong Li","Le Song"],"venue":"In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM ’18)","citeRegEx":"17","shortCiteRegEx":"17","year":2018},{"title":"Efficient estimation of word representations in vector space","author":["Tomas Mikolov","Kai Chen","Greg Corrado","Jeffrey Dean"],"venue":null,"citeRegEx":"18","shortCiteRegEx":"18","year":2013},{"title":"A General Recommendation Model for Heterogeneous Networks","author":["T.N. Pham","X. Li","G. Cong","Z. Zhang"],"venue":"IEEE Transactions on Knowledge and Data Engineering 28,","citeRegEx":"19","shortCiteRegEx":"19","year":2016},{"title":"Modeling and Predicting Learning Behavior in MOOCs","author":["Jiezhong Qiu","Jie Tang","Tracy Xiao Liu","Jie Gong","Chenhui Zhang","Qian Zhang","Yufei Xue"],"venue":"In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining (WSDM ’16)","citeRegEx":"20","shortCiteRegEx":"20","year":2016},{"title":"Factorization Machines with libFM","author":["Steffen Rendle"],"venue":"ACM TIST 3,","citeRegEx":"21","shortCiteRegEx":"21","year":2012},{"title":"BPR: Bayesian Personalized Ranking from Implicit Feedback","author":["S. Rendle","C. Freudenthaler","Z. Gantner","L. Schmidt-Thieme"],"venue":"arXiv e-prints (May","citeRegEx":"22","shortCiteRegEx":"22","year":2012},{"title":"Heterogeneous Information Network Embedding for Recommendation","author":["C. Shi","B. Hu","W.X. Zhao","P.S. Yu"],"venue":"IEEE Transactions on Knowledge and Data Engineering 31,","citeRegEx":"23","shortCiteRegEx":"23","year":2019},{"title":"Semantic Path Based Personalized Recommendation onWeighted Heterogeneous Information Networks","author":["Chuan Shi","Zhiqiang Zhang","Ping Luo","Philip S. Yu","Yading Yue","Bin Wu"],"venue":"In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM ’15)","citeRegEx":"24","shortCiteRegEx":"24","year":2015},{"title":"Collaborative intent prediction with real-time contextual data","author":["Yu Sun","Nicholas Jing Yuan","Xing Xie","Kieran McDonald","Rui Zhang"],"venue":"ACM Transactions on Information Systems (TOIS) 35,","citeRegEx":"25","shortCiteRegEx":"25","year":2017},{"title":"Adaptive graphconvolutional neural networks","author":["Feiyun Zhu T Ruoyu Li","Sheng Wang","Junzhou Huang"],"venue":null,"citeRegEx":"26","shortCiteRegEx":"26","year":2018},{"title":"Shine: Signed heterogeneous information network embedding for sentiment link prediction","author":["Hongwei Wang","Fuzheng Zhang","Min Hou","Xing Xie","Minyi Guo","Qi Liu"],"venue":"In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining","citeRegEx":"27","shortCiteRegEx":"27","year":2018},{"title":"Attentional heterogeneous graph neural network: Application to program reidentification","author":["Shen Wang","Zhengzhang Chen","Ding Li","Zhichun Li","Lu-An Tang","Jingchao Ni","Junghwan Rhee","Haifeng Chen","Philip S Yu"],"venue":"In Proceedings of the 2019 SIAM International Conference on Data Mining","citeRegEx":"28","shortCiteRegEx":"28","year":2019},{"title":"Heterogeneous graph matching networks for unknown malware detection","author":["ShenWang","Zhengzhang Chen","Xiao Yu","Ding Li","JingchaoNi","Lu-An Tang","Jiaping Gui","Zhichun Li","Haifeng Chen","Philip S Yu"],"venue":"In Proceedings of the 28th International Joint Conference on Artificial Intelligence","citeRegEx":"29","shortCiteRegEx":"29","year":2019},{"title":"Session-based Recommendation with Graph Neural Networks","author":["Shu Wu","Yuyuan Tang","Yanqiao Zhu","Liang Wang","Xing Xie","Tieniu Tan"],"venue":null,"citeRegEx":"30","shortCiteRegEx":"30","year":2018},{"title":"Graph Convolutional Neural Networks for Web-Scale Recommender Systems","author":["Rex Ying","Ruining He","Kaifeng Chen","Pong Eksombatchai","William L. Hamilton","Jure Leskovec"],"venue":"In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery  Data Mining (KDD ’18)","citeRegEx":"31","shortCiteRegEx":"31","year":2018},{"title":"Personalized Entity Recommendation: A Heterogeneous Information Network Approach","author":["Xiao Yu","Xiang Ren","Yizhou Sun","Quanquan Gu","Bradley Sturt","Urvashi Khandelwal","Brandon Norick","Jiawei Han"],"venue":"In Proceedings of the 7th ACM International Conference on Web Search and Data Mining (WSDM ’14)","citeRegEx":"32","shortCiteRegEx":"32","year":2014},{"title":"Collaborative knowledge base embedding for recommender systems","author":["Fuzheng Zhang","Nicholas Jing Yuan","Defu Lian","Xing Xie","Wei-Ying Ma"],"venue":"In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining","citeRegEx":"33","shortCiteRegEx":"33","year":2016},{"title":"Smart Jump: Automated Navigation Suggestion for Videos in MOOCs","author":["Han Zhang","Maosong Sun","Xiaochen Wang","Zhengyang Song","Jie Tang","Jimeng Sun"],"venue":"In Proceedings of the 26th International Conference on World Wide Web Companion,","citeRegEx":"34","shortCiteRegEx":"34","year":2017},{"title":"Hierarchical Reinforcement Learning for Course Recommendation in MOOCs","author":["Jing Zhang","Bowen Hao","Bo Chen","Cuiping Li","Hong Chen","Jimeng Sun"],"venue":"In Proceedings of the AAAI Conference on Artificial Intelligence,","citeRegEx":"35","shortCiteRegEx":"35","year":2019},{"title":"Graph Neural Networks: A Review of Methods and Applications","author":["Jie Zhou","Ganqu Cui","Zhengyan Zhang","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"venue":"CoRR abs/1812.08434","citeRegEx":"36","shortCiteRegEx":"36","year":2018}],"referenceMentions":[{"referenceID":18,"context":"In China, millions of users study in XuetangX1, which is one of the largest MOOC platforms[20], where thousands of courses are offered on various subjects.","startOffset":90,"endOffset":94},{"referenceID":11,"context":"To understand and capture student interests on MOOCs platforms, multiple efforts have been done, including course recommendation [13, 35], behavior prediction [20], user intentions understanding [34], etc.","startOffset":129,"endOffset":137},{"referenceID":33,"context":"To understand and capture student interests on MOOCs platforms, multiple efforts have been done, including course recommendation [13, 35], behavior prediction [20], user intentions understanding [34], etc.","startOffset":129,"endOffset":137},{"referenceID":18,"context":"To understand and capture student interests on MOOCs platforms, multiple efforts have been done, including course recommendation [13, 35], behavior prediction [20], user intentions understanding [34], etc.","startOffset":159,"endOffset":163},{"referenceID":32,"context":"To understand and capture student interests on MOOCs platforms, multiple efforts have been done, including course recommendation [13, 35], behavior prediction [20], user intentions understanding [34], etc.","startOffset":195,"endOffset":199},{"referenceID":9,"context":"To overcome this problem, a number of efforts have been done by leveraging side information, such as social networks [11], user/item attributes [27], images [33], contexts [25], etc.","startOffset":117,"endOffset":121},{"referenceID":25,"context":"To overcome this problem, a number of efforts have been done by leveraging side information, such as social networks [11], user/item attributes [27], images [33], contexts [25], etc.","startOffset":144,"endOffset":148},{"referenceID":31,"context":"To overcome this problem, a number of efforts have been done by leveraging side information, such as social networks [11], user/item attributes [27], images [33], contexts [25], etc.","startOffset":157,"endOffset":161},{"referenceID":23,"context":"To overcome this problem, a number of efforts have been done by leveraging side information, such as social networks [11], user/item attributes [27], images [33], contexts [25], etc.","startOffset":172,"endOffset":176},{"referenceID":21,"context":"To capture heterogeneous complex relationships, we model the MOOCs platform data as a heterogeneous information network (HIN) [23].","startOffset":126,"endOffset":130},{"referenceID":23,"context":"To address this issue, we use meta-paths [25] as the guidance to capture the heterogeneous context information in a HIN via GCN.","startOffset":41,"endOffset":45},{"referenceID":16,"context":"Specifically, we use Word2vector [18] to generate the word embedding.","startOffset":33,"endOffset":37},{"referenceID":21,"context":"Heterogeneous information network (HIN) [23].","startOffset":40,"endOffset":44},{"referenceID":1,"context":"Fortunately, there are some algorithms [2] proposed recently for automatically selecting the meta-paths for particular tasks.","startOffset":39,"endOffset":42},{"referenceID":6,"context":"To evaluate the recommendation performance, each positive instance in the test set is paired with 99 randomly sampled negative instances, and outputs prediction scores for the 100 instances (1 positive and 99 negatives)[7].","startOffset":219,"endOffset":222},{"referenceID":10,"context":"We evaluate all the methods in terms of the widely used metrics, including Hit Ratio of top-K items (HR@K) and Normalized Discounted Cumulative Gain of top-K items (NDCG@K) [12].","startOffset":173,"endOffset":177},{"referenceID":6,"context":"From the definition, we can see that a larger MRR value indicates a better performance of the model[7].","startOffset":99,"endOffset":102},{"referenceID":21,"context":"In this part of the experiments, we analyze how selection of meta-path combinations affect the performance of ACKRec , since a small number of high-quality meta-paths can lead to considerable performance [23].","startOffset":204,"endOffset":208},{"referenceID":20,"context":"• BPR [22]: It optimizes a pairwise ranking loss for the recommendation task in a Bayesian manner.","startOffset":6,"endOffset":10},{"referenceID":19,"context":"• FM [21]: This is a principled approach that can easily incorporate any heuristic features.","startOffset":5,"endOffset":9},{"referenceID":12,"context":"• FISM [14]: This is an item-to-item collaborative filtering algorithm that conducts recommendations based on the average embeddings of all behavior histories and the embeddings of the target knowledge concept.","startOffset":7,"endOffset":11},{"referenceID":6,"context":"• NAIS [7]: This is also an item-to-item collaborative filtering algorithm, but distinguishes the weights of different online learning behaviors using an attention mechanism method.","startOffset":7,"endOffset":10},{"referenceID":14,"context":"• NASR [16]: This is an improved GRU [9] model that estimates an attention coefficient for each behavior history based on the corresponding hidden vector output by GRU.","startOffset":7,"endOffset":11},{"referenceID":7,"context":"• NASR [16]: This is an improved GRU [9] model that estimates an attention coefficient for each behavior history based on the corresponding hidden vector output by GRU.","startOffset":37,"endOffset":40},{"referenceID":2,"context":"• metapath2vec [3]: This is a meta-paths based representation method in heterogeneous information network by random walk and skip-gram.","startOffset":15,"endOffset":18},{"referenceID":2,"context":"Different from metapath2vec [3] generating representations based on random walk strategy and skip-gram method to generate representations of nodes, our model utilizes the graph convolutional networks to learn representations and adaptive attention mechanism to learn the different meta-paths weights and can better capture the heterogeneity within the data.","startOffset":28,"endOffset":31},{"referenceID":4,"context":"Graphs play a crucial role in modern machine learning[5, 6].","startOffset":53,"endOffset":59},{"referenceID":5,"context":"Graphs play a crucial role in modern machine learning[5, 6].","startOffset":53,"endOffset":59},{"referenceID":0,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":3,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":13,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":24,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":28,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":29,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":34,"context":"Recently, graph neural networks[1, 4, 15, 26, 30, 31, 36] have become recurrent topics in machine learning, and both have broad applicability.","startOffset":31,"endOffset":57},{"referenceID":26,"context":"[28] proposed DeepHGNN, an attentional heterogeneous graph neural network model to learn from the heterogeneous program behavior graph to guide the reidentification process.","startOffset":0,"endOffset":4},{"referenceID":27,"context":"[29] presented HAGNN, a Hierarchical Attentional Graph Neural Encoder and used it for program behavior Figure 7: The case study of ACKRec bases different meta-","startOffset":0,"endOffset":4},{"referenceID":15,"context":"Additionally, the GEM[17] model, a heterogeneous graph neural network approach for detecting malicious accounts at Alipay, has been presented.","startOffset":21,"endOffset":25},{"referenceID":17,"context":"[19] proposed Heaters, a graph-based model, to solve the general recommendation problem in heterogeneous networks.","startOffset":0,"endOffset":4},{"referenceID":30,"context":"Yu et al.[32] proposed to use meta-paths based latent features to represent the connectivity between users and items along with different types of paths.","startOffset":9,"endOffset":13},{"referenceID":8,"context":"[10, 23, 24] proposed to use meta-path concept to mode the heterogeneous information in HIN.","startOffset":0,"endOffset":12},{"referenceID":21,"context":"[10, 23, 24] proposed to use meta-path concept to mode the heterogeneous information in HIN.","startOffset":0,"endOffset":12},{"referenceID":22,"context":"[10, 23, 24] proposed to use meta-path concept to mode the heterogeneous information in HIN.","startOffset":0,"endOffset":12}],"year":2020,"abstractText":"Massive open online courses (MOOCs) are becoming a modish way for education, which provides a large-scale and open-access learning opportunity for students to grasp the knowledge. To attract students’ interest, the recommendation system is applied by MOOCs providers to recommend courses to students. However, as a course usually consists of a number of video lectures, with each one covering some specific knowledge concepts, directly recommending courses overlook students’ interest to some specific knowledge concepts. To fill this gap, in this paper, we study the problem of knowledge concept recommendation. We propose an end-to-end graph neural network based approach called Attentional Heterogeneous Graph Convolutional Deep Knowledge Recommender (ACKRec) for knowledge concept recommendation in MOOCs. Like other recommendation problems, it suffers from sparsity issue. To address this issue, we leverage both content information and context information to learn the representation of entities via graph convolution network. In addition to students and knowledge concepts, we consider other types of entities (e.g., courses, videos, teachers) and construct a heterogeneous information network (HIN) to capture the corresponding fruitful semantic relationships among different types of entities and incorporate them into the representation learning process. Specifically, we use meta-path on the HIN to guide the propagation of students’ preferences. With the help of these meta-paths, the students’ preference distribution with respect to a candidate knowledge concept can be captured. Furthermore, we propose an attention mechanism to adaptively fuse the context ∗The first two authors contributed equally. †Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/11221.27 information from different meta-paths, in order to capture the different interests of different students. To learn the parameters of the proposedmodel, we propose to utilize extendedmatrix factorization (MF). A series of experiments are conducted, demonstrating the effectiveness of ACKRec across multiple popular metrics compared with state-of-the-art baseline methods. The promising results show that the proposedACKRec is able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs.","creator":"TeX"}}
{"name":"3397271.3401052.pdf","metadata":{"source":"CRF","title":"Query Rewriting for Voice Shopping NullQueries","authors":["Iftah Gamzu","Marina Haikin","Nissim Halabi"],"emails":["iftah@amazon.com","mhaikin@amazon.com","nissimh@amazon.com","permissions@acm.org."],"sections":[{"heading":null,"text":"Voice shopping using natural language introduces new challenges related to customer queries, like handling mispronounced, misexpressed, and misunderstood queries. Voice null queries, which result in no offers, have negative impact on customers shopping experience. Query rewriting (QR) attempts to automatically replace null queries with alternatives that lead to relevant results. We present a new approach for pre-retrieval QR of voice shopping null queries. Our proposed QR framework first generates alternative queries using a search index-based approach that targets different potential failures in voice queries. Then, a machine-learning component ranks these alternatives, and the original query is amended by the selected alternative. We provide an experimental evaluation of our approach based on data logs of a commercial voice assistant and an e-commerce website, demonstrating that it outperforms several baselines by more than 22%. Our evaluation also highlights an interesting phenomenon, showing that web shopping null queries are considerably different, and apparently easier to fix, than voice queries. This further substantiates the use of specialized mechanisms for the voice domain. We believe that our proposed framework, mapping tail queries to head queries, is of independent interest since it can be extended and applied to other domains.\nCCS CONCEPTS\n• Information systems → Information retrieval query processing.\nKEYWORDS\nquery rewriting, voice assistant, e-commerce search, null query, voice search\nACM Reference Format: Iftah Gamzu, Marina Haikin, and Nissim Halabi. 2020. Query Rewriting for Voice Shopping Null Queries. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401052"},{"heading":"1 INTRODUCTION","text":"In the past few years, speech has emerged has a natural mean for communicating information need to various search engines via\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401052\nvoice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Among various experiences, voice assistants enable customers to search and shop for products in an intuitive way using natural language. Providing a free-form shopping experience is a challenging task. This is especially true because the voice interface lacks assisting mechanisms, such as query completion and refinement, that can help customers easily express their need and iterate on it. As a consequence, a non-negligible portion of all voice shopping queries are null queries, that is, queries that result with no offers. Such interactions clearly have negative impact on customers shopping experience [40–42]. Query rewriting (QR) attempts to seamlessly replace null queries with alternatives that lead to relevant offers for the customer intent, and by that, help customers progress on their shopping journey.\nThe voice interface introduces technical and architectural challenges that are either unique or more emphasized compared to textual interfaces (e.g., web search). For example, the voice interface lacks assisting mechanisms that are common in the web, resulting in more shopping queries that contain erroneous attributes such as product brand, quantity, and others; While automatic speech recognition (ASR) and natural language understanding (NLU) techniques have made considerable progress in recent years (see, e.g., [27, 30, 36]), they still have failures that exhibit different patterns (e.g., phonetic) than common textual errors; Finally, customers fail in formulating a coherent spoken query in real-time, resulting in new types of speech imperfection errors such as stammered or misspronounced words. All these motivate the application of specialized mechanisms for the voice domain. Our contribution.We introduce a pre-retrieval QR approach designed to handle voice shopping null queries, as illustrated in Figure 1. In a pre-retrieval approach [11], QR happens without knowledge about the offers that return for the original query or any of the generated alternatives. This approach is more efficient than postretrieval methods, inducing less overhead to the search system. Upon receiving a transcribed query q, our QR framework applies a two step process of alternative queries generation and alternative queries ranking. As voice-user interface introduces a strong presentation bias towards the top-ranked offer [20], in most cases, only a single offer is communicated back to the customer. Therefore, practically, it is sufficient to identify a single “hero” alternative query to amend the customer query. The top-ranked alternative is passed along with the original query to a controller, which ensures the quality of the selected alternative. Specifically, the controller implements quality safeguards that prevent replacing the original query with an alternative query that is likely to retrieve irrelevant results to the intent of the original query. Then, the original query q is sent along with the selected alternative query r∗ for retrieving offers from the e-commerce search engine. If q turns to be a null\nquery with no offers, the customer is presented with the offers retrieved for r∗.\nOur alternative queries generation component is based on identifying web and voice shopping queries that frequently lead to positive events (e.g., purchases), and indexing them in a search engine. Those positive queries are indexed using various analyzers that target different potential failures in voice shopping (e.g., analyzers based on textual, n-grams, and phonetic similarities). Given a query, one can retrieve multiple alternatives from the index in hope that one of them can amend it, without the need to isolate the exact error. Conceptually, the core of our generation approach is by mapping tail (low frequency) queries to alternative head (high frequency) queries. The main motivation for this approach comes from the fact that head queries are known to exhibit much better performance than tail queries, due to richer historical behavioral features [23]. In fact, the distinction is even more extreme in voice due to the aforementioned strong presentation bias. In addition, Ingber et al. [20] recently observed that products purchased through voice are much more limited in terms of diversity, namely, products purchased on a regular basis such as groceries, and not niche long-tail products. This provides another motivation for our approach as positive head queries correspond well with commonly and regularly purchased products.\nThe alternative queries ranking component then ranks the alternatives that are more probable to fix the original query. This machine learning-based component utilizes multiple features (like textual and semantic similarities between the originating query and the alternatives, behavioral features, and more) to make its decisions. While our alternative queries generation component aims to improve the recall, the alternative queries ranking component is in charge of tuning the precision. For instance, in an e-commerce rewriting scenario, it is essential for the alternative queries to retrieve offers that capture the same intent as the originating query [41]. Rewriting that leads to offers that do not respect the desired product type are clearly poor. Our ranking approach considers features that help preserve the original customer’s intent (like, extracting the product type from queries), and by that, provides quality guarantees on the alternatives.\nConsider for example the following scenario, which demonstrates the ability to fix ASR errors outside the ASR system. A customer that is interested in purchasing “epilepsy bracelets” communicates her need to a voice assistant. Due to ambient noise, the ASR transcribed query results in “apple upci uhhh bracelets”, which turns to be a null query. However, “epilepsy bracelets” is a frequent positive query, and has the same phonetic representation, “APLPSPRSLTS”, as the transcribed query. Among the various alternatives,\nour QR framework selects “epilepsy bracelets”, which is retrieved by a phonetic analyzer, and a relevant offer is presented.\nWe provide an experimental evaluation for both voice and web null queries based on data logs of a commercial voice assistant and an e-commerce website. Our evaluation demonstrates that our voice query rewriting (VQR) approach outperforms several baselines by large margins. For instance, we show that VQR improves over the effectiveness of a simple textual similarity retrieval-based QR system by roughly 22% on voice data, and a term-dropping QR system by roughly 46% on web data. Although our focus is on voice QR, the improvements observed on web data, reaffirms the utility of our approach. In fact, we believe that our proposed framework is generic enough to be successfully applied in other domains beyond e-commerce. One notable highlight from our evaluation is that a term-dropping QR approach applied to a random set of web null queries attains much better performance than when it is employed to a random set of voice null queries (by roughly 56%). This indicates that web e-commerce null queries are considerably different than voice null queries, and apparently easier to fix. This observation adds to previous line of research identifying differentiating factors between the voice and web domains. For example, it was observed that voice queries are closer to natural language than text queries in general search [15], voice reformulations are distinguishable from textual reformulations [17, 22], and that shopping categories and behavioral patterns defer between voice and web e-commerce search [20]. In summary, we make the following key contributions:\n(1) We propose a new pre-retrieval QR framework for voice\nshopping null queries. Our approach maps tail queries to head queries, targets different potential failures in a voiceuser interface, and aims to maintain the customer’s intent. (2) We reveal interesting insights regarding differences between\nvoice and web null queries, establishing the need for specialized mechanisms treating the voice domain.\nThe rest of the paper is organized as follows: In Section 2, we review related work for QR. In Section 3, we describe our alternative queries generation approach, and in Section 4 we present our machine-learning model for ranking the alternatives. Section 5 presents an experimental evaluation of our proposed VQR approach. We discuss the positive findings of an online A/B tests that was conducted with our VQR approach on a commercial voice assistant in Section 6. We conclude the paper in Section 7.\nWe note that all processes performed as part of our analyses were conducted in accordance with strict privacy guidelines and all methods for handling the data are automated. We cannot share our data with the community due to its sensitivity."},{"heading":"2 RELATEDWORK","text":"Query rewriting has long been an important research area in information retrieval [3]. Extensive analysis has been done for handling and rewriting of queries in web search. The notion of query refinement, expansion, suggestion, substitution, and reformulation are sometime overloaded and have been commonly used synonymously with query rewriting. Most of the previous methods are not particularly suitable for e-commerce queries, which are shorter and more sensitive to context [34], let alone voice e-commerce\nqueries. Focusing on tail low-frequency e-commerce queries highlights additional unique challenges and opportunities, especially around finding and ranking good query alternatives [13]. Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21]. In this work, we concentrate on studying voice e-commerce null (tail) queries.\nOne notable research direction in QR, which is also applicable for e-commerce, focuses on increasing the recall. This direction is especially important for null queries that yield no results. Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query. For example, Jones et al. [25] proposed generating query alternatives by using a large set of ordered query pairs obtained from consecutive queries in web-search sessions. Then, various alternatives are generated by breaking a given query into segments and either dropping or generating substitutions for each of them separately. For dealing with null e-commerce queries, Singh et al. [41] suggested a post-retrieval approach for dropping terms from a given query that restricts the search results to the same taxonomy of results returned in the past for the original query. Tan et al. [43] suggested generating sub-queries by dropping unimportant terms based on their part-of-speech (POS) tag and additional features. Our results hint that term-dropping methods for e-commerce null queries do not adjust well to the voice domain. Other ideas for substitution-based solutions, using the query-flow graph [6], were also proposed [7, 16]. However, finding good recommendations for tail e-commerce queries based on session co-occurrence turns to be difficult [16]. For addressing also the long tail of the query distribution, Bonchi et al. [8] conceptually extend the query-flow graph with term nodes in addition to query nodes. Broccolo et al. [9] generate an inverted index of “successful” queries, i.e., ending query of a session with a click on its search result, and recommends queries retrieved from that index. Our approach has similarities with the later in using an index of successful queries, and extends the use of inverted index to include phonetic, sub-words, and semantic similarities upon retrieval.\nRecently, several attempts were made to apply deep learning to various query rewriting tasks. Grbovic et al. [14] proposed to use embedding techniques to expand a query via a k-nearest neighbor search. He et al. [19] proposed a framework that learns to rewrite queries by unsupervised candidate generation and supervised candidate ranking. For unsupervised candidate generation, they presented a a sequence-to-sequence LSTM model, but also incorporated several existing QR systems suggestions. Their scoring function required training over a large web click data. Xiao et al. [44] applied a similar technique based on post-retrieval method for e-commerce web search. The applicability of these techniques to voice queries is still unclear, especially in light of the data sparsity challenges that still exist as voice interfaces are not yet widely adopted. Indeed, users do not tend to switch between voice and text when reformulating queries [39]. Jiang et al. [22] showed that reformulation patterns of voice queries are different from those in conventional textual searches using both lexical and phonetic changes. Hassan et al. [17] developed classifiers for distinguishing reformulation of voice query pairs from textual query pairs. They extended text-based approaches with voice signals such as phonetic\nsimilarity. This hints regarding the importance of phonetic representations in voice query rewriting. Our alternative queries generation approach does not require prior training and adjusts to the voice medium characteristics, considering its phonetic representation.\nOne related direction in QR focuses on improving its precision by narrowing down a search query. In this case, the goal is to refine the query such that the refined alternative retrieves a more relevant subset of results. Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45]. Those latter techniques commonly employ post-retrieval methods and iteratively query the search engine for newly added terms [12, 35]. In general, this direction is not suitable for handling null queries since the main problem with those queries is recall rather than precision. Those query refinement and rewriting techniques are mostly applicable to head queries."},{"heading":"3 GENERATING ALTERNATIVE QUERIES","text":"The alternative queries generation component is responsible for identifying queries that have potential to successfully amend the original query. Our general approach is as follows.We build a search engine index (e.g., based on Elasticsearch [1]) that holds both web and voice queries that led to positive events, such as purchases or an adds-to-cart, with sufficient number of occurrences in the past. In practice, we indexed tens of millions of queries that were collected over a period of several months. Those queries are indexed while applying various analyzers that enable efficient retrieval. We discuss the specifics of those analyzers later on. Given a query q, an alternative queries set Rq is generated by utilizing each of the analyzers to retrieve a query with a maximal score (for that specific analyzer). We only consider the query with the highest score for each analyzer since we only need to identify a single “hero” offer by a single “hero” alternative to be presented to the customer due to the strong presentation bias in a voice-user interface.\nWe explore and combine multiple analyzers that target different potential types of errors in voice queries. An analyzer is typically composed of a tokenizer that splits the text into tokens, and a filter that is applied to the tokens, resulting with terms for indexing and retrieval. Arguably, the simplest analyzer that we utilize, referred to as MLT (more-like-this), splits the queries to their word-level. Then, given a query, the top retrieved queries are selected according to the Okapi BM25 scoring function [38] on the underlying terms. This analyzer can identify good alternative queries if the most distinctive words in the query are kept. For example, rewriting over-specified queries or miss-pronounced queries. Such a method, based on word tokens, fails when important distinctive words are corrupted, e.g., suffer from spelling mistakes. In such cases, it is much better to apply methods that focus on more local patterns. Taking this observation into consideration, we also utilize an analyzer based on character n-grams, that is, an analyzer that breaks the query into (partially overlapping) terms of character length n. When working with voice queries, there are additional special failures, like phonetic errors that originate in ASR systems. In order to cope with such errors, we also employ few phonetic analyzers based on Double Metaphone phonetic encoding [37]. One such analyzer works similarly to MLT, but on a phonetic encoding level.\nIt can successfully replace a query if the main keywords preserve their phonetic representation. Two additional phonetic analyzers that are used are a so-called full phonetic analyzer that considers the entire phonetic representation of a query as a single term, and a phonetic n-grams analyzer, which breaks the query into parts of length n and translates them into their phonetic representation. The former analyzer handles ASR errors that result in splitting or merging of words. For concreteness, we focus on n-grams-type analyzers with n ∈ {3, 4}. Table 1 presents a simple example illustrating the way different analyzers generate terms for indexing and retrieval. Table 2 outlines few synthetic examples that demonstrate the strengths of the various analyzers."},{"heading":"4 RANKING ALTERNATIVE QUERIES","text":"Once the set of alternative queries Rq has been retrieved, the alternative queries ranking component evaluates the probability of each of the alternatives to successfully amend the originating query q. While our component provides a complete ordering between the alternatives, for our voice scenario, we are only interested in the top-ranked alternative r∗ ∈ Rq . The reason lies in the presentation bias towards a single top-ranked offer.\nEvery alternative is retrieved as the top option by some analyzer to replace q. However, it may occur that a retrieved alternative is irrelevant for the originating customer intent. This is especially true for phonetic-based analyzers that due to their generality of soundsalike retrieval may result in an alternative query with different intent, but it is also true for the other types of analyzers. For this reason, we build a machine learning approach that not only ranks the alternatives, but also provides a confidence score regarding their probability to properly replace q. In a sense, the alternative queries generation component aims to improve the recall of our system, while our pointwise ranking component is in charge of tuning its precision. Table 3 exhibits an example in which textual and character-based analyzers fail to generate relevant alternatives, but phonetic-based analyzers do. Notice that the product type “strips” appears in the original query and in the alternatives retrieved from the phonetic-based analyzers. Having agreement between extracted product types from queries is a strong indicator for keeping the customer’s intent intact.\nWe assume to have a labeled dataset for voice shopping queries. Given a null query q, the dataset consists of a label y(q,r ) for every (q, r ) query pair, where r ∈ Rq is a generated alternative for q. This dataset is the result of annotation by an internal team dedicated to voice shopping related annotation tasks. The annotators are given the customer’s utterance that corresponds to q along with the top\nretrieved offers for each alternative r ∈ Rq . For each such offer, the annotators indicate whether it is relevant to the utterance of q or not. Because the annotators classify the relevance based on both the customer intent as implied by the utterance and the returned offers for the alternatives, the classification label captures both up-stream errors due to the voice interface and down-stream errors due to the products search engine. The relevance label y(q,r ) is set to be the relevance annotation between q and the top returned offer for r .\nWe collect features associated to each alternative r as well as various features relating to the relation between q and r (e.g., their textual similarities). Those features are utilized in conjunction with the dataset to optimize over several machine learning models and hyper-parameters. We describe the details of the features and the ranking models in the following subsections."},{"heading":"4.1 Features For Ranking","text":"We evaluated a large family of tens of handcrafted features as well as different standard manipulations of them for our alternative queries ranking approach. We refrain from providing an exhaustive list of evaluated features due to space constraints and since most of them had marginal contribution to the performance of the models. Instead, we provide an overview of the main feature categories that guided the feature engineering, and focus on a small subset of carefully selected features that were able to extract most of our performance improvements. Our main feature categories include:\nTextual similarities between the originating query and an alternative query, and between their extracted metadata (e.g., product type and brand). We consider variants of the Jaccard similarity and Edit distance. We consider distance variants on both word-level and character-level. We also consider distances of lemmatized queries, and distances of the lexicographic representations of queries (i.e., ordering of words by lexicographic order). Finally, we use the BM25 relevance score of the specific analyzer that retrieved the alternative as another tokens similarity measure.\nSemantic similarities between the originating query and an alternative query, and between their extracted metadata. We generated a specialized word embedding trained using the FastText algorithm [5, 26] on a proprietary e-commerce corpus that included shopping queries, product titles, and a concatenation of queries with the title of the product that was purchased as result of them. The semantic similarity between two sentences is defined as the cosine similarity between the average vector of their underlying normalized word embedding vectors.\nHistorical behavioral features of the alternative queries. Those\nfeatures capture the aggregated past performance of the alternative queries. We consider the number of past occurrences and positive actions (purchase or add-to-cart) of a query on both web and voice shopping logs, and its implied conversion rate (i.e., the rate between positive actions and its overall number of occurrences). We note that the behavioral features of the alternative queries are rich as those queries are head queries.\nWe used standard feature selection techniques to avoid overfitting due to small amount of available labeled data (i.e., few thousands). In what follows, we focus on a short list of 7 features that were able to extract most of our performance improvement. Table 4 lists those features. In feature f1 the sorted version of a query is the\none in which the words are lexicographically ordered. Note that in feature f5, PT(q) indicates the product type implied for the query q as extracted by a specialized model. A key challenge in mapping a tail query to a head query is to maintain the purchase intent of the query. Product type alignment is arguably the most coarse way to keep the intent intact."},{"heading":"4.2 The Ranking Model","text":"Our machine learning approach for the alternative queries ranking problem is a two-tier solution. First, for each analyzer type, we train a machine learning model that predicts whether an alternative r , generated by that analyzer, is a suitable replacement for a\nnull query q. Then, we rank all alternative queries according to the different models predictions, and effectively, return the alternative with the highest prediction score. For the purpose of building each analyzer’s model, we have experimented with several machine learning approaches (e.g., logistic regression, support vector machine, random forest) and various hyper-parameters (e.g., the main hyper-parameters for random forest were maximal depth and the number of estimators in the forest). Following an exhaustive search, we identified the best model for this task as a random forest model optimized over a cross-entropy loss. Further technical details about the models are provided in Section 5.3."},{"heading":"5 EXPERIMENTAL EVALUATION","text":""},{"heading":"5.1 Baselines","text":"We compare our VQR approach against two baselines. The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46]. Term-dropping approaches have been demonstrated to be good for increasing the recall of search systems, but they often result in lower precision. The second baseline is a simple retrieval-based approach, build on top of our positive queries index, which only considers word-level textual similarity. This baseline has the ability to both relax or extend the originating query, taking into account the importance of words with respect to a reference collection of queries. This baseline is expected to have better precision than the term-dropping baseline.\nTerm-Dropping QR (TDQR). Alternative queries are generated by exhaustively considering all options to drop different set of terms from the originating query. Selecting a top alternative is done using a post-retrieval method that gives preference to alternatives that lead to more focused set of offers (i.e., having high query scope measure [18, 28]).\nRetrieval-Based QR (RQR). An alternative query is selected based on a retrieval process that uses the same search index as VQR with an Okapi BM25 scoring function on the words. To guarantee high precision from this system, we required alternatives to have a sufficiently high relevance (specifically, a relevance score of 20). This threshold was identified as leading to good separation between higher and lower quality alternatives by manual annotation."},{"heading":"5.2 Datasets","text":"For the purpose of evaluating our approach, we collected two datasets based on a random sample of voice null queries and a random sample of web (text) null queries. All sampled queries were unique, appearing only once in the sample, as expected from tail queries. Each query in the dataset is appended with a list of its alternative queries and their annotated relevance ranking. We relied on an internal team of annotators, dedicated to voice shopping related annotation tasks in order to produce those datasets.\nThe relevance annotations of alternative queries for an originating query were performed as follows. The annotators were given the customer’s utterance (voice query)U that resulted in the textual query q, along with a list of product offers. They were asked to rate the relevance of each offer with respect to the customer intent as implied by the utteranceU on a three-points-scale: 0 indicates a no match betweenU and the offer, 1 indicates a match that (at least) respects the requested product type, and 2 indicates a full match (i.e., the offered product contains all the attributes mentioned in the utterance). The list of product offers consists of the top-ranked offer retrieved for every alternative query r generated for the query q. The alternative queries were generated by the different approaches under evaluation, that is, the baselines and a set of 6 analyzers (MLT, full phonetic, phonetic, 4-grams, 3-grams, and phonetic 4-grams). The relevance label for each (q, r ) query pair was set to be the relevance score between utterance U , which resulted in q, and the top returned offer for the alternative r , as agreed by majority of 3 annotators. Note that because the annotators classify the relevance of offers with respect to the customer utterance, the annotations\nalso capture failures due to the voice interface, like ASR errors and user mispronunciations.\nVoice null queries. A set of voice shopping null queries uniformly sampled at random over a period of one month from query logs of a commercial voice assistant. We consider only queries identified by the annotators to include a shopping intent. The dataset contains 3,564 unique null shopping queries along with their alternative queries rankings. The methods presented in this paper were developed using this dataset of voice null queries.\nWeb (text) null queries. A set of textual shopping null queries uniformly sampled at random over a period of one month from query logs of a commercial e-commerce website. The dataset contains 1,362 unique null shopping queries alongwith their alternative queries rankings. Note that this dataset is only used to evaluate the performance of the different methods for web e-commerce traffic. In this case, we consider multiple offers for each alternative (instead of only a single top-ranked offer) in order to evaluate the resulting offers ranking. This is motivated by the fact that the presentation bias on the web is much lower than on a voice interface, and thus, associating relevance only with a top offer makes less sense."},{"heading":"5.3 Training Random Forest Models","text":"The models for predicting the quality of alternatives of each analyzer type were only trained on the voice null queries dataset. We split the dataset so that 75% of the examples are used for training and validation, and 25% are used for testing, and translated the three-points relevance labeling to a binary labeling by assigning double weight for full match cases. Because the dataset is relatively small, we train the model by using 2 repetitions of 3-fold crossvalidation. Each of the random forest models for the analyzers were trained separately with same features and model parameters.\nIt is interesting to observe that some features have different importance for different analyzers. For example, for the full phonetic analyzer, textual similarity without words sorting is much more important than the one with sorting. This makes a lot of sense because full phonetic method retrieves alternatives that have very similar ordering of words, and therefore evaluating them based on a modified order can be detrimental. On the other hand, for the MLT analyzer, textual similarity with words sorting is more important than the one without sorting. Again, this is reasonable since MLT focuses on identifying alternatives that have similar intent but not necessarily with the same order of words.\nWe considered different combinations of analyzers and corresponding prediction models in order to understand the performance trade-offs. To simplify the presentation, we focus on two variants of VQR, differing by the set of employed analyzers:\nVQR-4. Utilizes a set of arguably the most basic analyzers (phonetic, full phonetic, 4-grams, and MLT) that spans the 3 families of similarities (word-level, character-level, and phoneme-level). In this setting, all random forest classifiers achieved the best results when the maximal tree depth was 6 and the number of estimators in the forest was bounded by 130.\nVQR-6. Uses 6 analyzers (those of VQR-4 appended with 3- grams and phonetic 4-grams). As the two additional analyzers have higher retrieval latency, this variant provides some insight regarding latency-performance trade-offs. In this setting, all random forest\nclassifiers achieved the best results with a maximal tree depth of 6 and number of estimators in the forest bounded by 170."},{"heading":"5.4 Evaluation Metrics","text":"We evaluate all approaches using 3 key metrics:\n• Coverage – The ratio between the number of null queries\nfor which an underlying method generates an alternative, and the overall number of null queries. • Precision (P@1) – The ratio between the number of alter-\nnative queries that were identified as suitable replacements and the overall number of alternative queries. • Effectiveness (E@1) – The multiplication between the cov-\nerage and precision. This metric essentially captures the impact on the customer, namely, the rate of null queries that were changed for the better.\nFor the evaluation of the approaches on the web null queries dataset, we also consider fewmetrics that measure the quality of the resulting ranked offers. These metrics include Pmax@3, which captures the ratio of alternative queries that have at least one relevant offer within their top 3 offers, and the corresponding effectiveness metric Emax@3. We also consider nDCG3, which uses a graded relevance scale, and its corresponding effectiveness metric EnDCG3."},{"heading":"5.5 Experimental Results","text":"We report our results on the test sets of both the voice and web null queries datasets. For confidentiality reasons, we report the evaluation metrics relative to the RQR baseline, omitting absolute metrics numbers. We begin by noting that although the effectiveness measure captures the direct impact on the customer, there is a subtle point here. Optimizing the effectiveness measure can be achieved by providing alternative queries whenever possible. However, from a customer point of view, it may be better that the system will not select an alternative when that query results in irrelevant offers. This observation is especially true on voice, when the presentation of irreverent offers create much higher friction than a graceful failure. This is one of the reasons for the existence of a controller component (recall Figure 1), safeguarding the quality. Consequently, although the best effectiveness is achieved by not setting any safeguard, we restrict our attention to solutions whose precision is around that of the RQR baseline.\nBased on performance analysis on the validation set, we chose a conservative quality threshold of 0.4 on the prediction score of the VQR-6 model. Note that although we concentrate on a specific operating point, VQR-6 model has a wide-range of operating points that have higher precision, coverage and effectiveness than the RQR baseline. Figure 2 illustrates the effectiveness and precision trade-off curves of VQR-6 model (relative to RQR) as a function of the threshold given by the prediction score of the VQR-6 model (x-axis). Notice that the shape of the curve suggests that the model can adjust for different business use-cases by a careful selection of the threshold, e.g., one can optimize for higher-precision customerfacing scenarios. The selected operating point where the prediction score (threshold) equals 0.4 is visualized upon the knee of the effectiveness curve, and as claimed, its achieved precision is higher than the baseline RQR.\nFigure 3 provides an alternative view, exhibiting the trade-offs of precision and effectiveness (y-axis) against the coverage (x-axis). The curves are generated by different thresholds on the prediction score of the VQR-6 model. It is evident that the VQR model has a wide range of operating points that have higher precision, coverage and effectiveness than RQR. Note that the precision of VQR-6 decreases as the coverage increases, indicating that high prediction scores of the model correspond to more confident query rewritings.\nFigure 4 illustrates the cumulative distributions of the prediction scores of the selected alternatives by the VQR-6 model over the validation set. One curve corresponds to the selected alternative queries labeled as relevant, and the second curve corresponds to less-relevant selected alternative queries. We notice that setting a quality threshold at 0.4 blocks many less-relevant alternatives while keeping almost all relevant alternatives. This trend can also be inferred from Figure 2, where the precision is consistently rising\nwhile the threshold increases towards 0.4, but the effectiveness hardly decreases. It seems important to note that each of the cumulative distribution curves is self-contained in the sense that it only takes into account either the number of relevant cases or the less-relevant cases.\nTable 5 presents an evaluation of the different methods on the voice null queries test set. Reported values are statistically significant with p < 0.05. As can be observed, the baseline RQR approach significantly outperforms the effectiveness of TDQR baseline.While the coverage of TDQR is higher than RQR by more than 22%, its precision is significantly lower, resulting in an overall decrease in effectiveness. Both VQR-4 and VQR-6 models outperform the effectiveness of RQR by more than 22% and 26%, respectively. Note that the improvements in the effectiveness and coverage are statistically significant at the 0.05 level using a student’s t-test. The fact that the improvement in precision is not statistically significant is by our design decision, restricting our attention to an operating point that achieves similar precision to that of RQR. Note that there are operating points in which our models improve over the RQR baseline in both precision, coverage and effectiveness in a statistically significant way.\nWhen we dive deeper into the contribution of each analyzer towards the output of VQR-6, we notice that in 26.0% of the cases the analyzer whose alternative had the highest score was phonetic 4-grams, in 21.7% it was MLT, in 18.8% it was 3-grams, in 17.2% it was phonetic, in 12.3% it was 4-grams, and only in 4.0% it was\nfull phonetic. From a precision point of view, the analyzer with the lowest precision is 3-grams. We consider its precision as a reference point. Then, full phonetic has a 2.5% better precision, MLT has a 6% better precision, phonetic has a 12.8% better precision, 4-grams has a 13.2% better precision, and phonetic 4-grams has a 19.6% better precision. Notice that the phonetic 4-grams alternatives have the highest traffic share and precision. This immediately raises the question how VQR-4 still maintains such high precision and effectiveness. It turns out that in many cases, the alternative with the highest score is also identified by other analyzers (with lower score). In VQR-4, the ranking model adjusts so that most of the traffic that phonetic 4-grams and 3-grams handled is spread between 4-grams (34.5%), MLT (32.0%), and phonetic (28.3%) with relatively small changes in precision.\nTable 6 presents an evaluation of the TDQR and VQR-4 methods relative to the RQR baseline on the web null queries test set. Reported values are statistically significant with p < 0.05. Again, RQR significantly outperforms the effectiveness of TDQR in-spite of the significant coverage increase, and VQR-4 outperforms RQR in all metrics with a statistically significant improvement in the coverage and various effectiveness measures at the 0.05 level. We decided not to report the results for VQR-6 as they follow the same trends presented for the voice dataset, and thus, carry a light conceptual message.\nOne especially interesting observation relates to the performance differences of TDQR on voice and web data. We first note that the performance of RQR on the voice and web test sets is roughly the same, having no difference in coverage and only small increase of 3.9% in precision and effectiveness for the web data. So, essentially, our reference performance does not change between voice and web. However, we observe that the degradations in precision and effectiveness of TDQR over the voice test set is much more significant than over the web test set. We see a −43.4% and −56.6% decrease in E@1 and P@1 over voice compared to −15.3% and −34.8% decrease over web. This is a statistically significant difference, hinting that web queries are considerably different, and apparently easier to fix, than voice queries. This important insight adds to previous line of research [15, 17, 20, 22], highlighting differentiating factors between the voice and web domains, and supporting the need for specialized mechanisms for voice."},{"heading":"6 ONLINE EVALUATION","text":"Variants of the proposed QR framework were evaluated for their effectiveness in handling null queries as part of an online A/B test on a commercial voice assistant. In that test, the control group experienced a RQR-like approach, while the treatment group experienced a VQR-4-like approach. The experiment ran for about a month. The A/B test demonstrated positive impact on customers, reducing null queries by 40.5%. While increasing the overall traffic coverage and providing customers with product offers, the quality metrics also demonstrated an improvement. The relevance of the queries handled by the QR system (P@1) increased by about 20%, as evaluated by internal annotators. Furthermore, the rate of positive actions (e.g., purchases) across the entire traffic increased by 1.7%, although the fraction of null queries was relatively small. All those\nresults were shown to be statistically significant at the 0.05 level using a student’s t-test."},{"heading":"7 CONCLUSIONS","text":"We presented a new framework for pre-retrieval query rewriting of voice shopping null queries. Our approach takes the characteristics of the voice-user interface into consideration. We conducted experiments with both voice and web data of a commercial voice assistant and an e-commerce website. Those experiments demonstrated that our approach outperforms several baselines by large margins in both offline and online settings. We also provided empirical evidence for a fundamental difference between voice null queries and web null queries, substantiating the use of specialized mechanisms for the voice domain. We believe that our proposed framework, mapping tail to head queries, is of independent interest as it can be extended and applied to other domains beyond voice shopping. As future work, we plan to explore more advanced mapping techniques. For example, one natural approach is to apply sequence-to-sequence deep learning techniques to learn a mapping from null queries to alternatives. It can be particularly valuable to work with phoneme representations of queries on top of textual ones. Another intention is to identify better ranking techniques. Since voice interfaces are new and not yet widely adopted, there is still a data sparsity issue that limits the ability to tackle those plans. There is a need to research ways to bypass this issue. One such approach may be to massively produce semi-supervised weak labels, e.g., use the relevance score that a search engine assigns to a (query, offer) pair to create labeled data for the ranking. Another promising approach is to use transfer learning from web to voice."},{"heading":"ACKNOWLEDGMENTS","text":"The authors would like to thank their colleagues Natali Arieli, Sagi Bernstein, Anna Chalupowicz, Anna Farberman, David Shamouilian, and Yochai Zvik for their fruitful collaboration, discussions, and help in implementation."}],"references":[{"title":"Voice query refinement","author":["Cyril Allauzen","Edward Benson","Ciprian Chelba","Michael Riley","Johan Schalkwyk"],"venue":"In Thirteenth Annual Conference of the International Speech Communication Association","citeRegEx":"2","shortCiteRegEx":"2","year":2012},{"title":"Discovering key concepts in verbose queries","author":[],"venue":"In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"4","shortCiteRegEx":"4","year":2008},{"title":"Enriching Word Vectors with Subword Information","author":["Piotr Bojanowski","Edouard Grave","Armand Joulin","Tomas Mikolov"],"venue":"Transactions of the Association for Computational Linguistics","citeRegEx":"5","shortCiteRegEx":"5","year":2017},{"title":"The Query-flow Graph: Model and Applications","author":["Paolo Boldi","Francesco Bonchi","Carlos Castillo","Debora Donato","Aristides Gionis","Sebastiano Vigna"],"venue":"In Proceedings of the 17th ACM Conference on Information and Knowledge Management","citeRegEx":"6","shortCiteRegEx":"6","year":2008},{"title":"Query Suggestions Using Query-flow Graphs","author":["Paolo Boldi","Francesco Bonchi","Carlos Castillo","Debora Donato","Sebastiano Vigna"],"venue":"In Proceedings of the 2009 Workshop on Web Search Click Data","citeRegEx":"7","shortCiteRegEx":"7","year":2009},{"title":"Efficient Query Recommendations in the Long Tail via Center-Piece Subgraphs","author":["Francesco Bonchi","Raffaele Perego","Fabrizio Silvestri","Hossein Vahabi","Rossano Venturini"],"venue":null,"citeRegEx":"8","shortCiteRegEx":"8","year":2012},{"title":"Generating suggestions for queries in the long tail with an inverted index","author":["Daniele Broccolo","Lorenzo Marcon","Franco Maria Nardini","Raffaele Perego","Fabrizio Silvestri"],"venue":"Information Processing & Management 48,","citeRegEx":"9","shortCiteRegEx":"9","year":2012},{"title":"Selecting Good Expansion Terms for Pseudo-Relevance Feedback","author":["Guihong Cao","Jian-Yun Nie","Jianfeng Gao","Stephen Robertson"],"venue":"In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"10","shortCiteRegEx":"10","year":2008},{"title":"Estimating the query difficulty for information retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services","author":["David Carmel","Elad Yom-Tov"],"venue":null,"citeRegEx":"11","shortCiteRegEx":"11","year":2010},{"title":"Pseudo-Query Reformulation","author":["Fernando Diaz"],"venue":"In Advances in Information Retrieval. Springer International Publishing,","citeRegEx":"12","shortCiteRegEx":"12","year":2016},{"title":"Anatomy of the Long Tail: Ordinary People with Extraordinary Tastes","author":["Sharad Goel","Andrei Broder","Evgeniy Gabrilovich","Bo Pang"],"venue":"In Proceedings of the Third ACM International Conference on Web Search and Data Mining","citeRegEx":"13","shortCiteRegEx":"13","year":2010},{"title":"Context- and Content-aware Embeddings for Query Rewriting in Sponsored Search","author":["Mihajlo Grbovic","Nemanja Djuric","Vladan Radosavljevic","Fabrizio Silvestri","Narayan Bhamidipati"],"venue":"In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"14","shortCiteRegEx":"14","year":2015},{"title":"Searching by Talking: Analysis of Voice Queries on Mobile Web Search","author":["Ido Guy"],"venue":"In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"15","shortCiteRegEx":"15","year":2016},{"title":"Query Suggestion for E-commerce Sites","author":["Mohammad Al Hasan","Nish Parikh","Gyanit Singh","Neel Sundaresan"],"venue":"In Proceedings of the Fourth ACM International Conference on Web Search and Data","citeRegEx":"16","shortCiteRegEx":"16","year":2011},{"title":"Characterizing and Predicting Voice Query Reformulation","author":["Ahmed Hassan Awadallah","Ranjitha Gurunath Kulkarni","Umut Ozertem","Rosie Jones"],"venue":"In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management","citeRegEx":"17","shortCiteRegEx":"17","year":2015},{"title":"Inferring Query Performance Using Pre-retrieval Predictors","author":["Ben He","Iadh Ounis"],"venue":"In String Processing and Information Retrieval","citeRegEx":"18","shortCiteRegEx":"18","year":2004},{"title":"Learning to Rewrite Queries","author":["Yunlong He","Jiliang Tang","Hua Ouyang","Changsung Kang","Dawei Yin","Yi Chang"],"venue":"In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management","citeRegEx":"19","shortCiteRegEx":"19","year":2016},{"title":"The Challenges of Moving fromWeb to Voice in Product Search","author":["Amir Ingber","Arnon Lazerson","Liane Lewin-Eytan","Alexander Libov","Eliyahu Osherovich"],"venue":"In Proc. 1st International Workshop on Generalization in Information Retrieval","citeRegEx":"20","shortCiteRegEx":"20","year":2018},{"title":"Offline vs. Online Evaluation in Voice Product Search","author":["Amir Ingber","Liane Lewin-Eytan","Alexander Libov","Yoelle Maarek","Eliyahu Osherovich"],"venue":"In 1st International Workshop on Generalization in Information Retrieval","citeRegEx":"21","shortCiteRegEx":"21","year":2018},{"title":"HowDo Users Respond to Voice Input Errors? Lexical and Phonetic Query Reformulation in Voice Search","author":["Jiepu Jiang","Wei Jeng","Daqing He"],"venue":"In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"22","shortCiteRegEx":"22","year":2013},{"title":"Optimizing search engines using clickthrough data","author":["Thorsten Joachims"],"venue":"In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data","citeRegEx":"23","shortCiteRegEx":"23","year":2002},{"title":"Query word deletion prediction","author":["Rosie Jones","Daniel C. Fain"],"venue":"In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"24","shortCiteRegEx":"24","year":2003},{"title":"Generating Query Substitutions","author":["Rosie Jones","Benjamin Rey","Omid Madani","Wiley Greiner"],"venue":"In Proceedings of the 15th International Conference on World Wide Web","citeRegEx":"25","shortCiteRegEx":"25","year":2006},{"title":"Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers (Valencia, Spain). Association for Computational Linguistics, 427–431","author":["Armand Joulin","Edouard Grave","Piotr Bojanowski","Tomas Mikolov"],"venue":"Session 8B: Multi-modal Retrieval and Ranking SIGIR","citeRegEx":"26","shortCiteRegEx":"26","year":2017},{"title":"Reducing long queries using query quality predictors","author":["Giridhar Kumaran","Vitor R. Carvalho"],"venue":"In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"28","shortCiteRegEx":"28","year":2009},{"title":"Relevance Based Language Models","author":["Victor Lavrenko","W. Bruce Croft"],"venue":"In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"29","shortCiteRegEx":"29","year":2001},{"title":"Jasper: An End-to-End Convolutional","author":["Jason Li","Vitaly Lavrukhin","Boris Ginsburg","Ryan Leary","Oleksii Kuchaiev","Jonathan M. Cohen","Huyen Nguyen","Ravi Teja Gadde"],"venue":"Neural Acoustic Model. CoRR","citeRegEx":"30","shortCiteRegEx":"30","year":2019},{"title":"Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion","author":["Liangda Li","Hongbo Deng","Anlei Dong","Yi Chang","Ricardo Baeza-Yates","Hongyuan Zha"],"venue":"In Proceedings of the 26th International Conference on World Wide Web","citeRegEx":"31","shortCiteRegEx":"31","year":2017},{"title":"Intent Term Weighting in E-Commerce Queries","author":["Saurav Manchanda","Mohit Sharma","George Karypis"],"venue":"In Proceedings of the 28th ACM International Conference on Information and Knowledge Management","citeRegEx":"32","shortCiteRegEx":"32","year":2019},{"title":"Latent Concept Expansion Using Markov Random Fields","author":["Donald Metzler","W. Bruce Croft"],"venue":"In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"33","shortCiteRegEx":"33","year":2007},{"title":"Similarity Measures for Short Segments of Text","author":["Donald Metzler","Susan Dumais","Christopher Meek"],"venue":"In Advances in Information Retrieval,","citeRegEx":"34","shortCiteRegEx":"34","year":2007},{"title":"Task-Oriented Query Reformulation with Reinforcement Learning","author":["Rodrigo Nogueira","Kyunghyun Cho"],"venue":"In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,","citeRegEx":"35","shortCiteRegEx":"35","year":2017},{"title":"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition","author":["Daniel S. Park","William Chan","Yu Zhang","Chung-Cheng Chiu","Barret Zoph","Ekin D. Cubuk","Quoc V. Le"],"venue":"CoRR abs/1904.08779","citeRegEx":"36","shortCiteRegEx":"36","year":2019},{"title":"Hanging on the Metaphone","author":["Lawrence Philips"],"venue":"Computer Language Magazine 7,","citeRegEx":"37","shortCiteRegEx":"37","year":1990},{"title":"The Probabilistic Relevance Framework: BM25 and Beyond","author":["Stephen E. Robertson","Hugo Zaragoza"],"venue":"Foundations and Trends in Information Retrieval 3,","citeRegEx":"38","shortCiteRegEx":"38","year":2009},{"title":"Mobile Query Reformulations","author":["Milad Shokouhi","Rosie Jones","Umut Ozertem","Karthik Raghunathan","Fernando Diaz"],"venue":"In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval","citeRegEx":"39","shortCiteRegEx":"39","year":2014},{"title":"User behavior in zerorecall ecommerce queries","author":["Gyanit Singh","Nish Parikh","Neel Sundaresan"],"venue":"In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"40","shortCiteRegEx":"40","year":2011},{"title":"Rewriting Null e- Commerce Queries to Recommend Products","author":["Gyanit Singh","Nish Parikh","Neel Sundaresan"],"venue":"In Proceedings of the 21st International Conference on World Wide Web","citeRegEx":"41","shortCiteRegEx":"41","year":2012},{"title":"User Intent, Behaviour, and Perceived Satisfaction in Product Search","author":["Ning Su","Jiyin He","Yiqun Liu","Min Zhang","Ma Shaoping"],"venue":"In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining","citeRegEx":"42","shortCiteRegEx":"42","year":2018},{"title":"Query Rewrite for Null and Low Search Results in eCommerce","author":["Zehong Tan","Canran Xu","Mengjie Jiang","Hua Yang","Xiaoyuan Wu"],"venue":"In Proceedings of the SIGIR Workshop On eCommerce","citeRegEx":"43","shortCiteRegEx":"43","year":2017},{"title":"Weakly Supervised Co-Training of Query Rewriting andSemantic Matching for e-Commerce","author":["Rong Xiao","Jianhui Ji","Baoliang Cui","Haihong Tang","Wenwu Ou","Yanghua Xiao","Jiwei Tan","Xuan Ju"],"venue":"In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining","citeRegEx":"44","shortCiteRegEx":"44","year":2019},{"title":"Query Expansion Using Local and Global Document Analysis","author":["Jinxi Xu","W. Bruce Croft"],"venue":"In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"45","shortCiteRegEx":"45","year":1996},{"title":"Improving verbose queries using subset distribution","author":["Xiaobing Xue","Samuel Huston","W. Bruce Croft"],"venue":"In Proceedings of the 19th ACM Conference on Information and Knowledge Management","citeRegEx":"46","shortCiteRegEx":"46","year":2010},{"title":"Term necessity prediction","author":["Le Zhao","Jamie Callan"],"venue":"In Proceedings of the 19th ACM Conference on Information and Knowledge Management","citeRegEx":"47","shortCiteRegEx":"47","year":2010}],"referenceMentions":[{"referenceID":26,"context":", [27, 30, 36]), they still have failures that exhibit different patterns (e.","startOffset":2,"endOffset":14},{"referenceID":32,"context":", [27, 30, 36]), they still have failures that exhibit different patterns (e.","startOffset":2,"endOffset":14},{"referenceID":8,"context":"In a pre-retrieval approach [11], QR happens without knowledge about the offers that return for the original query or any of the generated alternatives.","startOffset":28,"endOffset":32},{"referenceID":17,"context":"tation bias towards the top-ranked offer [20], in most cases, only a single offer is communicated back to the customer.","startOffset":41,"endOffset":45},{"referenceID":20,"context":"The main motivation for this approach comes from the fact that head queries are known to exhibit much better performance than tail queries, due to richer historical behavioral features [23].","startOffset":185,"endOffset":189},{"referenceID":17,"context":"[20] recently observed that products purchased through voice are much more limited in terms of diversity, namely, products purchased on a regular basis such as groceries, and not niche long-tail products.","startOffset":0,"endOffset":4},{"referenceID":37,"context":"queries to retrieve offers that capture the same intent as the originating query [41].","startOffset":81,"endOffset":85},{"referenceID":12,"context":"For example, it was observed that voice queries are closer to natural language than text queries in general search [15], voice reformulations are distinguishable from textual reformulations [17, 22], and that shopping categories and behavioral patterns defer between voice and web e-commerce search [20].","startOffset":115,"endOffset":119},{"referenceID":14,"context":"For example, it was observed that voice queries are closer to natural language than text queries in general search [15], voice reformulations are distinguishable from textual reformulations [17, 22], and that shopping categories and behavioral patterns defer between voice and web e-commerce search [20].","startOffset":190,"endOffset":198},{"referenceID":19,"context":"For example, it was observed that voice queries are closer to natural language than text queries in general search [15], voice reformulations are distinguishable from textual reformulations [17, 22], and that shopping categories and behavioral patterns defer between voice and web e-commerce search [20].","startOffset":190,"endOffset":198},{"referenceID":17,"context":"For example, it was observed that voice queries are closer to natural language than text queries in general search [15], voice reformulations are distinguishable from textual reformulations [17, 22], and that shopping categories and behavioral patterns defer between voice and web e-commerce search [20].","startOffset":299,"endOffset":303},{"referenceID":30,"context":"Most of the previous methods are not particularly suitable for e-commerce queries, which are shorter and more sensitive to context [34], let alone voice e-commerce Session 8B: Multi-modal Retrieval and Ranking SIGIR ’20, July 25–30, 2020, Virtual Event, China","startOffset":131,"endOffset":135},{"referenceID":10,"context":"Focusing on tail low-frequency e-commerce queries highlights additional unique challenges and opportunities, especially around finding and ranking good query alternatives [13].","startOffset":171,"endOffset":175},{"referenceID":12,"context":"Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21].","startOffset":100,"endOffset":112},{"referenceID":14,"context":"Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21].","startOffset":100,"endOffset":112},{"referenceID":19,"context":"Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21].","startOffset":100,"endOffset":112},{"referenceID":17,"context":"Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21].","startOffset":128,"endOffset":136},{"referenceID":18,"context":"Using voice as a new medium for search also reveals differences from traditional search in both web [15, 17, 22] and e-commerce [20, 21].","startOffset":128,"endOffset":136},{"referenceID":1,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":46,"endOffset":65},{"referenceID":21,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":46,"endOffset":65},{"referenceID":24,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":46,"endOffset":65},{"referenceID":42,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":46,"endOffset":65},{"referenceID":43,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":46,"endOffset":65},{"referenceID":4,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":82,"endOffset":93},{"referenceID":13,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":82,"endOffset":93},{"referenceID":22,"context":"Alternative queries are generated by dropping [4, 24, 28, 46, 47] or substituting [7, 16, 25] tokens from the original query.","startOffset":82,"endOffset":93},{"referenceID":22,"context":"[25] proposed generating query alternatives by using a large set of ordered query pairs obtained from consecutive queries in web-search sessions.","startOffset":0,"endOffset":4},{"referenceID":37,"context":"[41] suggested a post-retrieval approach for dropping terms from a given query that restricts the search results to the same taxonomy of results returned in the past for the original query.","startOffset":0,"endOffset":4},{"referenceID":39,"context":"[43] suggested generating sub-queries by dropping unimportant terms based on their part-of-speech (POS) tag and additional features.","startOffset":0,"endOffset":4},{"referenceID":3,"context":"Other ideas for substitution-based solutions, using the query-flow graph [6], were also proposed [7, 16].","startOffset":73,"endOffset":76},{"referenceID":4,"context":"Other ideas for substitution-based solutions, using the query-flow graph [6], were also proposed [7, 16].","startOffset":97,"endOffset":104},{"referenceID":13,"context":"Other ideas for substitution-based solutions, using the query-flow graph [6], were also proposed [7, 16].","startOffset":97,"endOffset":104},{"referenceID":13,"context":"However, finding good recommendations for tail e-commerce queries based on session co-occurrence turns to be difficult [16].","startOffset":119,"endOffset":123},{"referenceID":5,"context":"[8] conceptually extend the query-flow graph with term nodes in addition to query nodes.","startOffset":0,"endOffset":3},{"referenceID":6,"context":"[9] generate an inverted index of “successful” queries, i.","startOffset":0,"endOffset":3},{"referenceID":11,"context":"[14] proposed to use embedding techniques to expand a query via a k-nearest neighbor search.","startOffset":0,"endOffset":4},{"referenceID":16,"context":"[19] proposed a framework that learns to rewrite queries by unsupervised candidate generation and supervised candidate ranking.","startOffset":0,"endOffset":4},{"referenceID":40,"context":"[44] applied a similar technique based on post-retrieval method for e-commerce web search.","startOffset":0,"endOffset":4},{"referenceID":19,"context":"[22] showed that reformulation patterns of voice queries are different from those in conventional textual searches using both lexical and phonetic","startOffset":0,"endOffset":4},{"referenceID":14,"context":"[17] developed classifiers for distinguishing reformulation of voice query pairs from textual query pairs.","startOffset":0,"endOffset":4},{"referenceID":0,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":96,"endOffset":107},{"referenceID":27,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":96,"endOffset":107},{"referenceID":28,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":96,"endOffset":107},{"referenceID":7,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":158,"endOffset":174},{"referenceID":25,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":158,"endOffset":174},{"referenceID":29,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":158,"endOffset":174},{"referenceID":41,"context":"Approaches towards this task include learning rewritings based on past users’ query refinements [2, 31, 32] and applying pseudo-relevance feedback techniques [10, 29, 33, 45].","startOffset":158,"endOffset":174},{"referenceID":9,"context":"Those latter techniques commonly employ post-retrieval methods and iteratively query the search engine for newly added terms [12, 35].","startOffset":125,"endOffset":133},{"referenceID":31,"context":"Those latter techniques commonly employ post-retrieval methods and iteratively query the search engine for newly added terms [12, 35].","startOffset":125,"endOffset":133},{"referenceID":34,"context":"Then, given a query, the top retrieved queries are selected according to the Okapi BM25 scoring function [38] on the underlying terms.","startOffset":105,"endOffset":109},{"referenceID":33,"context":"In order to cope with such errors, we also employ few phonetic analyzers based on Double Metaphone phonetic encoding [37].","startOffset":117,"endOffset":121},{"referenceID":2,"context":"We generated a specialized word embedding trained using the FastText algorithm [5, 26] on a proprietary e-commerce corpus that included shopping queries, product titles, and a concatenation of queries with the title of the product that was purchased as result of them.","startOffset":79,"endOffset":86},{"referenceID":23,"context":"We generated a specialized word embedding trained using the FastText algorithm [5, 26] on a proprietary e-commerce corpus that included shopping queries, product titles, and a concatenation of queries with the title of the product that was purchased as result of them.","startOffset":79,"endOffset":86},{"referenceID":1,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":21,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":24,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":37,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":39,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":42,"context":"The first is a term-dropping based QR system, which is common in various usecases, especially when dealing with null queries [4, 24, 28, 41, 43, 46].","startOffset":125,"endOffset":148},{"referenceID":15,"context":", having high query scope measure [18, 28]).","startOffset":34,"endOffset":42},{"referenceID":24,"context":", having high query scope measure [18, 28]).","startOffset":34,"endOffset":42},{"referenceID":12,"context":"line of research [15, 17, 20, 22], highlighting differentiating factors between the voice and web domains, and supporting the need for specialized mechanisms for voice.","startOffset":17,"endOffset":33},{"referenceID":14,"context":"line of research [15, 17, 20, 22], highlighting differentiating factors between the voice and web domains, and supporting the need for specialized mechanisms for voice.","startOffset":17,"endOffset":33},{"referenceID":17,"context":"line of research [15, 17, 20, 22], highlighting differentiating factors between the voice and web domains, and supporting the need for specialized mechanisms for voice.","startOffset":17,"endOffset":33},{"referenceID":19,"context":"line of research [15, 17, 20, 22], highlighting differentiating factors between the voice and web domains, and supporting the need for specialized mechanisms for voice.","startOffset":17,"endOffset":33}],"year":2020,"abstractText":"Voice shopping using natural language introduces new challenges related to customer queries, like handling mispronounced, misexpressed, and misunderstood queries. Voice null queries, which result in no offers, have negative impact on customers shopping experience. Query rewriting (QR) attempts to automatically replace null queries with alternatives that lead to relevant results. We present a new approach for pre-retrieval QR of voice shopping null queries. Our proposed QR framework first generates alternative queries using a search index-based approach that targets different potential failures in voice queries. Then, a machine-learning component ranks these alternatives, and the original query is amended by the selected alternative. We provide an experimental evaluation of our approach based on data logs of a commercial voice assistant and an e-commerce website, demonstrating that it outperforms several baselines by more than 22%. Our evaluation also highlights an interesting phenomenon, showing that web shopping null queries are considerably different, and apparently easier to fix, than voice queries. This further substantiates the use of specialized mechanisms for the voice domain. We believe that our proposed framework, mapping tail queries to head queries, is of independent interest since it can be extended and applied to other domains.","creator":"Preview"}}
{"name":"3397271.3401204.pdf","metadata":{"source":"CRF","title":"Context-Aware TermWeighting For First Stage Passage Retrieval","authors":["Zhuyun Dai","Jamie Callan"],"emails":["zhuyund@cs.cmu.edu","callan@cs.cmu.edu"],"sections":[{"heading":null,"text":"KEYWORDS Deep Retrieval, Document Understanding, Term Weighting\nACM Reference Format: Zhuyun Dai and Jamie Callan. 2020. Context-Aware Term Weighting For First Stage Passage Retrieval . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401204"},{"heading":"1 INTRODUCTION","text":"State-of-the-art search engines use ranking pipelines in which an efficient first stage uses a query to fetch an initial set of documents, and one or more re-ranking algorithms to improve and prune the ranking. Typically the first stage ranker is bag-of-words retrieval model that use term frequency (tf ) to determine the documentspecific importance of terms. However, tf does not necessarily indicate whether a term essential to the meaning of the document, especially when the frequency distribution is flat, e.g., passages. In essence, tf ignores the interactions between a term and its text context, which is key to estimating document-specific term weights.\nThis paper seeks to improve term importance estimation in firststage retrieval models by using deep language models like BERT [3] to capture a term’s contextual features. We present the Deep Contextualized Term Weighting framework (DeepCT). DeepCT learns a contextualized term representation model based on BERT, and a mapping function from representations to term weights. The transformer encoder of BERT allows DeepCT to capture semantic and syntactic features from a term’s linguistic context, helping DeepCT to identify semantically important terms from the text. DeepCT\nSIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401204\ngenerates deep, context-aware document-specific term weights that can replace the standard tf.\nThis paper also present a novel approach that runs DeepCT at offline index time, making it possible to use it in first-stage retrieval where efficiency is crucial. Our approach applies DeepCT over each passage in the corpus, and stores the context-aware term weights in an ordinary inverted index to replace tf. The index can be searched efficiently using common bag-of-words retrieval models such as BM25 or statistical query likelihood models.\nExperiments demonstrate that DeepCT significantly improves the accuracy of first-stage retrieval. More accurate first-stage document rankings also provide better candidates for downstream reranking, and improves end-to-end accuracy and/or efficiency. Analysis shows that DeepCT’s main advantage is the ability to differentiate between key terms and other frequent but non-central terms. Code and data are made public available 1."},{"heading":"2 RELATED WORK","text":"Document Term Weighting. Most first-stage retrieval models such as BM25 and query likelihood use term frequencies (tf ) to term importance in a document. A popular alternative to tf are graphbased methods, e.g., TextRank [6]. A few recent work investigated using word embeddings [5] for document term weighting, but most of them only learn a global idf -like term weight because the word embeddings are context-independent. Our work aims to learn tf - like term weights that are context-specific.\nNeural Approaches for First Stage Ranking. Most neural ranking models are cost-prohibitive to be used in the first stage [1, 2, 10]. Recent research addresses this efficiency problem in two ways. One way is to learn latent embedding representations of queries and documents [13]. However, fix-dimension representations introduce the specificity vs. exhaustiveness trade-off [14]. Another approach modifies the bag-of-words document representations using neural network. Mitra et al. [8] uses neural rankers to generate term-document scores, but it is time-consuming when applied at a large-scale. Nogueira et al. [11] proposed to generate queries from documents using neural machine translation and index queries as document expansion terms [11, 12]. Our work are orthogonal to [11, 12] – their approaches add terms to documents, while our method weights existing terms; future work may consider combining the two approaches."},{"heading":"3 DeepCT","text":"DeepCT Framework DeepCT leverages the transformer encoder of BERT to extract a word’s contextual features. In the transformer, a term gradually absorbs contextual information based on its attention to every other term in the same text. At the last layer, the transformer generates a contextualized term embedding for every\n1https://github.com/AdeDZY/DeepCT\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nterm in the input text, which can be viewed as a feature vector that characterizes the term’s syntactic and semantic role in a given context.\nDeepCT then linearly combines a term’s contextualized embedding into a term importance score:\nŷt ,d = ®wTt ,d + b (1)\nwhere Tt ,d is term t ’s contextualized embedding in text d ; and, ®w and b are the linear combination weights and bias.\nTrain DeepCT DeepCT is trained end-to-end using a per-token regression task. Given the ground truth term weight for every word in text d , denoted as {y1,d , . . . ,yn,d }, DeepCT aims to minimize the mean square error (MSE) between the predicted weights ŷ and the ground truth weights y:\nlossMSE = ∑ d ∑ t (yt ,d − ŷt ,d )2. (2)\nIt is an open question how to obtain the ground truth term weights y. As search queries can reflect the key idea of a document, we propose query term recall as an estimation of the ground truth term weights:\nyt ,d = QTR(t,d) = |Qd ,t | |Qd | . (3)\nQd is the set of queries for which passage d is judged relevant.Qd ,t is the subset of Qd that contains term t . In other words, QTR(t,d) is the percentage of d’s queries that mention term t .\nIndex with DeepCT The queries are only used to generate training labels. During testing, the model is query-independent – the prediction is solely based on the document content. This allows estimated term weights to be calculated and stored during offline indexing.\nWe run the trained DeepCT model over all passages in the collection. Most predictions fall into 0-1 (because the ground truth weights are in 0-1, as shown in Eq.3). The predicted weights are then scaled into an integer that can be used with existing retrieval models. We call the scaled weight tfDeepCT to convey that it is an alternate way of representing the importance of term t in d :\ntfDeepCT(t,d) = round(ŷt ,d ∗ N ). (4)\nN scales the predicted weights into a integer range. We use N = 100 in this work as two digits of precision is sufficient for this task. Terms with negative weights are discarded.\ntfDeepCT is used to replace the original tf in the inverted index. The new index, DeepCT-Index, can be searched by mainstream bag-of-words retrieval model like BM25 or query likelihood model. The context-aware term weight tfDeepCT is expect to bias the retrieval models to central terms in the pasessag, preventing off-topic passages being retrieved.\nEfficiency The main difference between DeepCT-Index and a typical inverted index is that the term weight is based on tfDeepCT instead of tf. This calculation is done offline. No new posting lists are created, thus the query latency does not become longer. To the contrary, a side-effect of Eq 4 is that tfDeepCT of some terms becomes negative, which may be viewed as a form of index pruning. We leave that aspect of this work for future investigation."},{"heading":"4 EXPERIMENTAL METHODOLOGY","text":"Datasets used MSMARCO [9] and TREC-CAR [4].MSMARCO is a passage retrieval dataset with 8.8M passages [9]. The training set contains approximately 0.5M pairs of queries and relevant passages. The development (dev) set contains 6,980 queries and their relevance labels. The test set contains 6,900 queries, but the relevance labels are hidden byMicrosoft. Therefore, the dev set is our main evaluation set. In a few experiments, we also evaluated on the test set by submitting our rankings to the MS MARCO competition. TRECCAR [4] consists of 29.7M English Wikipedia passages. Following prior work [10, 11], we use the automatic relevance judgments. The training set have 3.3M query-passage pairs. The test set contains 1,860 queries.\nFirst-Stage Retrieval Baselines. We compare DeepCT term weights with three popular term weighting methods used in firststage retrieval.\n• tf uses standard term frequency weights, e.g., as used by BM25. • TextRank [6] is a widely-used graph-based term weighting approach. We use the open source PyTextRank implementation2. Term weights from TextRank are in the range (0, 1); we scale them to integers as described in Eq. 4 for indexing. • Doc2Query [11] is a supervised baseline. It trains a neural sequence-to-sequence model to generate potential queries from passages, and indexes the queries as document expansion terms. We use the Doc2Query MS MARCO index released by the authors. No such index is available for TRECCAR, so we use published values for that dataset [11].\nWe used the Anserini toolkit to index documents using the above three term weights as well as the proposed DeepCT term weights. First-stage ranking was done by two popular retrieval models: BM25 and query likelihood with Jelinek-Mercer smoothing (QL). We finetuned BM25 parameters k1 andb, and QL smoothing factor λ through a parameter sweep on 500 queries from the training set.\nThe transformer of DeepCTwas initializedwith pre-trained BERT parameters. For MS MARCO, we used the official pre-trained BERT (uncased, base model) [3]. For TREC-CAR, we follow Nogueira and Cho [10] and used a pre-trained BERT model released by the authors. DeepCT was trained for 3 epochs on the training split of our datasets, using a learning rate of 2e−5 and a max input text length of 128 tokens.\nEvaluation used MRR@10, the official MS MARCO evaluation metric. For TREC-CAR, we also reportMAP at depth 1,000 following the evaluation methodology used in previous work [10, 11]."},{"heading":"5 EXPERIMENTAL RESULTS","text":"Three experiments investigate DeepCT’s first-stage retrieval accuracy, its impacts on down-stream rerankers, and why DeepCT term weights are effective."},{"heading":"5.1 First-Stage Retrieval using DeepCT","text":"The first experiment examines whether DeepCT improves first-stage retrieval accuracy over baseline retrieval methods. It also compares DeepCT to several supervised rerankers.\n2https://github.com/DerwenAI/pytextrank\nComparison to First-stageRetrieval Baselines. Table 1 shows the first-stage retrieval accuracy of BM25 and QL using indexes generated by four methods. The TextRank index failed to beat the common tf index. The Doc2Query index was effective for MS MARCO, but only marginally better for TREC-CAR. On the other hand, DeepCT outperformed the baselines by large margins. It improved BM25 by 27% on MS MARCO and 46% on TREC-CAR. It produced similar gains for QL, showing that DeepCT is useful to different retrieval models. DeepCT-Index also surpassed Doc2Query by large margins, which also used deep neural networks and supervised training.\nIt is uncommon in prior research for a non-tf term weighting method to generate such substantially better rankings. These results show that tf is no longer sufficient, and that better term importance signals can be generated with deep document understanding.\nComparision to SupervisedRerankers.We further compared the single-stage DeepCT retrieval to several multi-stage reranking pipelines. Table 2 shows results from the MS MARCO leaderboard4. It lists representative reranking approaches for featurebased learning-to-rank, previous state-of-the-art neural rerankers (non-ensemble versions), and BERT-based rerankers. All rerankers used the top 1,000 passages from BM25.\nA single-stage BM25 retrieval from DeepCT-Index was better than several reranking pipelines. It is more accurate than featurebased LeToR, a widely used reranking approach in modern search engines. It is also more accurate than a popular neural reranking\n3Statistical significance is unknown because the MS MARCO website publishes only summary results. 4http://www.msmarco.org/leaders.aspx\nmodel K-NRM [15]. Compared to Duet V2 (the official reranking baseline) and Conv-KNRM [2], DeepCT achieves similar accuracy while being more efficient as it does not need the reranking stage. The results demonstrate that it is possible to move some of the complex ranking process to offline analysis, building deep yet simple text representations that can be retrieved very efficiently.\nFinally, strong neural rerankers like the BERT reranker were much more effective than DeepCT BM25. The next section studies whether the two approaches can be used together.\n5.2 Combine DeepCT with Rerankers This experiment examines whether a first-stage ranking produced by DeepCT BM25 can be used together with later-stage rerankers to improve end-to-end performance. Table 3 reports the performance of two rerankers applied to candidate passages retrieved from the standard tf and the DeepCT index. The two rerankers were Conv-KNRM [2], which has medium accuracy, and BERT ReRanker [10], which has high accuracy. We tested various reranking depths. Reranking at a shallower depth has higher efficiency but may miss more relevant passages.\nThe recall values show the percentage of relevant passages in the reranking passage set. DeepCT had higher recall at all depths, meaning a ranking from DeepCT provided more relevant passages to a reranker. Both rerankers consistently achieved higher MRR@10 by using DeepCT compared to using tf. For Conv-KNRM, the best\n4We did not run this experiment on MS MARCO test set because test set results can only be evaluated by submitting to the MS MARCO competition. The organizers discourage too many submission from the same group to avoid \"P-hacking\".\nMRR@10 improved from 0.256 to 0.278, and the required reranking depth decreased from 100 to 50. For BERT ReRanker, DeepCT enabled it to achieve similar accuracy using much fewer passages. Reranking the top 100-200 passages from DeepCT produced similar MRR@10 as reranking the top 1,000 passages from tf index, meaning that the reranker can be 5-10× more efficient.\nIn summary, DeepCT puts relevant passages at the top, so that downstream rerankers can achieve similar or higher accuracy with much smaller candidate sets, leading to lower computational cost in the retrieval pipeline."},{"heading":"5.3 Sources of Effectiveness","text":"The last experiment aims to understand the sources of effectiveness of DeepCT-Index through several analyses.\nTable 4 visualizes DeepCTweights. It shows that DeepCT is able to emphasize central terms and suppress non-central terms. Non-central terms are assigned with low weight even they are frequent. For example, in the off-topic passage, “DNA” has low DeepCT weight even though it is mentioned 3 times. On the other hand, “Genomics” has higher weight even though is is mentioned fewer times. This extent of independence from frequency signals is uncommon in previous term weighting approaches.\nFigure 1 compares the termweight distribution of DeepCT-Index and tf index.The original tf distribution is flat. DeepCT-Index assigns high weights to a few central terms, resulting in a skewed term weight distribution. Such skewed distribution confirms our observations from the case study that DeepCT-Index aggressively emphasizes a few central terms and supresses the others."},{"heading":"6 CONCLUSION","text":"Most first-stage rankers are efficient bag-of-words retrieval models that use term frequency signals. This paper presents DeepCT, a deep learning approach that better estimates term importance for first stage retrieval. DeepCT uses the transformer encoder of BERT to capture contextual features of words, and maps the features into document-specific term weights. Trained on a supervised per-token regression task, DeepCT is capable of producing context-aware term weights that reflect the essential meanings of the document. Importantly, DeepCT moves the neural document processing to the offline indexing time – the term weights can be stored in a typical inverted index and used with efficient retrieval models such as BM25.\nExperimental results show that DeepCT improves the accuracy of popular first-stage retrieval algorithms by up to 40%. Running BM25 on DeepCT-Index can be as effective as several previous state-ofthe-art rankers that need to run slow deep learning models at the query time. The higher-quality ranking enabled by DeepCT-Index improves the accuracy/efficiency tradeoff for later-stage re-rankers. Analysis shows that DeepCT is capable of finding the central words in a text even if they are mentioned only once. We view DeepCT as an encouraging step from “frequencies” to “ meanings”.\nFor several decades, first-stage retrieval models have relied on term frequency signals (tf ). Results from this paper indicate that tf is no longer sufficient. With recent advances in deep learning and NLP, it is time to revisit the indexers and retrieval models, towards building new deep and efficient first stage rankers.\nAcknowledgments. This work was supported by the National Science Foundation (NSF) grant IIS-1815528."}],"references":[{"title":"Deeper Text Understanding for IR with Contextual Neural Language Modeling","author":["Zhuyun Dai","Jamie Callan"],"venue":"In Proc. SIGIR","citeRegEx":"1","shortCiteRegEx":"1","year":2019},{"title":"Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search","author":["Zhuyun Dai","Chenyan Xiong","Jamie Callan","Zhiyuan Liu"],"venue":"In Proc. WSDM","citeRegEx":"2","shortCiteRegEx":"2","year":2018},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"venue":"In Proc. ACL","citeRegEx":"3","shortCiteRegEx":"3","year":2019},{"title":"TREC complex answer retrieval overview","author":["Laura Dietz","Manisha Verma","Filip Radlinski","Nick Craswell"],"venue":"In Proc. TREC","citeRegEx":"4","shortCiteRegEx":"4","year":2017},{"title":"A Deep Relevance Matching Model for Ad-hoc Retrieval","author":["Jiafeng Guo","Yixing Fan","Qingyao Ai","andW. Bruce Croft"],"venue":"In Proc. CIKM","citeRegEx":"5","shortCiteRegEx":"5","year":2016},{"title":"Textrank: Bringing order into text","author":["Rada Mihalcea","Paul Tarau"],"venue":"In Proc. EMNLP","citeRegEx":"6","shortCiteRegEx":"6","year":2004},{"title":"An Updated Duet Model for Passage Re-ranking","author":["Bhaskar Mitra","Nick Craswell"],"venue":"arXiv preprint arXiv:1903.07666","citeRegEx":"7","shortCiteRegEx":"7","year":2019},{"title":"Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks","author":["Bhaskar Mitra","Corby Rosset","David Hawking","Nick Craswell","Fernando Diaz","Emine Yilmaz"],"venue":null,"citeRegEx":"8","shortCiteRegEx":"8","year":2019},{"title":"2016. MS MARCO: A human generated machine reading comprehension","author":["Tri Nguyen","Mir Rosenberg","Xia Song","Jianfeng Gao","Saurabh Tiwary","Rangan Majumder","Li Deng"],"venue":null,"citeRegEx":"9","shortCiteRegEx":"9","year":2016},{"title":"Passage Re-ranking with BERT","author":["Rodrigo Nogueira","Kyunghyun Cho"],"venue":null,"citeRegEx":"10","shortCiteRegEx":"10","year":2019},{"title":"Document Expansion by Query Prediction","author":["Rodrigo Nogueira","Kyunghyun Cho","Yang Wei","Lin Jimmy"],"venue":null,"citeRegEx":"11","shortCiteRegEx":"11","year":2019},{"title":"Introduction to Modern Information Retrieval","author":["Gerard Salton","Michael McGill"],"venue":null,"citeRegEx":"14","shortCiteRegEx":"14","year":1984},{"title":"End-to-End Neural Ad-hoc Ranking with Kernel Pooling","author":["Chenyan Xiong","Zhuyun Dai","Jamie Callan","Zhiyuan Liu","Russell Power"],"venue":"In Proc. SIGIR. Short Research Papers I SIGIR","citeRegEx":"15","shortCiteRegEx":"15","year":2017}],"referenceMentions":[{"referenceID":2,"context":"This paper seeks to improve term importance estimation in firststage retrieval models by using deep language models like BERT [3] to capture a term’s contextual features.","startOffset":126,"endOffset":129},{"referenceID":4,"context":"A few recent work investigated using word embeddings [5] for document term weighting, but most of them only learn a global idf -like term weight because the word embeddings are context-independent.","startOffset":53,"endOffset":56},{"referenceID":0,"context":"Most neural ranking models are cost-prohibitive to be used in the first stage [1, 2, 10].","startOffset":78,"endOffset":88},{"referenceID":1,"context":"Most neural ranking models are cost-prohibitive to be used in the first stage [1, 2, 10].","startOffset":78,"endOffset":88},{"referenceID":9,"context":"Most neural ranking models are cost-prohibitive to be used in the first stage [1, 2, 10].","startOffset":78,"endOffset":88},{"referenceID":7,"context":"[8] uses neural rankers to generate term-document scores, but it is time-consuming when applied at a large-scale.","startOffset":0,"endOffset":3},{"referenceID":10,"context":"[11] proposed to generate queries from documents using neural machine translation and index queries as document expansion terms [11, 12].","startOffset":0,"endOffset":4},{"referenceID":10,"context":"[11] proposed to generate queries from documents using neural machine translation and index queries as document expansion terms [11, 12].","startOffset":128,"endOffset":136},{"referenceID":10,"context":"Our work are orthogonal to [11, 12] – their approaches add terms to documents, while our method weights existing terms; future work may consider combining the two approaches.","startOffset":27,"endOffset":35},{"referenceID":8,"context":"Datasets used MSMARCO [9] and TREC-CAR [4].","startOffset":22,"endOffset":25},{"referenceID":3,"context":"Datasets used MSMARCO [9] and TREC-CAR [4].","startOffset":39,"endOffset":42},{"referenceID":9,"context":"Following prior work [10, 11], we use the automatic relevance judgments.","startOffset":21,"endOffset":29},{"referenceID":10,"context":"Following prior work [10, 11], we use the automatic relevance judgments.","startOffset":21,"endOffset":29},{"referenceID":5,"context":"• TextRank [6] is a widely-used graph-based term weighting approach.","startOffset":11,"endOffset":14},{"referenceID":10,"context":"• Doc2Query [11] is a supervised baseline.","startOffset":12,"endOffset":16},{"referenceID":10,"context":"No such index is available for TRECCAR, so we use published values for that dataset [11].","startOffset":84,"endOffset":88},{"referenceID":2,"context":"For MS MARCO, we used the official pre-trained BERT (uncased, base model) [3].","startOffset":74,"endOffset":77},{"referenceID":9,"context":"For TREC-CAR, we follow Nogueira and Cho [10] and used a pre-trained BERT model released by the authors.","startOffset":41,"endOffset":45},{"referenceID":9,"context":"For TREC-CAR, we also reportMAP at depth 1,000 following the evaluation methodology used in previous work [10, 11].","startOffset":106,"endOffset":114},{"referenceID":10,"context":"For TREC-CAR, we also reportMAP at depth 1,000 following the evaluation methodology used in previous work [10, 11].","startOffset":106,"endOffset":114},{"referenceID":1,"context":"Compared to Duet V2 (the official reranking baseline) and Conv-KNRM [2], DeepCT achieves similar accuracy while being more efficient as it does not need the reranking stage.","startOffset":68,"endOffset":71},{"referenceID":1,"context":"The two rerankers were Conv-KNRM [2], which has medium accuracy, and BERT ReRanker [10], which has high accuracy.","startOffset":33,"endOffset":36},{"referenceID":9,"context":"The two rerankers were Conv-KNRM [2], which has medium accuracy, and BERT ReRanker [10], which has high accuracy.","startOffset":83,"endOffset":87}],"year":2020,"abstractText":"Term frequency is a commonmethod for identifying the importance of a term in a document. But term frequency ignores how a term interacts with its text context, which is key to estimating documentspecific term weights. This paper proposes a Deep Contextualized Term Weighting framework (DeepCT) that maps the contextualized term representations fromBERT to into context-aware termweights for passage retrieval. The new, deep term weights can be stored in an ordinary inverted index for efficient retrieval. Experiments on two datasets demonstrate that DeepCT greatly improves the accuracy of first-stage passage retrieval algorithms.","creator":"LaTeX with hyperref"}}
{"name":"3397271.3401467.pdf","metadata":{"source":"META","title":"Deep Reinforcement Learning for Information Retrieval:Fundamentals and Advances","authors":["Weinan Zhang","Shanghai Jiao Tong","Xiangyu Zhao","Li Zhao","Dawei Yin","Grace Hui Yang","Alex Beutel"],"emails":["wnzhang@sjtu.edu.cn","zhaoxi35@msu.edu","lizo@microsoft.com","yindawei@acm.org","huiyang@cs.georgetown.edu","alexbeutel@google.com","permissions@acm.org."],"sections":[{"heading":null,"text":"KEYWORDS Deep Reinforcement Learning, Information Retrieval\nACM Reference Format: Weinan Zhang, Xiangyu Zhao, Li Zhao, Dawei Yin, Grace Hui Yang, and Alex Beutel. 2020. Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401467"},{"heading":"1 BACKGROUND AND MOTIVATIONS","text":"Recent years have witnessed the increased popularity and explosive growth of the World Wide Web, which has generated huge amounts of data and leaded to progressively severe information\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401467\noverload problem [4]. In consequence, how to extract information (products or services) that satisfy users’ information requirements at the proper time and place has become increasingly important. This motivates several information retrieval mechanisms, such as search, recommendation, and online advertising. These retrieval mechanisms could generate a set of objects that best match users’ explicit or implicit preferences [10]. Efforts have been made on developing supervised or unsupervised methods for these information retrieval mechanisms [21]. However, since the widely use of mobile applications during the recent years, more and more information retrieval services have provided interactive functionality and products [41], these conventional techniques typically face several common challenges. First, most of traditional approaches consider information retrieval tasks in a static environment and extract information via a fixed greedy strategy, whichmay fail to catch the dynamic characteristics of users’ preferences (or environment). Second, the majority of existing methods aims to maximize user’s immediate satisfaction, while completely overlooking whether the generate information can benefit more to user’s preference in the long run [24]. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning [27].\nDriven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14]. By integrating deep learning into reinforcement learning, DRL is not only capable of continuing sensing and learning to act, but also capturing complex patterns with the power of deep learning. Under the DRL schema, complex problems are addressed by acquiring experiences through interactions with a dynamic environment. The result is an optimal policy that can provide decision making solutions to complex tasks without any specific instructions [13]. Introducing DRL to information retrieval community can naturally tackle the above-mentioned challenges. First, DRL considers information retrieval tasks as sequential interactions between an RL agent (system) and users (environment), where the agent continuously update the information retrieval strategies based on users’ real-time feedback, so as to generate information best match users’ dynamic preferences. Second, the DRL-based techniques targets to optimize users’ long-term satisfaction or engagement. Therefore, the agent could identify information to achieve the trade-off between users’ short-term and long-term satisfaction.\nGiven the advantages of reinforcement learning, there have been tremendous interests in developing RL based information retrieval techniques [3, 5, 11, 12, 15, 38–40]. While these successes show the promise of DRL, applying learning from game-based DRL to information retrieval is fraught with unique challenges, including, but not limited to, extreme data sparsity, power-law distributed samples, and large state and action spaces.\nTherefore, in this workshop, we will provide a venue for both academia researchers and industry practitioners to discuss the fundamental principles, technical and practice limitations, and observations and lessons learned from applications of DRL for (interactive) information retrieval. We also aim to foster research on novel information retrieval algorithms, techniques and applications of DRL."},{"heading":"2 REVIEW OF EXISTINGWORK","text":"In this section, we briefly review related work of deep reinforcement learning in main IR scenarios i.e., search, recommendation and online advertising.\nSearch targets at retrieving and ranking a set of items (e.g. documents, records) according to a user query [7, 29]. For query understanding, Nogueira et al. [19] proposed a reinforcement learning based query reformulation task, which maximizes the number of recalled relevant documents via rewriting a query. In [18], a multiagent reinforcement learning framework is proposed to increase the diverse query reformulation efficiency, where each agent learns a local policy that performs well on a subset of examples, and all agents are trained with parallelism to make the learning faster. For relevance ranking, conventional methods typically optimize the evaluation metric before a predefined position (e.g. NDCG@K), which ignores the information after rank K . MDPRank [31] is proposed to address this problem by using the metrics calculated upon all the positions as reward function, and the model parameters are be optimized via maximizing the accumulated rewards for all decisions. Beyond relevance ranking, another important goal is to increase the diversity of search results [8, 22], which needs to capture the utility of information users have perceived from the preceding documents in a sequential document selection. MDPDIV [34] formalized diverse ranking as a continuous state Markov decision process, and policy gradient algorithm of REINFORCE is leveraged to maximize the accumulated long-term rewards in terms of the diversity metric.\nRecommender systems aim to learn users’ preferences based on their feedback and suggest items to match their preferences. User’s preference is assumed to be static in traditional recommendation algorithms such as collaborative filtering, which is usually not true in real-world recommender systems where users’ preferences are highly dynamic. Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions). Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over time. In [39], a user’s dynamic preference (state) is learned from her browsing history and feedback. Each time a user provides feedback (skip, click or purchase) to an item, the recommender systemwill update the state to capture user’s new preferences. Conventional recommender algorithms also suffer from the exploitation-exploration\ndilemma, where exploitation is to suggest items that best match users’ preferences, while exploration is to randomly suggest items to mine more users’ possible preferences. The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ϵ-greedy [30], EXP3 [2], and UCB1 [1].\nOnline advertising is to suggest the right ads to the right users so as to maximize the click-through rate (CTR) or return on investment (ROI) of the advertising campaign, which consists of two main marketing strategy, i.e., guaranteed delivery (GD) and real-time bidding (RTB). In guaranteed delivery setting, ads that grouped into campaigns are charged on a pay-per-campaign basis for the pre-specified number of deliveries [23]. A multi-agent reinforcement learning approach [32] is proposed to derive cooperative policies for the publisher, where impression allocation problem is formulated as an auction problem, and publishers can submit virtual bids for impressions. In Real-Time Bidding setting, an advertiser submits a bid for each impression in a very short time frame. The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out. Thus, the MDP setting is introduced. For example, a model-based RL framework is proposed in RTB setting [3], where the state value is approximated by neural network to address the scalability problem of large auction amounts and the limited budget. In [12], a multi-agent bidding model is proposed to jointly consider all the advertisers’ biddings in the system, and a clustering approach is introduced to deal with a large number of advertisers."},{"heading":"3 KEY CHALLENGES AND OPEN QUESTIONS","text":"While there has been a plenty of research work on deep reinforcement learning for information retrieval published during the recent years, several problems are still unsolved or remain as key challenges. Here we list some of them for broad discussion.\nSample Efficiency. Sample efficiency has been of key problem for DRL since most DRL methods are model-free. Given sufficient training data from the agent interacting with the environment, the paradigm of model-free RL is suitable for training deep neural network based value functions or policies. However, most IR systems directly interact with the users and collect the training data, which is normally insufficient to train the model-free RL solutions.\nSparse & Biased Feedback Data. Feedback data is commonly biased. In RL view, the experience data is sampled from the occupancy measure distribution of the policy-environment interaction. Although off-policy training method can still help improve the policy based on biased data, the sample efficiency is seriously reduced. Moreover, in some IR scenarios such as online advertising, the positive reward is highly sparse (e.g., 0.3% click-through rate for display ads), which results in very low efficiency or failure of RL.\nOnline Deployment. From industry perspective, deploying DRL solution onto production IR platform is challenging. The common model pipelines in IR platforms center on relevance estimation models, e.g., relevance or CTR estimation, while the DRL model pipelines center on the policy module. Bridging gap between two\ngenerations of model pipelines should be positioned as high priority for applied research team in this field."},{"heading":"4 PROGRAM SKETCH","text":""},{"heading":"4.1 Workshop Format","text":"The workshop is planned to be host a whole day, with 2 keynotes, 4 invited talks and 6 oral research talks. The keynote speakers should be well-recognized professors or scientists working on the area. There are two encouraged types of invited talks and peer reviewed oral research talks: (i) the academic talk on fundamental research on reinforcement learning with an attempt of application on IR; (ii) the industrial talk on practice of designing or applying deep reinforcement learning techniques for real-world IR tasks.\nEach talk is expected to be presented as a lecture with slides. There will be a QA session at the end of each talk."},{"heading":"4.2 Online Materials","text":"A website (http://drl4ir.github.io) for this workshop will be made available online right before the lecture is presented. All the relevant materials will be made available on this website, including the talk information, presentation slides, referred papers, speaker information and related open source projects etc."},{"heading":"5 RELATEDWORKSHOPS","text":"The Deep Reinforcement Learning Workshop at NeurIPS (2015- 2019)1 and IJCAI (2016)2 focused on the techniques to combine neural networks with reinforcement learning, and domains like robotics, strategy games, and multi-agent interaction. The Deep Reinforcement Learning Meets Structured Prediction at ICLR (2019)3 focused on leveraging reinforcement learning paradigm on tasks of structured predictions. The workshops at ICML (2019)4 and KDD (2019)5 focus on a wide range of real life reinforcement learning applications. The proposedworkshop is the first to focus on deep reinforcement learning for information retrieval. This workshop will bring together experts in information retrieval and reinforcement learning. Our proposed workshop will have invited keynotes and talks, paper presentation, poster session, and panel discussion to help interested researchers gain a high-level view about the current state of the art and potential directions for future contributions. Real datasets and codes will also be released for attendees to practice in the future."},{"heading":"6 ORGANIZERS INFORMATION AND QUALIFICATION","text":"Dr. Weinan Zhang, the workshop lead organizer, is currently a tenure-track associate professor in Shanghai Jiao Tong University. His research interests include machine learning and big data mining, particularly, deep learning and reinforcement learning techniques for real-world data mining scenarios, such as computational advertising, recommender systems, text mining, web search and\n1https://sites.google.com/view/deep-rl-workshop-neurips-2019/home 2https://sites.google.com/site/deeprlijcai16/ 3https://sites.google.com/view/iclr2019-drlstructpred/ 4https://sites.google.com/view/RL4RealLife 5http://www.cse.msu.edu/~zhaoxi35/DRL4KDD/\nknowledge graphs. He has published over 80 papers on first-tier international conferences and journals, including KDD, SIGIR, ICML, ICLR, JMLR, IJCAI, AAAI,WSDM, CIKM etc. He won the Best Paper Honorable Mention Award in SIGIR 2017, the Best Paper Award in DLP Workshop in KDD 2019, ACM Rising Star Award, Alibaba DAMO Young Scholar Award etc. Weinan has organized workshops and tutorials in SIGIR, KDD, CIKM and ECIR etc.\nXiangyu Zhao is a senior Ph.D. student of computer science and engineering at Michigan State University (MSU). His supervisor is Dr. Jiliang Tang. Before joining MSU, he completed his MS (2017) at USTC and BS (2014) at UESTC. He is the student member of IEEE, SIGIR, and SIAM. His current research interests include data mining and machine learning, especially (1) Reinforcement Learning and AutoML for E-commerce; (2) Urban Computing and Spatio-Temporal Data Analysis. After joiningMSU, he has published his work in top journals (e.g. SIGKDD, SIGWeb) and conferences (e.g., KDD, SIGIR, CIKM, ICDM, RecSys). He was the recipients of the KDD’18/19, RecSys’18, SDM’18, and CIKM’17 Student Travel Award.\nDr. Li Zhao is currently a Senior Researcher in Machine Learning Group, Microsoft Research Asia (MSRA). Her research interests mainly lie in deep learning and reinforcement learning, and their applications for text mining, recommendation, finance and games. She has co-organized the 3rd Asian Workshop on Reinforcement Learning (AWRL’18), and is one of the invited speakers for AWRL’19. She obtained her Ph.D. degree majoring in Computer Science in July, 2016, from Tsinghua University, supervised by Professor Xiaoyan Zhu. During her Ph.D. studies, she has conducted research on sentiment extraction, text mining and weakly supervised learning. She published several research papers in top conferences, including NeurIPS, KDD, IJCAI, AAAI, EMNLP and CIKM.\nDr. Dawei Yin is Engineering Director at Baidu inc.. He is managing the search science team at Baidu, leading BaiduâĂŹs science efforts of web search, question answering, video search, image search, news search, app search, etc.. Previously, he was Senior Director, managing the recommendation engineering team at JD.com between 2016 and 2020. Prior to JD.com, he was Senior Research Manager at Yahoo Labs, leading relevance science team and in charge of Core Search Relevance of Yahoo Search. He obtained Ph.D. (2013), M.S. (2010) from Lehigh University and B.S. (2006) from Shandong University. From 2007 to 2008, he was an M.Phil. student in The University of Hong Kong. His research interests include data mining, applied machine learning, information retrieval and recommender system. He published more than 80 research papers in premium conferences and journals, and was the recipients of WSDM2016 Best Paper Award, KDD2016 Best Paper Award, WSDM2018 Best Student Paper Award, and ICHI 2019 Best Paper Honorable Mention.\nDr. Grace Hui Yang is an Associate Professor in the Department of Computer Science at Georgetown University. Dr. Yang is leading the InfoSense (Information Retrieval and Sense-Making) group at Georgetown University, Washington D.C., U.S.A. Dr. Yang obtained her Ph.D. from the Language Technologies Institute, Carnegie Mellon University in 2011. Dr. Yang’s current research interests include deep reinforcement learning, dynamic information retrieval, search\nengine evaluation, privacy-preserving information retrieval, internet of things, and information organization. Prior to this, she has conducted research on question answering, ontology construction, near-duplicate detection, multimedia information retrieval, and opinion and sentiment detection. Dr. Yang has co-chaired SIGIR 2013 and 2014 Doctoral Consortiums, SIGIR 2017Workshop,WSDM 2017 Workshop, ICTIR 2017 Workshop, CIKM 2015 Tutorial, ICTIR 2018 Short Paper and SIGIR 2018 Demonstration Paper Program Committees. Dr. Yang served on the editorial board of Information Retrieval Journal from 2014 to 2017.\nDr. Alex Beutel is a Staff Research Scientist in Google Brain SIR, leading a team working on responsible and fair ML, as well as researching neural recommendation and ML for Systems. He received his Ph.D. in 2016 from Carnegie Mellon University’s Computer Science Department, and previously received his B.S. from Duke University in computer science and physics. His Ph.D. thesis on large-scale user behavior modeling, covering recommender systems, fraud detection, and scalable machine learning, was given the SIGKDD 2017 Doctoral Dissertation Award Runner-Up. He also received the Best Paper Award at KDD 2016 and ACM GIS 2010, was a finalist for best paper in KDD 2014 and ASONAM 2012, and was awarded the Facebook Fellowship in 2013 and the NSF Graduate Research Fellowship in 2011. More details can be found at alexbeutel.com.\nAcknowledgement. Weinan Zhang is supported by \"New Generation of AI 2030\" Major Project (2018AAA0100900) and NSFC (61632017, 61702327, 61772333)."}],"references":[{"title":"Finite-time analysis of the multiarmed bandit problem","author":["Peter Auer","Nicolo Cesa-Bianchi","Paul Fischer"],"venue":"Machine learning 47,","citeRegEx":"1","shortCiteRegEx":"1","year":2002},{"title":"The nonstochastic multiarmed bandit problem","author":["Peter Auer","Nicolo Cesa-Bianchi","Yoav Freund","Robert E Schapire"],"venue":"SIAM journal on computing 32,","citeRegEx":"2","shortCiteRegEx":"2","year":2002},{"title":"Real-time bidding by reinforcement learning in display advertising","author":["Han Cai","Kan Ren","Weinan Zhang","Kleanthis Malialis","Jun Wang","Yong Yu","Defeng Guo"],"venue":"WSDM","citeRegEx":"3","shortCiteRegEx":"3","year":2017},{"title":"A survey of web information extraction systems","author":["Chia-Hui Chang","Mohammed Kayed","Moheb R Girgis","Khaled F Shaalan"],"venue":"TKDE 18,","citeRegEx":"4","shortCiteRegEx":"4","year":2006},{"title":"Large-scale interactive recommendation with tree-structured policy gradient","author":["Haokun Chen","Xinyi Dai","Han Cai","Weinan Zhang","Xuejian Wang","Ruiming Tang","Yuzhou Zhang","Yong Yu"],"venue":"In AAAI,","citeRegEx":"5","shortCiteRegEx":"5","year":2019},{"title":"Top-K Off-Policy Correction for a REINFORCE Recommender System","author":["Minmin Chen","Alex Beutel","Paul Covington","Sagar Jain","Francois Belletti","Ed H Chi"],"venue":"In WSDM","citeRegEx":"6","shortCiteRegEx":"6","year":2019},{"title":"Query representation and understanding workshop","author":["W Bruce Croft","Michael Bendersky","Hang Li","Gu Xu"],"venue":"In ACM SIGIR Forum,","citeRegEx":"7","shortCiteRegEx":"7","year":2011},{"title":"Search result diversification","author":["Marina Drosou","Evaggelia Pitoura"],"venue":"ACM SIGMOD Record 39,","citeRegEx":"8","shortCiteRegEx":"8","year":2010},{"title":"Reinforcement learning for relation classification from noisy data","author":["Jun Feng","Minlie Huang","Li Zhao","Yang Yang","Xiaoyan Zhu"],"venue":"In Thirty-Second AAAI Conference on Artificial Intelligence","citeRegEx":"9","shortCiteRegEx":"9","year":2018},{"title":"Information seeking: convergence of search, recommendations, and advertising","author":["Hector Garcia-Molina","Georgia Koutrika","Aditya Parameswaran"],"venue":"Commun. ACM 54,","citeRegEx":"10","shortCiteRegEx":"10","year":2011},{"title":"Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement Learning","author":["Li He","Liang Wang","Kaipeng Liu","Bo Wu","Weinan Zhang"],"venue":null,"citeRegEx":"11","shortCiteRegEx":"11","year":2018},{"title":"Real-time bidding with multi-agent reinforcement learning in display advertising","author":["Junqi Jin","Chengru Song","Han Li","Kun Gai","Jun Wang","Weinan Zhang"],"venue":"In CIKM","citeRegEx":"12","shortCiteRegEx":"12","year":2018},{"title":"Reinforcement learning: A survey","author":["Leslie Pack Kaelbling","Michael L Littman","Andrew W Moore"],"venue":"Journal of artificial intelligence research","citeRegEx":"13","shortCiteRegEx":"13","year":1996},{"title":"Socially compliant mobile robot navigation via inverse reinforcement learning","author":["Henrik Kretzschmar","Markus Spies","Christoph Sprunk","Wolfram Burgard"],"venue":"The International Journal of Robotics Research 35,","citeRegEx":"14","shortCiteRegEx":"14","year":2016},{"title":"Deep reinforcement learning based recommendation with explicit user-item interactions modeling","author":["Feng Liu","Ruiming Tang","Xutao Li","Weinan Zhang","Yunming Ye","Haokun Chen","Huifeng Guo","Yuzhou Zhang"],"venue":null,"citeRegEx":"15","shortCiteRegEx":"15","year":2018},{"title":"Asynchronous methods for deep reinforcement learning","author":["Volodymyr Mnih","Adria Puigdomenech Badia","Mehdi Mirza","Alex Graves","Timothy Lillicrap","Tim Harley","David Silver","Koray Kavukcuoglu"],"venue":"In ICML","citeRegEx":"16","shortCiteRegEx":"16","year":2016},{"title":"Playing atari with deep reinforcement learning","author":["Volodymyr Mnih","Koray Kavukcuoglu","David Silver","Alex Graves","Ioannis Antonoglou","Daan Wierstra","Martin Riedmiller"],"venue":null,"citeRegEx":"17","shortCiteRegEx":"17","year":2013},{"title":"Learning to coordinate multiple reinforcement learning agents for diverse query reformulation","author":["Rodrigo Nogueira","Jannis Bulian","Massimiliano Ciaramita"],"venue":null,"citeRegEx":"18","shortCiteRegEx":"18","year":2018},{"title":"Task-oriented query reformulation with reinforcement learning","author":["Rodrigo Nogueira","Kyunghyun Cho"],"venue":"arXiv preprint arXiv:1704.04572","citeRegEx":"19","shortCiteRegEx":"19","year":2017},{"title":"A combinatorial-bandit algorithm for the online joint bid/budget optimization of pay-per-click advertising campaigns","author":["Alessandro Nuara","Francesco Trovo","Nicola Gatti","Marcello Restelli"],"venue":null,"citeRegEx":"20","shortCiteRegEx":"20","year":2018},{"title":"Product-based neural networks for user response prediction over multi-field categorical data","author":["Yanru Qu","Bohui Fang","Weinan Zhang","Ruiming Tang","Minzhe Niu","Huifeng Guo","Yong Yu","Xiuqiang He"],"venue":"TOIS 37,","citeRegEx":"21","shortCiteRegEx":"21","year":2018},{"title":"A unified optimization framework for auction and guaranteed delivery in online advertising","author":["Konstantin Salomatin","Tie-Yan Liu","Yiming Yang"],"venue":"In CIKM","citeRegEx":"23","shortCiteRegEx":"23","year":2012},{"title":"An MDP-based recommender system","author":["Guy Shani","David Heckerman","Ronen I Brafman"],"venue":"JMLR 6,","citeRegEx":"24","shortCiteRegEx":"24","year":2005},{"title":"Mastering the game of Go with deep neural networks and tree search","author":["David Silver","Aja Huang","Chris J Maddison","Arthur Guez","Laurent Sifre","George Van Den Driessche","Julian Schrittwieser","Ioannis Antonoglou","Veda Panneershelvam","Marc Lanctot"],"venue":null,"citeRegEx":"25","shortCiteRegEx":"25","year":2016},{"title":"Mastering the game of Go without human knowledge","author":["David Silver","Julian Schrittwieser","Karen Simonyan","Ioannis Antonoglou","Aja Huang","Arthur Guez","Thomas Hubert","Lucas Baker","Matthew Lai","Adrian Bolton"],"venue":"Nature 550,","citeRegEx":"26","shortCiteRegEx":"26","year":2017},{"title":"Reinforcement learning: An introduction","author":["Richard S Sutton","Andrew G Barto"],"venue":null,"citeRegEx":"27","shortCiteRegEx":"27","year":2018},{"title":"Automatic ad format selection via contextual bandits","author":["Liang Tang","Romer Rosales","Ajit Singh","Deepak Agarwal"],"venue":"In CIKM","citeRegEx":"28","shortCiteRegEx":"28","year":2013},{"title":"A Reinforcement Learning Approach for Dynamic Search","author":["Zhiwen Tang","Grace Hui Yang"],"venue":null,"citeRegEx":"29","shortCiteRegEx":"29","year":2017},{"title":"Learning from delayed rewards","author":[],"venue":"Ph.D. Dissertation. King’s College,","citeRegEx":"30","shortCiteRegEx":"30","year":1989},{"title":"Reinforcement learning to rank with Markov decision process","author":["Zeng Wei","Jun Xu","Yanyan Lan","Jiafeng Guo","Xueqi Cheng"],"venue":"In SIGIR","citeRegEx":"31","shortCiteRegEx":"31","year":2017},{"title":"A multi-agent reinforcement learning method for impression allocation in online display advertising","author":["Di Wu","Cheng Chen","Xun Yang","Xiujun Chen","Qing Tan","Jian Xu","Kun Gai"],"venue":null,"citeRegEx":"32","shortCiteRegEx":"32","year":2018},{"title":"Learning contextual bandits in a non-stationary environment","author":["Qingyun Wu","Naveen Iyer","Hongning Wang"],"venue":"In SIGIR","citeRegEx":"33","shortCiteRegEx":"33","year":2018},{"title":"Adapting Markov decision process for search result diversification","author":["Long Xia","Jun Xu","Yanyan Lan","Jiafeng Guo","Wei Zeng","Xueqi Cheng"],"venue":"In SIGIR","citeRegEx":"34","shortCiteRegEx":"34","year":2017},{"title":"Estimation bias in multi-armed bandit algorithms for search advertising","author":["Min Xu","Tao Qin","Tie-Yan Liu"],"venue":"In NIPS","citeRegEx":"35","shortCiteRegEx":"35","year":2013},{"title":"Dynamic contextual multi arm bandits in display advertisement","author":["Hongxia Yang","Quan Lu"],"venue":"In ICDM","citeRegEx":"36","shortCiteRegEx":"36","year":2016},{"title":"Online context-aware recommendation with time varying multi-armed bandit","author":["Chunqiu Zeng","Qing Wang","Shekoofeh Mokhtari","Tao Li"],"venue":"In KDD","citeRegEx":"37","shortCiteRegEx":"37","year":2016},{"title":"Deep Reinforcement Learning for Page-wise Recommendations","author":["Xiangyu Zhao","Long Xia","Liang Zhang","Zhuoye Ding","Dawei Yin","Jiliang Tang"],"venue":"In Proceedings of the 12th ACM Recommender Systems Conference","citeRegEx":"38","shortCiteRegEx":"38","year":2018},{"title":"Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning","author":["Xiangyu Zhao","Liang Zhang","Zhuoye Ding","Long Xia","Jiliang Tang","Dawei Yin"],"venue":"In KDD","citeRegEx":"39","shortCiteRegEx":"39","year":2018},{"title":"Deep Reinforcement Learning for List-wise Recommendations","author":["Xiangyu Zhao","Liang Zhang","Zhuoye Ding","Dawei Yin","Yihong Zhao","Jiliang Tang"],"venue":null,"citeRegEx":"40","shortCiteRegEx":"40","year":2017},{"title":"Interactive collaborative filtering","author":["Xiaoxue Zhao","Weinan Zhang","Jun Wang"],"venue":"In CIKM","citeRegEx":"41","shortCiteRegEx":"41","year":2013},{"title":"Jointly Learning to Recommend and Advertise","author":["Xiangyu Zhao","Xudong Zheng","Xiwang Yang","Xiaobing Liu","Jiliang Tang"],"venue":"arXiv preprint arXiv:2003.00097","citeRegEx":"42","shortCiteRegEx":"42","year":2020}],"referenceMentions":[{"referenceID":9,"context":"These retrieval mechanisms could generate a set of objects that best match users’ explicit or implicit preferences [10].","startOffset":115,"endOffset":119},{"referenceID":20,"context":"Efforts have been made on developing supervised or unsupervised methods for these information retrieval mechanisms [21].","startOffset":115,"endOffset":119},{"referenceID":39,"context":"However, since the widely use of mobile applications during the recent years, more and more information retrieval services have provided interactive functionality and products [41], these conventional techniques typically face several common challenges.","startOffset":176,"endOffset":180},{"referenceID":22,"context":"Second, the majority of existing methods aims to maximize user’s immediate satisfaction, while completely overlooking whether the generate information can benefit more to user’s preference in the long run [24].","startOffset":205,"endOffset":209},{"referenceID":25,"context":"Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning [27].","startOffset":137,"endOffset":141},{"referenceID":23,"context":"Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].","startOffset":238,"endOffset":246},{"referenceID":24,"context":"Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].","startOffset":238,"endOffset":246},{"referenceID":15,"context":"Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].","startOffset":260,"endOffset":268},{"referenceID":16,"context":"Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].","startOffset":260,"endOffset":268},{"referenceID":13,"context":"Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].","startOffset":283,"endOffset":287},{"referenceID":6,"context":"documents, records) according to a user query [7, 29].","startOffset":46,"endOffset":53},{"referenceID":27,"context":"documents, records) according to a user query [7, 29].","startOffset":46,"endOffset":53},{"referenceID":18,"context":"[19] proposed a reinforcement learning based query reformulation task, which maximizes the number of recalled relevant documents via rewriting a query.","startOffset":0,"endOffset":4},{"referenceID":17,"context":"In [18], a multiagent reinforcement learning framework is proposed to increase the diverse query reformulation efficiency, where each agent learns a local policy that performs well on a subset of examples, and all agents are trained with parallelism to make the learning faster.","startOffset":3,"endOffset":7},{"referenceID":29,"context":"MDPRank [31] is proposed to address this problem by using the metrics calculated upon all the positions as reward function, and the model parameters are be optimized via maximizing the accumulated rewards for all decisions.","startOffset":8,"endOffset":12},{"referenceID":7,"context":"Beyond relevance ranking, another important goal is to increase the diversity of search results [8, 22], which needs to capture the utility of information users have perceived from the preceding documents in a sequential document selection.","startOffset":96,"endOffset":103},{"referenceID":32,"context":"MDPDIV [34] formalized diverse ranking as a continuous state Markov decision process, and policy gradient algorithm of REINFORCE is leveraged to maximize the accumulated long-term rewards in terms of the diversity metric.","startOffset":7,"endOffset":11},{"referenceID":31,"context":"Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions).","startOffset":15,"endOffset":23},{"referenceID":35,"context":"Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions).","startOffset":15,"endOffset":23},{"referenceID":5,"context":"Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over","startOffset":49,"endOffset":59},{"referenceID":8,"context":"Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over","startOffset":49,"endOffset":59},{"referenceID":40,"context":"Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over","startOffset":49,"endOffset":59},{"referenceID":37,"context":"In [39], a user’s dynamic preference (state) is learned from her browsing history and feedback.","startOffset":3,"endOffset":7},{"referenceID":28,"context":"The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].","startOffset":138,"endOffset":142},{"referenceID":1,"context":"The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].","startOffset":149,"endOffset":152},{"referenceID":0,"context":"The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].","startOffset":163,"endOffset":166},{"referenceID":21,"context":"In guaranteed delivery setting, ads that grouped into campaigns are charged on a pay-per-campaign basis for the pre-specified number of deliveries [23].","startOffset":147,"endOffset":151},{"referenceID":30,"context":"A multi-agent reinforcement learning approach [32] is proposed to derive cooperative policies for the publisher, where impression allocation problem is formulated as an auction problem, and publishers can submit","startOffset":46,"endOffset":50},{"referenceID":19,"context":"The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.","startOffset":79,"endOffset":95},{"referenceID":26,"context":"The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.","startOffset":79,"endOffset":95},{"referenceID":33,"context":"The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.","startOffset":79,"endOffset":95},{"referenceID":34,"context":"The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.","startOffset":79,"endOffset":95},{"referenceID":2,"context":"For example, a model-based RL framework is proposed in RTB setting [3], where the state value is approximated by neural network to address the scalability problem of large auction amounts and the limited budget.","startOffset":67,"endOffset":70},{"referenceID":11,"context":"In [12], a multi-agent bidding model is proposed to jointly consider all the advertisers’ biddings in the system, and a clustering approach is introduced to deal with a large number of advertisers.","startOffset":3,"endOffset":7}],"year":2020,"abstractText":"Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users’ information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users’ real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.","creator":"LaTeX with acmart 2018/01/24 v1.49 Typesetting articles for the Association for Computing Machinery and hyperref 2017/03/14 v6.85a Hypertext links for LaTeX"}}
{"name":"3397271.3401106.pdf","metadata":{"source":"META","title":"The Cortical Activity of Graded Relevance","authors":["Zuzana Pinkosova","William J. McGeown","Yashar Moshfeghi"],"emails":["zuzana.pinkosova@strath.ac.uk","william.mcgeown@strath.ac.uk","yashar.moshfeghi@strath.ac.uk"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems → Information retrieval; • Users and interactive retrieval; KEYWORDS\nInformation Retrieval, Relevance Judgement, Graded Relevance, Binary Relevance, Brain Signals, EEG, ERPs ACM Reference Format: Zuzana Pinkosova, William J. McGeown, and Yashar Moshfeghi. 2020. The Cortical Activity of Graded Relevance. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401106"},{"heading":"1 INTRODUCTION","text":"Relevance is a key concept in Information Science and Retrieval [45, 57, 59, 66]. While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401106\nshould be collected [44]. To answer this question, it is crucial to understand what each grade of relevance actually means. The value of evaluating information based on graded relevance has begun to receive attention in recent years both from system [40, 55] and user [2, 15, 48] point of views. This is particularly important since the granularity of relevance judgements in previous studies have been based on investigating this phenomenon indirectly, via some sort of mediator [29, 71]. This, therefore, limits the understanding of how searchers perceive different degrees of information relevance [55]. This paper aims to investigate the neural underpinnings of graded relevance directly.\nRelevance is also known to be subjective and difficult to quantify [48] since it depends on a searcher’s perception of information relating to a specific Information Need (IN) at a certain point in time [8, 57]. However, given the semantic gap between a searcher’s IN and their formulated queries [7, 27, 52], IR systems have employed various techniques to capture the subjective aspect of relevance [2] to improve the effectiveness of retrieved results. Examples of such techniques are explicit [38], implicit (e.g.[24, 36]) and physiological [47] feedback. More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65]. These studies have either investigated relevance in the context of word associations (i.e. relevance of a word with respect to another) [15, 16, 65] without subjects experiencing any IN; or investigated relevance in the context of Information Retrieval (IR) when IN has been introduced to subjects. In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored. In this paper, we aim to investigate three fundamental research questions:\n• RQ1: “Is there a clear, detectable, physical manifestation of graded relevance in human brains?”; • RQ2: “Do such manifestations differ when a user perceives different degrees of relevance? i.e., when searchers judge a document as highly relevant, low relevant or non-relevant.”. • RQ3: “What is the nature of graded relevance from a cognitive neuroscience perspective?”;\nAnswers to these questions will undoubtedly further our understanding of the concept of relevance and will provide the evidence needed to strengthen the theoretical foundations. This study is the first to incorporate relevance theory and a cognitive neuroscience approach to investigate the neural correlates of graded relevance judgements. Here we utilise an experimental design that enables the investigation of graded relevance within the context of IR and in real-time. In this study, participants will perform a Question Answering (Q/A) retrieval task, while their brain activity is monitored using EEG. Our central aims are to identify: (i) the brain activity associated with distinct graded relevance judgement across time from stimulus onset (ii) test whether there are neural manifestations of cognitive activity underlying each grade of relevance judgement\nand (iii) test whether processing distinct grades of relevance is associated with significantly different neural signatures. We are focusing on a range of ERP components - the P300, N400 and P600. Being able to capture brain activity associated with graded relevance could provide better inputs to information systems, which in turn could lead to improved retrieval effectiveness and greater searcher’s satisfaction."},{"heading":"2 RELATEDWORK","text":"Relevance. Relevance is the fundamental concept in IR [45, 57– 59]. It plays a crucial role in the user-system interaction since it is a substantial indicator of system retrieval performance [8, 57]. Despite significant attention dedicated to examining this concept, relevance is still not fully understood, and it is a subject of many ongoing scientific debates. Past research has investigated the concept of relevance at different granularity levels from both the user [29, 71] and system [35, 40] perspective. Within the system side, graded relevance (in comparison to the binary one) has been shown to improve ranking functions [34, 55]. Within the user side, recent research supports the idea of categorical thinking [71], suggesting that users divide retrieved results into 3-5 categories based on relevance [41]. However, levels of granularity were decided based on a self-report mechanism, without clear evidence that those levels have different physical manifestations in the brain. In this paper, we aim to provide evidence for different grades of relevance from a neuroscience perspective.\nCapturing Relevance Judgement. Given the importance of the user side of relevance, IR systems have been employing mechanisms and techniques to capture this phenomenon, namely explicit and implicit feedback. Explicit feedback is easy to use, however, difficult to obtain due to the cognitive burden associated with it [47], as the user is required to explicitly state whether presented content is subjectively perceived as relevant or not [67]. Implicit feedback is an unobtrusive data collection method. Popular techniques used to measure implicit relevance feedback are, for example, dwell time (i.e.[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47]. However, implicit feedback is often found to be noisy, which decreases its accuracy [2]. The findings of novel studies employing neuroscience have shown that brain imaging is an effective method to capture relevance judgement in-real time.\nNeuroscience & IR. Recent research has begun to apply brain imaging methods to study aspects of the IR process from a neuroscience perspective. One particular area of emphasis for this research has been to examine the IN process [50, 51]. In addition, it has been found that prediction of the IN state experienced by a user is possible using brain signals [50]. Apart from IN, recent studies have employed brain imaging techniques to gain a better understanding of other parts of the information seeking and retrieval process, such as query formulation [28], search [49, 70] and relevance (e.g. [33]).\nNeuroscientific Approach to Relevance. Recent research using a neuroscience approach to investigate relevance might be categorised in two ways based on the context within which the relevance was measured. The first line of brain-imaging research has position relevance within the IR task. For instance, Moshfeghi and colleagues [48] employed functional magnetic resonance imaging\n(fMRI), to localise differences in brain activity in cortical regions during the processing of relevant vs non-relevant images. The research was able to identify regions engaged in the relevance judgement processing and the increased activation of these regions for relevant items was related to visuospatial working memory [49, 51].\nAdditionally, examining the neural activity underlying postrelevance judgement revealed that there were differences in the processing of non-relevant and relevant words, that persisted for approximately 260 to 320 ms for relevant words and 500 to 530ms for an irrelevant word [19].\nRelevance has been inferred using EEG [25] or in combination with pupillometry or/and eye-tracking devices [22] within the context of the IR task, not only for textual stimuli but also for videos [37] and images [2]. For instance, Allegreti et al. (2015) examined the processing of relevant vs non-relevant images, finding the most significant differences to occur between 500 – 800ms [2]. Kim and Kim (2019) explored the ERPs associated with topical relevance of video skims and classified the data based on two specific ERP components (N400 and P600), which have been shown to be indicators of relevant and non-relevant judgements [37]. Moreover, recent findings have shown that relevance can be predicted in real-time from EEG brain signals and eye movements while the user engages with the system and IR task [28].\nAnother line of research has examined relevance in the context of word associations, employing EEG in isolation, or in combination with eye gaze [15, 16, 65]. In these scenarios, participants did not experience IN, but they engaged in judging word association to the topic. The findings of these studies have shown that brain signals differ when subjects process relevant vs. non-relevant words across time [16]. Later, Eugster and colleagues (2016) introduced a brainrelevance paradigm enabling recommendation of information to users without any explicit user interaction, based on EEG signals alone evoked by users’ engagement with the textual content [15].\nDespite valuable insight provided through past neuroimaging studies examining the complex relevance process, it is important to note that relevance has been investigated in binary terms. Hence, it is not clear whether graded relevance judgements are associated with significantly different cognitive processes and whether using more than two relevance categories would be associated with significantly different neural activity. This work takes an essential step to understanding the neural processes involved with graded relevance judgements."},{"heading":"3 METHODOLOGY","text":"Participants. Using opportunistic sampling, data were collected from 25 participants. From these, two were excluded due to excessive movement induced artefacts. Within the 23 remaining participants, there were 14 females and 9 males, the mean age was 25.39, the SD1 5.00 years, and the range 19 to 39 years. All participants had normal or corrected-to-normal vision. Overall, nine participants reported to be native English speakers, and the rest had high English proficiency. On average, participants had the experience of 17.65 (±3.46) years of formal education. Only one participant reported being left-handed. Most of the participants were either\n1Standard Deviation\nundergraduate or postgraduate students (60.87%), and the rest were either employed in skilled jobs (30.43%) or unemployed (8.67%).\nDesign. Weused a within-subject experimental design, in which participants performed a Q/A task. The independent variable was graded relevance judgement (with three levels: “No-Relevance” (NONR), “Low Relevance” (LOWR) and “High Relevance” (HIGHR). We controlled the number of relevant vs non-relevant sentences (i.e. answers) presented to the user, but we did not control the number of words presented to each participant. This allowed us to simulate an information search and retrieval, as we did not enforce participants to go through the whole answer, once the relevance judgement has been made. The dependent variables were the EEG signals of the brain, gathered from the users during the Q/A task.\nERP Components. To capture the brain response associated with relevance, we measured small voltage fluctuations across the scalp, that are time-locked to a specific event or stimulus. These are known as ERPs. In this study we focused on the main ERP components associated with relevance (i.e. P300, N400, P600) (e.g. [15, 37] ). The P300 component is a positive-going voltage deflection that peaks around 300ms post-stimulus [43]. This time-locked ERP component has been included in the analysis as it is thought to reflect the amount of cognitive resource employed for information processing [53]. The N400 component is a negative-going potential, reaching its peak at 400ms from the stimulus presentation [43]. This ERP component is usually elicited by irrelevant/incongruent stimuli, and its amplitude is associated with semantic integration. Words that can be integratedmore easily to the context elicit smaller (more positive) amplitude and vice versa [43]. The P600 component is characterised as a positive deflection, peaking around 600ms from stimulus onset [43]. It is elicited during cognitive processing involved in text comprehension [32].\nApparatus. The experiment was conducted in a neurophysiology laboratory at the University of Strathclyde. Stimuli were presented with E-Prime 2.0, installed on a PC with 22\" Mitsubishi Diamond Pro 2040u NF CRT monitor with the resolution of 2048 x 1536 and a refresh rate of 75 Hz. The screen was positioned approximately 60cm from the participant. A soft light was used to avoid distractions. Participants were able to interact with the PC using the keyboard. EEG recordings were obtained using a 128-channel geodesic sensor net (Electrical Geodesics Inc) using Net station 4.3.1 software. We aimed to keep electrode-scalp impedance below 50 kΩ for all participants. The sampling rate was set at 1000 Hz. To set the system for recording, we followed Electrical Geodesics Inc guidelines. The EEG sensor net was soaked in the potassium chloride (KCl) solution for conductivity. Each participant’s head was measured to determine the correct EEG net size, and the net was positioned using standardised procedures, ensuring that the vertex is halfway between the inion and nasion and halfway between both bilateral preauricular points. The EEG net VREF electrode was positioned at the marked vertex. A NetAmps 200 amplifier was used for synchronisation between the behavioural responses of participants and their brain signals. Finally, we used entry, Post-task and Exit Questionnaires for each participant.\nQuestionnaires. At the beginning of the experiment, an Entry Questionnaire was introduced to gather background and demographic information about the participant and participants were screened to assess whether they are eligible to take part in the study.\nAny participants who had existing conditions (i.e. neurological or psychiatric disorder) that might impact the EEG signal recordings were excluded. Once participants completed the task, they were asked to fill in a Post-task Questionnaire asking them to rate how they perceived the encountered task. Finally, all participants completed an Exit Questionnaire, which examined the perception of their overall performance."},{"heading":"3.1 Procedure","text":"The user study was carried out in the following manner. Ethical permission for the study was obtained from the Department of Computer and Information Sciences Ethics Committee at the University of Strathclyde. The formal meeting with the participants took place in the laboratory setting. At the beginning of the session, all the participants were briefed as to the procedure and the purpose of the experiment through the information sheet. Then, they were asked to provide informed consent. All participants were notified about their right to withdraw at any time during the study, without giving a reason and without any consequences. After that, participants were instructed to fill in an Entry Questionnaire. Prior to the main experimental trials, participants underwent training, which resembled the main experimental task. This ensured that all the participants have a good general understanding of the procedure. The training was not limited by time and participants were able to repeat it if required. In total, each participant completed 64 trials. To avoid fatigue, the stimuli were presented to the participants in two blocks of 32 trials each, separated by a break. The average duration of the main experimental task was approximately 53.57 minutes and each participant was presented on average with 799.61 words (±161.11).\n3.1.1 Procedure of the Main Experimental Task. A schematic representation of the main experimental task is illustrated in Figure 1. At the beginning of the trial, participants were presented with the task instructions. Next, they viewed a question randomly selected from the data set. Once participants read and fully understood the question, they were presented with the fixation cross, to indicate the location of the answer presentation. The answer was then presented word by word in the middle of the screen to control freeviewing and to minimise saccade-related measurement artefacts. This approach has traditionally been applied in the ERP studies examining neurological correlates of reading [14]. As a result, the ERPs were time-locked to the word presentation. Each word was presented for 950ms. This reading pace was appropriate to model fluent reading as much as possible and to avoid the presence of the overlapping effect of two consecutive words on the ERPs [15]. Participants were instructed to read individual words, that formed sentences representing either a relevant or non-relevant answer. They had an option of terminating the presentation of the words and continuing to the next step. As brain activity was recorded throughout the task, to avoid the possibility of confounding hemispheric effects, the hand used for button responses during the task was counter-balanced across participants (each participant used only the left hand or right hand, as instructed).\nAfter that, participants were again presented with the same answer appearing on the screen in the same order (up to the point of presentation abandonment). In this stage of the trial, the answers\nwere presented word by word as continuous text. Participants were instructed to assign a (subjectively perceived) graded relevance judgement (NONR, LOWR, and HIGHR) for each word segment of the answer while taking into account information accumulation, rather than judging words in isolation with relationship to the question. The graded relevance judgements were then assigned retrospectively, enabling this detailed information to be applied to the corresponding EEG segments of interest. The interpretation of each graded relevance category depended on each participant’s subjective understanding, which enabled capturing the subjective nature of relevance judgement [57].\nAll text events were presented in Arial font, size 16. After completion of the main experimental task, participants were instructed to fill out a Post-Task Questionnaire and an Exit Questionnaire. All participants were debriefed after the experiment.\nQuestion/Answering Dataset. For our experiment, we adopted the Question Answering dataset developed and used by Moshfeghi et al. [51]. We chose this dataset since it has been widely in previous studies investigating IR phenomena from a neuroscience perspective [49, 51]. The adopted dataset was further expanded through the selection of additional questions and answers from TREC-8 and TREC-2001. We chose these two Tracks since they (i) provide the correct answer to the question, and (ii) they are independent of one another. We ensured that selected questions were not ambiguous or time-dependent, making sure the answers\nprovided in the Track were still appropriate by manually validating each answer using a search engine.\nThe created data set was then split into two parts (Data Set A and Data Set B)2, each containing 64 questions, answers and relevance assessments in total. The decision of splitting the dataset in two was made during the pilot study after observing the length of the experiment and to reduce the fatigue level of the participants. We made sure that Data Set A and B had similar characteristics (as shown in Table 1) to avoid introducing any bias in our results. For example, we ensured that both data sets were balanced to be apriori 50% relevant and 50% no-relevant. In addition, we balanced the answer length (long vs short), and question difficulty (easy vs difficult)3. This was done to reduce any potential bias that might occur from the emphasis of one particular question/answer type.\nAn example of an easy question presented to the participants was “Who was Galileo?”, which was followed by the short, relevant answer “In 1642, astronomer Galileo died in Arcetri, Italy.”. The order of the questions was randomised for each participant. This randomisation ensured that the recorded signals and effects were related to the users’ subjective relevance judgement of the presented stimuli, and not related to the stimulus presentation frequency, potentially causing an oddball effect [15]. We ensured that answers from both data sets were approximately the same lengths, based on the mean number of characters per answer category. Participants were then randomly assigned to one of the two data sets.\nPilot Studies. Prior to running the main study, a pilot study was performed employing 4 participants, whose data were not included in the analysis. Detailed feedback obtained from participants involved in the pilot study was used to adjust the study presentation and design. After the final pilot study, it was determined that the participants were able to complete the user study without problems and that the system was capturing all necessary data.\nPre-processing steps. The brain activity was recorded from participants as the answer was unfolding to them. The obtained signals were then matched with the graded relevance judgement up to the point where the participant stopped the answer presentation. All collected data were first visually inspected. Then we\n2The Question Answering Data Set A and the Data Set B are available upon request. 3To assess the difficulty level, two annotators separately judged the difficulty of the questions (i.e. hard or easy) and then selected a subset of 128 questions where both annotators agreed upon their difficulties, i.e. 64 of them were hard, and 64 were easy questions.\napplied a low-pass filter of 30Hz. By attenuating the higher frequencies, the anti-aliasing is reduced, which is an important step before data down-sampling. We have then down-sampled the data from 1000Hz to 250Hz, and then a high pass filter of 0.3Hz was applied. Filtering is a common pre-processing procedure, used to attentuate frequencies commonly associated with noise rather than signal of interest. Downsampling, another commonly applied procedure, is used to reduce file size for easier manipulation (a level was chosen to maintain the signal quality). We then automatically rejected bad channels (EEG data-streams/sensors that were not functioning properly during the data acquisition and that were high in noise throughout the task). On average, we removed 13.08 bad channels (±7.43). The CleanLine EEGLAB plugin was used to filter line noise. The vertex (Cz) sensor was initially used as the reference electrode, but re-referencing to average (across all electrodes) was subsequently performed (to provide an approximation of zero microvolts for the reference at each timepoint). All epochs (the time-windows of interest) were then extracted from 200ms before stimulus presentation to 950ms afterwards. To detect and remove components associated with ocular, cardiac and muscular artefacts based on their power spectrum and time-course, we performed Independent Component Analysis (ICA). ADJUST was then used to remove artefacts. A mean number of 14.91 (±8.26) components were identified and removed. Bad channels were interpolated (reconstructed) using a spherical interpolation method. Next, we removed the outermost belt of electrodes of the sensor net (19 peripheral channels: E43, E48, E49, E56, E63, E68, E73, E81, E88, E94, E99, E107, E113, E119, E120, E125, E126, E127, E128)[9]. Epochs were then extracted again from 100ms before stimulus presentation to 950ms afterwards based on the stimulus label (’NONR’, ’LOWR’ and ’HIGHR’). All epochs were baseline-corrected and bad epochs were then automatically identified and removed using the epoch rejection plugin. On average, we rejected 35.91 (±12.26) of NONR epochs, 27.83 (±13.86) HIGHR epochs and 23.89 (±12.38) LOWR epochs."},{"heading":"3.2 ERP - Components of Interest","text":"To analyse our three components of interest (P300, N400, and P600), we selected three electrode configurations (that partially overlap) and three time-windows (that do not overlap), based on the findings of previous studies as well as a visual inspection of the grand average waveforms. For the P300 component, we chose a time-window of 220–440 ms from stimulus onset [3, 53] and the representative subset of 22 centro-parietal channels over the midline (E55, E62), left (LH: E30, E36, E37, E42, E52, E53, E54, E60, E61, E67) and right hemispheres (RH: E77, E78, E79, E85, E86, E87, E92, E93, E104, E105) as shown in Figure 2a. The electrode selection was based on the previous literature examining the P300 component through the calculation of mean amplitudes [54]. The mean amplitudes for N400 component were measured between 400 – 600 ms [18]. We identified 14 electrodes, covering parietal-temporal regions bilaterally (LH: E31, E37, E42 E53, E54, E61 and RH: E79 E78, E86, E87, E80, E93), including mid-line channels (E55, E62) [26] as shown in Figure 2b. To analyse the P600 component, we again selected the electrodes and the time-window that the component is typically measured most strongly. We selected the time window between 550 – 750 ms\n[6] and a bilateral group of electrodes covering central, parietal and temporal regions (E52, E53, E54, E59, E60, E61, E66, E67, E72, E77, E78, E79, E80, E85, E86, E87, E92, E93) [21] as shown in Figure 2c."},{"heading":"4 RESULTS","text":"Questionnaire Analysis. Before analysing the results we investigated participants’ perception of the task. In the Post-task Questionnaire, we asked participants how they found the task, questions presented to them, familiarity with questions and whether they felt comfortable throughout the task (answers: 1: ”Strongly Agree”, 2: ”Agree”, 3:”Somewhat Agree”, 4:”Neither Agree or Disagree”, 5:”Somewhat Disagree”, 6: ”Disagree”, 7: ”Strongly Disagree”). The results shown in Figure 3 indicate that participants found the task (M = 1.96, SD = 1.26) and questions (M = 1.91, SD = .75) rather interesting. Perceived difficulty of the task (M = 3.74, SD = 1.84) and questions (M = 4.13, SD = 1.58) was rated as moderate. Presented questions (M = 1.96, SD = 1.33) and task in general (M = 2.00, SD = 1.21) were considered as readable. Additionally, both, questions (M = 2.17, SD = 1.50) and task (M = 2.13, SD = 1.18) were also considered as understandable. On average, participants felt moderate physical comfort (M = 2.74, SD = 1.57) and task was not rated as stressful (M = 4.57, SD = 1.85). Questions selected for the experiment were perceived by participants as moderately familiar (M = 3.26, SD = 1.48) and relevant to them (M = 2.48, SD = 1.53).\nUsing the Exit Questionnaire, we examined participants’ perception of their overall performance during the task (answers: 1: ”Strongly Agree”, 2: ”Agree”, 3:”Somewhat Agree”, 4:”Neither Agree or Disagree”, 5:”Somewhat Disagree”, 6: ”Disagree”, 7: ”Strongly Disagree”). Overall, the results indicate that participants felt that they had enough time to press a button to terminate the answer presentation (M = 1.61, SD = .84). Additionally, they found the speed of the word presentation (M = 1.36, SD = .58) to be appropriate for reading. The font size (M = 1.30, SD = .70) and monitor luminance (M = 1.87, SD = 1.14) were also rated to be task appropriate. Most of the participants felt comfortable (M = 2.23, SD = 1.19) during the task and the EEG cap was not causing them any discomfort (M = 1.65, SD = .98). Participants found following the procedure to be easy (M = 1.70, SD = .88), with instructions being clear (M = 1.26, SD = .45) and they were mostly satisfied with their performance (M = 2.04, SD = .98).\nGraded Relevance Component Analysis. The ERP analysis relied upon a participant’s graded relevance judgement assessment (coded as NONR, LOWR, and HIGHR) to the presented information.\nTable 2 shows the statistics for the collected graded relevance judgements across participants. Analyses focused on the P300, N400 and P600 components during the graded relevance judgement process. The electrode configurations and time-windows for each component was based on the previous literature and adjusted by visual inspection (see Section 3.2). The ERPs were obtained by averaging across the selected electrode configurations. For this study, we calculated the mean amplitude, which enabled us to factor out latency jitter and to obtain more robust results [10, 42].\nTo avoid possible misinterpretations when comparing the three conditions for peak amplitude of the components (e.g., when high frequency fluctuations may influence the results), mean amplitudes were instead used. These were determined by averaging the activity within each time window of interest for each of the electrode groupings. To investigate the statistical significance of differences between recorded neurological signal associated with graded relevance judgements (NONR, LOWR, and HIGHR) for P300, N400, and P600 components, we applied repeated measures ANOVAs. To do so, we first investigated whether the assumptions that are required for repeated measures ANOVA are met. The dependent variables were approximately normally distributed. Mauchly’s Test was used to investigate whether the assumption of sphericity was met for each condition. Since this was not the case, we used the Greenhouse-Geisser method, which enabled us to obtain F-ratios\nwith greater accuracy. The epsilon (\uD835\uDF16), estimating the amount of sphericity, is reported. For scenarios in which the ANOVA test indicated statistically significant differences between the recorded neurological signals, we conducted pairwise comparisons of the conditions (i.e. HIGHR vs LOWR, HIGHR vs NONR, etc.) using Bonferroni tests. Results were considered significant at \uD835\uDC5D < 0.01.\n4.0.1 Main Findings. Our experimental results show that significant differences exist in brain activity when judging information as having high-relevance, low-relevance, or no-relevance. Differences were present in all components of interest (the P300, N400, and P600 components), which suggests that a variety of distinct cognitive processes are underpinning the graded relevance evaluations.\n4.0.2 P300. The P300 component waveforms grand-averaged across participants for the NONR, LOWR and HIGHR conditions are shown in Figure 4a. Component latency considered for the analysis is highlighted in grey. The P300 amplitude was highest for HIGHR condition (M = .39 \uD835\uDF07V, SD = .11), followed by the LOWR condition (M = .35 \uD835\uDF07V, SD = .10) and NONR condition (M = .26 \uD835\uDF07V, SD = .11). The results of repeated measures ANOVA with Greenouse-Geisser correction showed that the brain activation associated with the P300 component differed significantly across NONR, LOWR and HIGHR conditions [F(1.16, 63.98) = 42.42, \uD835\uDC5D < 0.001, \uD835\uDF16 = .44]. PostHoc tests using the Bonferroni correction revealed that the highest mean difference was between HIGHR and NONR conditions (Mdiff = .13 \uD835\uDF07V , \uD835\uDC5D < .001), following the LOWR and NONR (Mdiff = .0.83 \uD835\uDF07V, \uD835\uDC5D < .001) and LOWR and HIGHR conditions (Mdiff = .05 \uD835\uDF07V, \uD835\uDC5D < .001).\nInformation judged as HIGHR was associated with significantly greater P300 amplitudes across central and centro-parietal electrode sites when compared to LOWR and NONR. Hence, during this early stage of implicit relevance judgement, users’ selective attention is allocated towards highly relevant stimuli, which are also easier to process in terms of cognitive load [1, 53]. These findings are consistent with the previous literature, showing that the degree of subjectively perceived information relevance is proportional to the P300 component amplitude [2, 23].\n4.0.3 N400. Figure 4b presents the N400 component waveforms for the NONR, LOWR and HIGHR conditions. The mean negativity of the N400 component was the most prominent in NONR condition (M = .00 \uD835\uDF07V, SD = .11), following the LOWR (M = .10 \uD835\uDF07V, SD = .13) and HIGHR condition (M = .40 \uD835\uDF07V, SD = .11). A repeated measures ANOVA carried out with Greenouse-Geisser correction revealed significant differences in the mean amplitude across the graded relevance judgement conditions [F(1.64, 82.09) = 784.16, p < 0.001, \uD835\uDF16 = .94]. Post-Hoc comparisons using the Bonferroni correction indicated that the highest mean amplitude differences for the N400 component were measured between the HIGHR and NONR conditions (Mdiff = .40 \uD835\uDF07V , \uD835\uDC5D < .001), followed by the LOWR and HIGHR (Mdiff = .0.30 \uD835\uDF07V, \uD835\uDC5D < .001) and LOWR and NONR conditions (Mdiff = .10 \uD835\uDF07V, p < .001).\nThe processing of NONR, LOWR and HIGHR information was associated with negative-going deflection, which was the most prominent for the NONR condition, which is consistent with previous literature [15, 37, 65]. Although highly relevant information reduces the N400 amplitude [63], our results indicate that the N400\n(a) The grand average ERP waveforms for the P300 electrode configuration, by condition type (HIGHR, LOWR and NONR), with the 220–440 ms (P300) time interval of interest highlighted in grey.\n(b) The grand average ERP waveforms for the N400 electrode configuration, by condition type (HIGHR, LOWR and NONR), with the 400–600 ms (N400) time interval of interest highlighted in grey.\n(c) The grand average ERP waveforms for the P600 electrode configuration, by condition type (HIGHR, LOWR and NONR), with the 460–680 ms (P600) time interval of interest highlighted in grey.\nFigure 4: The grand average ERP waveforms from the selected electrode configurations. Time intervals of interest (in milliseconds) are highlighted in grey for each component. Change in the amplitude of the ERP relative to the prestimulus baseline (-100ms to 0ms) is represented on the y-axis (in microvolts), and time following stimulus onset (from 0ms onwards) is represented on the x-axis.\ndeflection was also salient for the LOWR condition. The observed difference between the HIGHR and LOWR category for the N400 component might indicate that LOWR (and NONR) stimuli require significantly greater cognitive effort to process and integrate within the given context [13]. The context within this experiment has been provided through the question, and hence it is possible to assume that higher N400 amplitudes elicited during NONR condition signalise contextual violation [5] and a degree of uncertainty during LOWR condition [64].\n4.0.4 P600. The grand-averaged P600 waveforms are displayed in Figure 4c for each condition (NONR, LOWR and HIGHR). The mean amplitude of the P600 component was the highest in HIGHR condition, (M = .40 \uD835\uDF07V, SD = .13), following the LOWR (M = .33 \uD835\uDF07V, SD = .06) and NONR condition (M = .12 \uD835\uDF07V, SD = .11).\nThe results of repeated measures ANOVA carried out using Greenhouse-Geisser correction method revealed significant differences in the mean amplitude across the graded relevance judgement conditions [F(1.34, 67.07) = 108.31, p < 0.001, \uD835\uDF16 = .68]. Post-Hoc tests\nusing Bonferroni correction revealed that the highest mean difference was between HIGHR and NONR conditions (Mdiff = .27 , p < .001), following the LOWR and NONR (Mdiff = .21, p < .001) and HIGHR and LOWR conditions (Mdiff = .07, p < .01) displayed in Figure 4c over the centro-parietal areas. Our findings are in alignment with previous studies, suggesting that processing of NONR information is associated with low P600 amplitudes [15, 37]. As the P600 amplitude is highest for the HIGHR condition, this may reflect that the amount of information carried by the processed term is higher in comparison to the LOWR condition [32]."},{"heading":"5 DISCUSSION","text":"The paradigm developed for this study enabled judgements of relevance to be assessed in a graded fashion, and for the corresponding neural activity to be recorded. Specifically, participants reflected on sentences that they had seen in response to a question and reported after each word of the sentences what their perception of relevance was at that time (HIGHR, LOWR or NONR). Participants were, therefore, processing each word of the sentence within the context of whether they subjectively perceived the information segment at that time to be relevant to the question.\nThe key findings which emerged from the study are that levels of neural activity across time are dependent on whether the person perceived the sentence as of high relevance, low relevance or norelevance to the question (this finding addresses RQ1). Significant differences were detected in the three late-stage ERPs of interest (P300, N400 and P600), that followed words that were processed in the context of whether the information segment was deemed to be of high relevance, low relevance or no-relevance to the question (this finding addresses RQ2). The differences in neural activity suggest that during assessment of relevance, a variety of cognitive processes are relied upon to different degrees: For example the higher the relevance, it may be, the greater the attentional engagement, the higher the perception of semantic relatedness (the lower the semantic incongruency between context of the question and the answer), and the greater the requirement for engagement of memory (relevant information might be deemed more important to encode and recall than irrelevant information) (this discussion provides an early step towards answering RQ3).\nHigh Relevance: The results suggest that greater attentional resources are allocated to highly relevant stimuli, as indicated through the differences observed within the P300 component. P300 amplitude has been shown to be proportional to attentional engagement [30]. Greater P300 amplitude has also been suggested to reflect the quantity of information transmitted [56], the quantity of useful information [31], relevance to the self [23], or relevance to the task [17, 62], or to judgements of relevance specifically [2]. Eugster et al. [15], when assessing term relevance, did not find a difference in the P300 between relevant and irrelevant words. It is possible that our P300 measure is influenced by the N400 deflections, but the P300 and N400 may both be modulated during the assessment of relevance. The underlying processes also might not be wholly independent [5]. Future research will provide clarification.\nThere was a clear reduction of the N400 within the time-interval of 400 – 600 ms. Typically, in studies of language, the N400 provides\nan index of semantic relatedness; it is larger when there is a semantic mismatch than semantic congruency (see e.g., [61]). Given the task requirements of the current study and the differences we observed in the N400 in response to graded relevance, it seems that in this case the component is modulated not necessarily in response to the meaning of the word, but to the relatedness of the sentence to the question. Words processed in the context of high relevance are semantically aligned to the question, which likely explains the attenuated N400 response. The N400 results fit closely with the study by Eugster and colleagues [15], who found a reduced N400 for relevant words, compared to irrelevant words.\nIn our study, words linked to sentences of high relevance were associated with the highest P600 amplitudes. Similarly, Eugster et al. [15] found that their relevant words elicited larger P600 components than irrelevant words. The link between higher relevance and P600 amplitude is not completely clear. The P600 has often been associated with syntactic processing, and later research has flagged a semantic-thematic role (however, larger P600s are found due to violations – see e.g., [39]). A more likely reason, or at least a partial explanation, may be that the late stage positivity is instead linked to memory processing (e.g., through a process such as recognising that the answer is relevant and is linked to the question). A late positive complex has been observed during memory recognition, it is higher for old versus new stimuli, occurs at around 600ms after a stimulus and also has a central posterior topography [69].\nIt should be noted that the task requirements of our study differed from that of Eugster and colleagues, due to our assessment of graded relevance (high, low, no), as opposed to a binary judgement. A further key difference was in the way the relevance judgements were made - participants in our study did not evaluate individual words for relevance, but rather, in response to pre-established IN, they repeatedly assessed the relevance of the entire information segment each time a new word was presented.\nNo-Relevance: Words processed without any perceived relevance to the question had the lowest mean P300 values, the greatest N400, and the lowest P600. Conversely to the words processed in the high relevance context, the words processed that are not relevant to the question may have a low P300 due to cognitive factors such as low attentional engagement, a large N400 due to a mismatch between the semantic material offered in the answer given the context of the question (larger semantic incongruity), and a lower P600 due to reduced memory processing given that the answer is not relevant to the question (e.g., information to be retained results in larger P600 amplitudes than information to be forgotten - see e.g.[68].\nLow Relevance: A crucial question relates to the manner in which words are processed in the low relevance context. Specifically, is the processing of these words more similar to words viewed in the high relevant context or the non-relevant context? The ERP component amplitudes for the words processed in the context of low relevance fell somewhere in between those processed in the context of high relevance and those processed in the context of low relevance (significant differences were seen for all three components across the three conditions). A key difference appears to be in the N400. Although the N400 to the word segments deemed of low relevance differed significantly from those deemed of high and of no-relevance, the waveform pattern more closely resembled\nthe waveform for no-relevance, suggesting that low relevance segments may be perceived more like non-relevant segments, until a critical threshold is reached. On the other hand, although low relevant segments differed significantly from the high relevance and no-relevance segments for the P600 component, the low relevance segment waveform more closely resembled the high relevance waveform. This may reflect a dual nature of the stimuli evaluated as having low relevance. When words are processed in the context of low relevance, the sentences semantically have not crossed any critical threshold for relevance (there is still semantic incongruency), whereas there is still the potential for relevance to increase in these stimuli, so they must be processed adequately in memory and matched to the question (in a similar fashion as words in the context of high relevance)."},{"heading":"6 CONCLUSION AND FUTURE DIRECTIONS","text":"In conclusion, our findings provide support for the concept of graded relevance, given the clear differences in neural activity when information segments are perceived as having high relevance, low relevance or no-relevance. The P300, N400 and P600 all differed due to the perceived relevance of the answer. Despite a number of ERP components being identified that relate in different ways to the level of relevance perceived, it will be important to understand how robust/reliable these differences are and how alterations in the questions (e.g., the difficulty level) or in the answer (e.g., the length of the response) may interact with these features. Future competing hypotheses may be whether there may be a dual nature to relevance processing, wherein cases of ambiguity (e.g., low relevance - where the person reserves judgement while seeking more information) some neural features may be more like non-relevant features (e.g., as seen in pattern of the N400 waveforms in the current study), and others are more like high relevance features (e.g., the pattern of P600 waveforms), or alternatively perhaps whether more finegrained distinctions may be reflected in the neural signal. More fine-grained distinctions could be tested with relevance scales that offer more options for evaluation than we did in the current study (e.g., no, low, moderate and high relevance; or a judgement scale with even more options could be used). These results further our understanding of the concept of relevance and provide evidence needed to strengthen its theoretical foundations. Finally, we believe our conclusions constitute an important step in unravelling the nature of graded relevance and knowledge of the electrophysiological modulation to each grade of relevance.\nAcknowledgement: This work was supported by the Engineering and Physical Sciences Research Council [grant number EP/R513349/1]."}],"references":[{"title":"Working memory load can both improve and impair selective attention: evidence from the Navon paradigm","author":["Lubna Ahmed","Jan W de Fockert"],"venue":"Attention, Perception, & Psychophysics 74,","citeRegEx":"1","shortCiteRegEx":"1","year":2012},{"title":"When Relevance Judgement is Happening? An EEG-Based Study","author":["Marco Allegretti","Yashar Moshfeghi","Maria Hadjigeorgieva","Frank E. Pollick","Joemon M. Jose","Gabriella Pasi"],"venue":"In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SI- GIR ’15)","citeRegEx":"2","shortCiteRegEx":"2","year":2015},{"title":"P300 correlates with learning & memory abilities and fluid intelligence","author":["Hafeez Ullah Amin","Aamir Saeed Malik","Nidal Kamel","Weng-Tink Chooi","Muhammad Hussain"],"venue":"Journal of neuroengineering and rehabilitation 12,","citeRegEx":"3","shortCiteRegEx":"3","year":2015},{"title":"Affective feedback: an investigation into the role of emotions in the information seeking process","author":["Ioannis Arapakis","Joemon M. Jose","Philip D. Gray"],"venue":"In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’08)","citeRegEx":"4","shortCiteRegEx":"4","year":2008},{"title":"The N400 and the P300 are not all that independent","author":["Yael Arbel","Kevin M Spencer","Emanuel Donchin"],"venue":"Psychophysiology 48,","citeRegEx":"5","shortCiteRegEx":"5","year":2011},{"title":"Brainwaves are stethoscopes: ERP correlates of novel metaphor comprehension","author":["Yossi Arzouan","Abraham Goldstein","Miriam Faust"],"venue":"Brain research","citeRegEx":"6","shortCiteRegEx":"6","year":2007},{"title":"Ask for information retrieval: Part ii. results of a design study","author":["N.J. Belkin","H.M. Brooks"],"venue":"Journal of Documentation 38,","citeRegEx":"7","shortCiteRegEx":"7","year":1982},{"title":"The concept of relevance in IR","author":["Pia Borlund"],"venue":"Journal of the American Society for information Science and Technology 54,","citeRegEx":"8","shortCiteRegEx":"8","year":2003},{"title":"How context influences the interpretation of facial expressions: a source localization high-density EEG study on the “Kuleshov effect","author":["Marta Calbi","Francesca Siri","Katrin Heimann","Daniel Barratt","Vittorio Gallese","Anna Kolesnikov","Maria Alessandra Umiltà"],"venue":"Scientific reports 9,","citeRegEx":"9","shortCiteRegEx":"9","year":2019},{"title":"How does noise affect amplitude and latency measurement of event-related potentials (ERPs)? A methodological critique and simulation study","author":["Peter E Clayson","Scott A Baldwin","Michael J Larson"],"venue":"Psychophysiology 50,","citeRegEx":"10","shortCiteRegEx":"10","year":2013},{"title":"Characteristics of text affecting relevance judgments","author":["Colleen Cool","Nick Belkin","Ophir Frieder","Paul Kantor"],"venue":"In National online meeting,","citeRegEx":"11","shortCiteRegEx":"11","year":1993},{"title":"Dimensions of relevance","author":["Erica Cosijn","Peter Ingwersen"],"venue":"Information Processing & Management 36,","citeRegEx":"12","shortCiteRegEx":"12","year":2000},{"title":"The N400 potential could index a semantic inhibition","author":["J Bruno Debruille"],"venue":"Brain research reviews 56,","citeRegEx":"13","shortCiteRegEx":"13","year":2007},{"title":"Coregistration of eye movements and EEG in natural reading: analyses and review","author":["Olaf Dimigen","Werner Sommer","Annette Hohlfeld","Arthur M Jacobs","Reinhold Kliegl"],"venue":"Journal of Experimental Psychology: General 140,","citeRegEx":"14","shortCiteRegEx":"14","year":2011},{"title":"Natural brain-information interfaces: Recommending information by relevance inferred from human brain signals","author":["Manuel JA Eugster","Tuukka Ruotsalo","Michiel M Spapé","Oswald Barral","Niklas Ravaja","Giulio Jacucci","Samuel Kaski"],"venue":"Scientific reports","citeRegEx":"15","shortCiteRegEx":"15","year":2016},{"title":"Predicting Term-Relevance from Brain Signals","author":["Manuel J.A. Eugster","Tuukka Ruotsalo","Michiel M. Spapé","Ilkka Kosunen","Oswald Barral","Niklas Ravaja","Giulio Jacucci","Samuel Kaski"],"venue":"In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SI- GIR ’14)","citeRegEx":"16","shortCiteRegEx":"16","year":2014},{"title":"The truth will out: Interrogative polygraphy (“lie detection”) with event-related brain potentials","author":["Lawrence A Farwell","Emanuel Donchin"],"venue":"Psychophysiology 28,","citeRegEx":"17","shortCiteRegEx":"17","year":1991},{"title":"Fourteen-month-old infants track the language comprehension of communicative partners","author":["Bálint Forgács","Eugenio Parise","Gergely Csibra","György Gergely","Lisa Jacquey","Judit Gervain"],"venue":"Developmental science 22,","citeRegEx":"18","shortCiteRegEx":"18","year":2019},{"title":"Decision-making in information seeking on texts: an eye-fixation-related potentials investigation","author":["Aline Frey","Gelu Ionescu","Benoit Lemaire","Francisco López-Orozco","Thierry Baccino","Anne Guérin-Dugué"],"venue":"Frontiers in systems neuroscience","citeRegEx":"19","shortCiteRegEx":"19","year":2013},{"title":"Relevance reconsidered—Towards an agenda for the 21st century: Introduction to special topic issue on relevance research","author":["Thomas J Froehlich"],"venue":"Journal of the American Society for Information Science 45,","citeRegEx":"20","shortCiteRegEx":"20","year":1994},{"title":"Brain responses to contrastive and noncontrastive morphosyntactic structures in African American English and Mainstream American English: ERP evidence for the neural indices of dialect","author":["Felicidad Marcia Garcia"],"venue":null,"citeRegEx":"21","shortCiteRegEx":"21","year":2017},{"title":"Implicit relevance feedback from electroencephalography and eye tracking in image search","author":["Jan-Eike Golenia","Markus A Wenzel","Mihail Bogojeski","Benjamin Blankertz"],"venue":"Journal of neural engineering 15,","citeRegEx":"22","shortCiteRegEx":"22","year":2018},{"title":"P300 as an index of attention to self-relevant stimuli","author":["Heather M Gray","Nalini Ambady","William T Lowenthal","Patricia Deldin"],"venue":"Journal of experimental social psychology 40,","citeRegEx":"23","shortCiteRegEx":"23","year":2004},{"title":"Characterizing Relevance with Eye-Tracking Measures. In Proceedings of the 5th Information Interaction in Context Symposium (IIiX ’14)","author":["Jacek Gwizdka"],"venue":"Association for Computing Machinery,","citeRegEx":"24","shortCiteRegEx":"24","year":2014},{"title":"Temporal dynamics of eye-tracking and EEG during reading and relevance decisions","author":["Jacek Gwizdka","Rahilsadat Hosseini","Michael Cole","Shouyi Wang"],"venue":"Journal of the Association for Information Science and Technology 68,","citeRegEx":"25","shortCiteRegEx":"25","year":2017},{"title":"What does the brain of children with developmental dyslexia tell us Session 2B: User Behavior and Experience SIGIR ’20","author":["Sandra Hasko","Katarina Groth","Jennifer Bruder","Jürgen Bartling","Gerd Schulte- Körne"],"venue":"July 25–30,","citeRegEx":"26","shortCiteRegEx":"26","year":2014},{"title":"The foundation of the concept of relevance. Journal of the american society for information science and technology","author":["Birger Hjørland"],"venue":null,"citeRegEx":"27","shortCiteRegEx":"27","year":2010},{"title":"Integrating neurophysiologic relevance feedback in intent modeling for information retrieval","author":["Giulio Jacucci","Oswald Barral","Pedram Daee","MarkusWenzel","Baris Serim","Tuukka Ruotsalo","Patrik Pluchino","Jonathan Freeman","Luciano Gamberini","Samuel Kaski"],"venue":"Journal of the Association for Information Science and Technology","citeRegEx":"28","shortCiteRegEx":"28","year":2019},{"title":"Understanding Ephemeral State of Relevance","author":["Jiepu Jiang","Daqing He","Diane Kelly","James Allan"],"venue":"Association for Computing Machinery,","citeRegEx":"29","shortCiteRegEx":"29","year":2017},{"title":"The amplitude of the P300 component of the event-related potential: Review and synthesis","author":["R Johnson Jr."],"venue":"Advances in psychophysiology","citeRegEx":"30","shortCiteRegEx":"30","year":1988},{"title":"On how P300 amplitude varies with the utility of the eliciting stimuli","author":["Ray Johnson Jr.","Emanuel Donchin"],"venue":"Electroencephalography and Clinical Neurophysiology 44,","citeRegEx":"31","shortCiteRegEx":"31","year":1978},{"title":"Why Do Users Issue Good Queries? Neural Correlates of Term Specificity","author":["Lauri Kangassalo","Michiel Spapé","Giulio Jacucci","Tuukka Ruotsalo"],"venue":"In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’19). Association for Computing Machinery,","citeRegEx":"32","shortCiteRegEx":"32","year":2019},{"title":"Towards brain-activity-controlled information retrieval: Decoding image relevance from MEG","author":["Jukka-Pekka Kauppi","Melih Kandemir","Veli-Matti Saarinen","Lotta Hirvenkari","Lauri Parkkonen","Arto Klami","Riitta Hari","Samuel Kaski"],"venue":"signals. NeuroImage","citeRegEx":"33","shortCiteRegEx":"33","year":2015},{"title":"Binary and graded relevance in IR evaluations—comparison of the effects on ranking of IR systems","author":["Jaana Kekäläinen"],"venue":"Information processing & management 41,","citeRegEx":"34","shortCiteRegEx":"34","year":2005},{"title":"Using graded relevance assessments in IR evaluation","author":["Jaana Kekäläinen","Kalervo Järvelin"],"venue":"Journal of the American Society for Information Science and Technology 53,","citeRegEx":"35","shortCiteRegEx":"35","year":2002},{"title":"Display time as implicit feedback: understanding task effects","author":["Diane Kelly","Nicholas J Belkin"],"venue":"In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval","citeRegEx":"36","shortCiteRegEx":"36","year":2004},{"title":"ERP/MMR algorithm for classifying topic-relevant and topic-irrelevant visual shots of documentary videos","author":["Hyun Hee Kim","Yong Ho Kim"],"venue":"Journal of the Association for Information Science and Technology 70,","citeRegEx":"37","shortCiteRegEx":"37","year":2019},{"title":"A case for interaction: A study of interactive information retrieval behavior and effectiveness","author":["Jürgen Koenemann","Nicholas J Belkin"],"venue":"In Proceedings of the SIGCHI conference on human factors in computing systems","citeRegEx":"38","shortCiteRegEx":"38","year":1996},{"title":"Neural mechanisms of language comprehension: Challenges to syntax","author":["Gina R Kuperberg"],"venue":"Brain research","citeRegEx":"39","shortCiteRegEx":"39","year":2007},{"title":"Using Graded Implicit Feedback for Bayesian Personalized Ranking","author":["Lukas Lerche","Dietmar Jannach"],"venue":"In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys ’14)","citeRegEx":"40","shortCiteRegEx":"40","year":2014},{"title":"Categorical relevance judgment","author":["Mark Levene","J Bar-Ilan","M Zhitomirsky-Geffet"],"venue":"Journal of the Association for Information Science and Technology","citeRegEx":"41","shortCiteRegEx":"41","year":2018},{"title":"An introduction to the event-related potential technique","author":["Steven J Luck"],"venue":null,"citeRegEx":"42","shortCiteRegEx":"42","year":2014},{"title":"The Oxford handbook of event-related potential components","author":["Steven J Luck","Emily S Kappenman"],"venue":null,"citeRegEx":"43","shortCiteRegEx":"43","year":2011},{"title":"Why is that relevant? Collecting annotator rationales for relevance judgments","author":["Tyler McDonnell","Matthew Lease","Mucahid Kutlu","Tamer Elsayed"],"venue":"In Fourth AAAI Conference on Human Computation and Crowdsourcing","citeRegEx":"44","shortCiteRegEx":"44","year":2016},{"title":"Relevance: The whole history","author":["Stefano Mizzaro"],"venue":"Journal of the American society for information science 48,","citeRegEx":"45","shortCiteRegEx":"45","year":1997},{"title":"An effective implicit relevance feedback technique using affective, physiological and behavioural features","author":["Yashar Moshfeghi","Joemon M Jose"],"venue":"In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval","citeRegEx":"47","shortCiteRegEx":"47","year":2013},{"title":"Understanding relevance: an fMRI study","author":["Yashar Moshfeghi","Luisa R Pinto","Frank E Pollick","Joemon M Jose"],"venue":"In European conference on information","citeRegEx":"48","shortCiteRegEx":"48","year":2013},{"title":"Search process as transitions between neural states","author":["Yashar Moshfeghi","Frank E Pollick"],"venue":"In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee,","citeRegEx":"49","shortCiteRegEx":"49","year":2018},{"title":"Towards predicting a realisation of an information need based on brain signals","author":["Yashar Moshfeghi","Peter Triantafillou","Frank Pollick"],"venue":"In The World Wide Web Conference","citeRegEx":"50","shortCiteRegEx":"50","year":2019},{"title":"Understanding information need: An fMRI study","author":["Yashar Moshfeghi","Peter Triantafillou","Frank E Pollick"],"venue":"In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval","citeRegEx":"51","shortCiteRegEx":"51","year":2016},{"title":"ASK for information retrieval: Part I","author":["Robert N Oddy","NJ Belkin","Helen M Brooks"],"venue":"Background and theory. Emerald: Journal of Documentation","citeRegEx":"52","shortCiteRegEx":"52","year":1982},{"title":"Updating P300: an integrative theory of P3a and P3b","author":["John Polich"],"venue":"Clinical neurophysiology 118,","citeRegEx":"53","shortCiteRegEx":"53","year":2007},{"title":"Taking it easy when playing ultimatum game with a Down syndrome proposer: Effects on behavior and medial frontal negativity","author":["Gabriel Gaudencio Rêgo","Camila Campanhã","Julia Horta Tabosa do Egito","Paulo Sérgio Boggio"],"venue":"Social neuroscience 12,","citeRegEx":"54","shortCiteRegEx":"54","year":2017},{"title":"Extending average precision to graded relevance judgments","author":["Stephen E Robertson","Evangelos Kanoulas","Emine Yilmaz"],"venue":"In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval","citeRegEx":"55","shortCiteRegEx":"55","year":2010},{"title":"Emitted P300 potentials and temporal unvertainty","author":["DS Ruchkin","S Sutton"],"venue":"Electroencephalography and Clinical Neurophysiology 45,","citeRegEx":"56","shortCiteRegEx":"56","year":1978},{"title":"Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance","author":["Tefko Saracevic"],"venue":"Journal of the American Society for information Science and Technology 58,","citeRegEx":"57","shortCiteRegEx":"57","year":2007},{"title":"The Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Synthesis Lectures on Information Concepts, Retrieval, and Services","author":["Tefko Saracevic"],"venue":null,"citeRegEx":"58","shortCiteRegEx":"58","year":2016},{"title":"Relevance: The Search for a Definition","author":["Linda Schamber","Michael Eisenberg"],"venue":null,"citeRegEx":"59","shortCiteRegEx":"59","year":1988},{"title":"A reexamination of relevance: toward a dynamic, situational definition","author":["Linda Schamber","Michael B Eisenberg","Michael S Nilan"],"venue":"Information processing & management 26,","citeRegEx":"60","shortCiteRegEx":"60","year":1990},{"title":"Electrophysiological insights into language processing in schizophrenia","author":["Tatiana Sitnikova","Dean F Salisbury","Gina Kuperberg","Phillip J Holcomb"],"venue":"Psychophysiology 39,","citeRegEx":"61","shortCiteRegEx":"61","year":2002},{"title":"On the influence of task relevance and stimulus probability on event-relatedpotential components. Electroencephalography and clinical neurophysiology","author":["Kenneth C Squires","Emanuel Donchin","Ronald I Herning","Gregory McCarthy"],"venue":null,"citeRegEx":"62","shortCiteRegEx":"62","year":1977},{"title":"Gender-selective effects of the P300 and N400 components of the visual evoked potential","author":["Scott C Steffensen","Allison J Ohran","Daniel N Shipp","Kimberly Hales","Sarah H Stobbs","Donovan E Fleming"],"venue":"Vision research 48,","citeRegEx":"63","shortCiteRegEx":"63","year":2008},{"title":"Searching for the names of pictures: An event-related potential study","author":["Don T Stuss","TW Picton","AM Cerri"],"venue":"Psychophysiology 23,","citeRegEx":"64","shortCiteRegEx":"64","year":1986},{"title":"Realtime inference of word relevance from electroencephalogram and eye gaze","author":["Markus Andreas Wenzel","Mihail Bogojeski","Benjamin Blankertz"],"venue":"Journal of neural engineering 14,","citeRegEx":"65","shortCiteRegEx":"65","year":2017},{"title":"Relevance theory and distributions of judgments in document retrieval","author":["Howard D White"],"venue":"Information Processing & Management 53,","citeRegEx":"66","shortCiteRegEx":"66","year":2017},{"title":"The Use of Implicit Evidence for Relevance Feedback in Web Retrieval","author":["Ryen W. White","Ian Ruthven","Joemon M. Jose"],"venue":"In Advances in Information Retrieval,","citeRegEx":"67","shortCiteRegEx":"67","year":2002},{"title":"Retrieval intention modulates the effects of directed forgetting instructions on recollection","author":["Xin Xiao","Heather D Lucas","Ken A Paller","Jin-hong Ding","Chun-yan Guo"],"venue":"PloS one 9,","citeRegEx":"68","shortCiteRegEx":"68","year":2014},{"title":"Late positive complex in event-related potentials tracks memory signals when they are decision relevant","author":["Haopei Yang","Geoffrey Laforge","Bobby Stojanoski","Emily S Nichols","Ken McRae","Stefan Köhler"],"venue":"Scientific reports 9,","citeRegEx":"69","shortCiteRegEx":"69","year":2019},{"title":"Promoting effects of color-text congruence in banner advertising","author":["Tengxiao Zhang","Chuting Bao","Chunqu Xiao"],"venue":"Color Research & Application 44,","citeRegEx":"70","shortCiteRegEx":"70","year":2019},{"title":"How and why do users change their assessment of search results over time? Proceedings of the Association for Information","author":["Maayan Zhitomirsky-Geffet","Judit Bar-Ilan","Mark Levene"],"venue":"Science and Technology 52,","citeRegEx":"71","shortCiteRegEx":"71","year":2015}],"referenceMentions":[{"referenceID":11,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":53,"endOffset":65},{"referenceID":44,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":53,"endOffset":65},{"referenceID":55,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":53,"endOffset":65},{"referenceID":10,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":87,"endOffset":103},{"referenceID":19,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":87,"endOffset":103},{"referenceID":58,"context":"While relevance is considered to be multidimensional [12, 45, 57], dynamic and complex [11, 20, 46, 60], there are still debates around the granularity level of relevance judgements that","startOffset":87,"endOffset":103},{"referenceID":39,"context":"The value of evaluating information based on graded relevance has begun to receive attention in recent years both from system [40, 55] and user","startOffset":126,"endOffset":134},{"referenceID":53,"context":"The value of evaluating information based on graded relevance has begun to receive attention in recent years both from system [40, 55] and user","startOffset":126,"endOffset":134},{"referenceID":53,"context":"This, therefore, limits the understanding of how searchers perceive different degrees of information relevance [55].","startOffset":111,"endOffset":115},{"referenceID":46,"context":"Relevance is also known to be subjective and difficult to quantify [48] since it depends on a searcher’s perception of information relating to a specific Information Need (IN) at a certain point in time [8, 57].","startOffset":67,"endOffset":71},{"referenceID":7,"context":"Relevance is also known to be subjective and difficult to quantify [48] since it depends on a searcher’s perception of information relating to a specific Information Need (IN) at a certain point in time [8, 57].","startOffset":203,"endOffset":210},{"referenceID":55,"context":"Relevance is also known to be subjective and difficult to quantify [48] since it depends on a searcher’s perception of information relating to a specific Information Need (IN) at a certain point in time [8, 57].","startOffset":203,"endOffset":210},{"referenceID":6,"context":"However, given the semantic gap between a searcher’s IN and their formulated queries [7, 27, 52], IR systems have employed various techniques to capture the subjective aspect of relevance [2] to improve the effectiveness of retrieved results.","startOffset":85,"endOffset":96},{"referenceID":26,"context":"However, given the semantic gap between a searcher’s IN and their formulated queries [7, 27, 52], IR systems have employed various techniques to capture the subjective aspect of relevance [2] to improve the effectiveness of retrieved results.","startOffset":85,"endOffset":96},{"referenceID":50,"context":"However, given the semantic gap between a searcher’s IN and their formulated queries [7, 27, 52], IR systems have employed various techniques to capture the subjective aspect of relevance [2] to improve the effectiveness of retrieved results.","startOffset":85,"endOffset":96},{"referenceID":1,"context":"However, given the semantic gap between a searcher’s IN and their formulated queries [7, 27, 52], IR systems have employed various techniques to capture the subjective aspect of relevance [2] to improve the effectiveness of retrieved results.","startOffset":188,"endOffset":191},{"referenceID":37,"context":"Examples of such techniques are explicit [38], implicit (e.","startOffset":41,"endOffset":45},{"referenceID":23,"context":"g.[24, 36]) and physiological [47] feedback.","startOffset":2,"endOffset":10},{"referenceID":35,"context":"g.[24, 36]) and physiological [47] feedback.","startOffset":2,"endOffset":10},{"referenceID":45,"context":"[24, 36]) and physiological [47] feedback.","startOffset":28,"endOffset":32},{"referenceID":1,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":14,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":15,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":21,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":24,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":27,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":32,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":36,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":46,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":63,"context":"More recently, researchers have shown the possibility of capturing the neural processes associated with relevance, using brain imaging techniques [2, 15, 16, 22, 25, 28, 33, 37, 48, 65].","startOffset":146,"endOffset":185},{"referenceID":14,"context":"relevance of a word with respect to another) [15, 16, 65] without subjects experiencing any IN; or investigated relevance in the context of Information Retrieval (IR) when IN has been introduced to subjects.","startOffset":45,"endOffset":57},{"referenceID":15,"context":"relevance of a word with respect to another) [15, 16, 65] without subjects experiencing any IN; or investigated relevance in the context of Information Retrieval (IR) when IN has been introduced to subjects.","startOffset":45,"endOffset":57},{"referenceID":63,"context":"relevance of a word with respect to another) [15, 16, 65] without subjects experiencing any IN; or investigated relevance in the context of Information Retrieval (IR) when IN has been introduced to subjects.","startOffset":45,"endOffset":57},{"referenceID":1,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":18,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":21,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":23,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":24,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":27,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":36,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":46,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":47,"context":"In the latter scenario, relevance was investigated only as a binary notion [2, 19, 22, 24, 25, 28, 37, 48, 49] leaving the graded nature of it unexplored.","startOffset":75,"endOffset":110},{"referenceID":7,"context":"It plays a crucial role in the user-system interaction since it is a substantial indicator of system retrieval performance [8, 57].","startOffset":123,"endOffset":130},{"referenceID":55,"context":"It plays a crucial role in the user-system interaction since it is a substantial indicator of system retrieval performance [8, 57].","startOffset":123,"endOffset":130},{"referenceID":33,"context":"Within the system side, graded relevance (in comparison to the binary one) has been shown to improve ranking functions [34, 55].","startOffset":119,"endOffset":127},{"referenceID":53,"context":"Within the system side, graded relevance (in comparison to the binary one) has been shown to improve ranking functions [34, 55].","startOffset":119,"endOffset":127},{"referenceID":69,"context":"Within the user side, recent research supports the idea of categorical thinking [71], suggesting that users divide retrieved results into 3-5 categories based on relevance [41].","startOffset":80,"endOffset":84},{"referenceID":40,"context":"Within the user side, recent research supports the idea of categorical thinking [71], suggesting that users divide retrieved results into 3-5 categories based on relevance [41].","startOffset":172,"endOffset":176},{"referenceID":45,"context":"Explicit feedback is easy to use, however, difficult to obtain due to the cognitive burden associated with it [47], as the user is required to explicitly state whether presented content is subjectively perceived as relevant or not [67].","startOffset":110,"endOffset":114},{"referenceID":65,"context":"Explicit feedback is easy to use, however, difficult to obtain due to the cognitive burden associated with it [47], as the user is required to explicitly state whether presented content is subjectively perceived as relevant or not [67].","startOffset":231,"endOffset":235},{"referenceID":35,"context":"e.[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47].","startOffset":2,"endOffset":6},{"referenceID":23,"context":"[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47].","startOffset":37,"endOffset":41},{"referenceID":3,"context":"[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47].","startOffset":80,"endOffset":87},{"referenceID":45,"context":"[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47].","startOffset":80,"endOffset":87},{"referenceID":45,"context":"[36]), eye-tracking and pupillometry [24], and/or the measurements of affective [4, 47] and physiological signals [47].","startOffset":114,"endOffset":118},{"referenceID":1,"context":"However, implicit feedback is often found to be noisy, which decreases its accuracy [2].","startOffset":84,"endOffset":87},{"referenceID":48,"context":"One particular area of emphasis for this research has been to examine the IN process [50, 51].","startOffset":85,"endOffset":93},{"referenceID":49,"context":"One particular area of emphasis for this research has been to examine the IN process [50, 51].","startOffset":85,"endOffset":93},{"referenceID":48,"context":"In addition, it has been found that prediction of the IN state experienced by a user is possible using brain signals [50].","startOffset":117,"endOffset":121},{"referenceID":27,"context":"understanding of other parts of the information seeking and retrieval process, such as query formulation [28], search [49, 70] and relevance (e.","startOffset":105,"endOffset":109},{"referenceID":47,"context":"understanding of other parts of the information seeking and retrieval process, such as query formulation [28], search [49, 70] and relevance (e.","startOffset":118,"endOffset":126},{"referenceID":68,"context":"understanding of other parts of the information seeking and retrieval process, such as query formulation [28], search [49, 70] and relevance (e.","startOffset":118,"endOffset":126},{"referenceID":46,"context":"For instance, Moshfeghi and colleagues [48] employed functional magnetic resonance imaging (fMRI), to localise differences in brain activity in cortical regions during the processing of relevant vs non-relevant images.","startOffset":39,"endOffset":43},{"referenceID":47,"context":"was able to identify regions engaged in the relevance judgement processing and the increased activation of these regions for relevant items was related to visuospatial working memory [49, 51].","startOffset":183,"endOffset":191},{"referenceID":49,"context":"was able to identify regions engaged in the relevance judgement processing and the increased activation of these regions for relevant items was related to visuospatial working memory [49, 51].","startOffset":183,"endOffset":191},{"referenceID":18,"context":"processing of non-relevant and relevant words, that persisted for approximately 260 to 320 ms for relevant words and 500 to 530ms for an irrelevant word [19].","startOffset":153,"endOffset":157},{"referenceID":24,"context":"Relevance has been inferred using EEG [25] or in combination","startOffset":38,"endOffset":42},{"referenceID":21,"context":"with pupillometry or/and eye-tracking devices [22] within the context of the IR task, not only for textual stimuli but also for videos [37] and images [2].","startOffset":46,"endOffset":50},{"referenceID":36,"context":"with pupillometry or/and eye-tracking devices [22] within the context of the IR task, not only for textual stimuli but also for videos [37] and images [2].","startOffset":135,"endOffset":139},{"referenceID":1,"context":"with pupillometry or/and eye-tracking devices [22] within the context of the IR task, not only for textual stimuli but also for videos [37] and images [2].","startOffset":151,"endOffset":154},{"referenceID":1,"context":"(2015) examined the processing of relevant vs non-relevant images, finding the most significant differences to occur between 500 – 800ms [2].","startOffset":137,"endOffset":140},{"referenceID":36,"context":"Kim (2019) explored the ERPs associated with topical relevance of video skims and classified the data based on two specific ERP components (N400 and P600), which have been shown to be indicators of relevant and non-relevant judgements [37].","startOffset":235,"endOffset":239},{"referenceID":27,"context":"Moreover, recent findings have shown that relevance can be predicted in real-time from EEG brain signals and eye movements while the user engages with the system and IR task [28].","startOffset":174,"endOffset":178},{"referenceID":14,"context":"Another line of research has examined relevance in the context of word associations, employing EEG in isolation, or in combination with eye gaze [15, 16, 65].","startOffset":145,"endOffset":157},{"referenceID":15,"context":"Another line of research has examined relevance in the context of word associations, employing EEG in isolation, or in combination with eye gaze [15, 16, 65].","startOffset":145,"endOffset":157},{"referenceID":63,"context":"Another line of research has examined relevance in the context of word associations, employing EEG in isolation, or in combination with eye gaze [15, 16, 65].","startOffset":145,"endOffset":157},{"referenceID":14,"context":"Later, Eugster and colleagues (2016) introduced a brainrelevance paradigm enabling recommendation of information to users without any explicit user interaction, based on EEG signals alone evoked by users’ engagement with the textual content [15].","startOffset":241,"endOffset":245},{"referenceID":42,"context":"The P300 component is a positive-going voltage deflection that peaks around 300ms post-stimulus [43].","startOffset":96,"endOffset":100},{"referenceID":51,"context":"This time-locked ERP component has been included in the analysis as it is thought to reflect the amount of cognitive resource employed for information processing [53].","startOffset":162,"endOffset":166},{"referenceID":42,"context":"The N400 component is a negative-going potential, reaching its peak at 400ms from the stimulus presentation [43].","startOffset":108,"endOffset":112},{"referenceID":42,"context":"Words that can be integratedmore easily to the context elicit smaller (more positive) amplitude and vice versa [43].","startOffset":111,"endOffset":115},{"referenceID":42,"context":"The P600 component is characterised as a positive deflection, peaking around 600ms from stimulus onset [43].","startOffset":103,"endOffset":107},{"referenceID":31,"context":"It is elicited during cognitive processing involved in text comprehension [32].","startOffset":74,"endOffset":78},{"referenceID":13,"context":"This approach has traditionally been applied in the ERP studies examining neurological correlates of reading [14].","startOffset":109,"endOffset":113},{"referenceID":14,"context":"This reading pace was appropriate to model fluent reading as much as possible and to avoid the presence of the overlapping effect of two consecutive words on the ERPs [15].","startOffset":167,"endOffset":171},{"referenceID":55,"context":"The interpretation of each graded relevance category depended on each participant’s subjective understanding, which enabled capturing the subjective nature of relevance judgement [57].","startOffset":179,"endOffset":183},{"referenceID":47,"context":"We chose this dataset since it has been widely in previous studies investigating IR phenomena from a neuroscience perspective [49, 51].","startOffset":126,"endOffset":134},{"referenceID":49,"context":"We chose this dataset since it has been widely in previous studies investigating IR phenomena from a neuroscience perspective [49, 51].","startOffset":126,"endOffset":134},{"referenceID":14,"context":"This randomisation ensured that the recorded signals and effects were related to the users’ subjective relevance judgement of the presented stimuli, and not related to the stimulus presentation frequency, potentially causing an oddball effect [15].","startOffset":243,"endOffset":247},{"referenceID":8,"context":"Next, we removed the outermost belt of electrodes of the sensor net (19 peripheral channels: E43, E48, E49, E56, E63, E68, E73, E81, E88, E94, E99, E107, E113, E119, E120, E125, E126, E127, E128)[9].","startOffset":195,"endOffset":198},{"referenceID":2,"context":"of 220–440 ms from stimulus onset [3, 53] and the representative subset of 22 centro-parietal channels over the midline (E55, E62), left (LH: E30, E36, E37, E42, E52, E53, E54, E60, E61, E67) and right hemispheres (RH: E77, E78, E79, E85, E86, E87, E92, E93, E104, E105) as shown in Figure 2a.","startOffset":34,"endOffset":41},{"referenceID":51,"context":"of 220–440 ms from stimulus onset [3, 53] and the representative subset of 22 centro-parietal channels over the midline (E55, E62), left (LH: E30, E36, E37, E42, E52, E53, E54, E60, E61, E67) and right hemispheres (RH: E77, E78, E79, E85, E86, E87, E92, E93, E104, E105) as shown in Figure 2a.","startOffset":34,"endOffset":41},{"referenceID":52,"context":"previous literature examining the P300 component through the calculation of mean amplitudes [54].","startOffset":92,"endOffset":96},{"referenceID":17,"context":"The mean amplitudes for N400 component were measured between 400 – 600 ms [18].","startOffset":74,"endOffset":78},{"referenceID":25,"context":"E93), including mid-line channels (E55, E62) [26] as shown in Figure 2b.","startOffset":45,"endOffset":49},{"referenceID":5,"context":"We selected the time window between 550 – 750 ms [6] and a bilateral group of electrodes covering central, parietal and temporal regions (E52, E53, E54, E59, E60, E61, E66, E67, E72, E77,","startOffset":49,"endOffset":52},{"referenceID":20,"context":"E78, E79, E80, E85, E86, E87, E92, E93) [21] as shown in Figure 2c.","startOffset":40,"endOffset":44},{"referenceID":9,"context":"For this study, we calculated the mean amplitude, which enabled us to factor out latency jitter and to obtain more robust results [10, 42].","startOffset":130,"endOffset":138},{"referenceID":41,"context":"For this study, we calculated the mean amplitude, which enabled us to factor out latency jitter and to obtain more robust results [10, 42].","startOffset":130,"endOffset":138},{"referenceID":0,"context":"stage of implicit relevance judgement, users’ selective attention is allocated towards highly relevant stimuli, which are also easier to process in terms of cognitive load [1, 53].","startOffset":172,"endOffset":179},{"referenceID":51,"context":"stage of implicit relevance judgement, users’ selective attention is allocated towards highly relevant stimuli, which are also easier to process in terms of cognitive load [1, 53].","startOffset":172,"endOffset":179},{"referenceID":1,"context":"These findings are consistent with the previous literature, showing that the degree of subjectively perceived information relevance is proportional to the P300 component amplitude [2, 23].","startOffset":180,"endOffset":187},{"referenceID":22,"context":"These findings are consistent with the previous literature, showing that the degree of subjectively perceived information relevance is proportional to the P300 component amplitude [2, 23].","startOffset":180,"endOffset":187},{"referenceID":14,"context":"associated with negative-going deflection, which was the most prominent for the NONR condition, which is consistent with previous literature [15, 37, 65].","startOffset":141,"endOffset":153},{"referenceID":36,"context":"associated with negative-going deflection, which was the most prominent for the NONR condition, which is consistent with previous literature [15, 37, 65].","startOffset":141,"endOffset":153},{"referenceID":63,"context":"associated with negative-going deflection, which was the most prominent for the NONR condition, which is consistent with previous literature [15, 37, 65].","startOffset":141,"endOffset":153},{"referenceID":61,"context":"Although highly relevant information reduces the N400 amplitude [63], our results indicate that the N400 Session 2B: User Behavior and Experience SIGIR ’20, July 25–30, 2020, Virtual Event, China","startOffset":64,"endOffset":68},{"referenceID":12,"context":"component might indicate that LOWR (and NONR) stimuli require significantly greater cognitive effort to process and integrate within the given context [13].","startOffset":151,"endOffset":155},{"referenceID":4,"context":"The context within this experiment has been provided through the question, and hence it is possible to assume that higher N400 amplitudes elicited during NONR condition signalise contextual violation [5] and a degree of uncertainty during","startOffset":200,"endOffset":203},{"referenceID":14,"context":"Our findings are in alignment with previous studies, suggesting that processing of NONR information is associated with low P600 amplitudes [15, 37].","startOffset":139,"endOffset":147},{"referenceID":36,"context":"Our findings are in alignment with previous studies, suggesting that processing of NONR information is associated with low P600 amplitudes [15, 37].","startOffset":139,"endOffset":147},{"referenceID":31,"context":"that the amount of information carried by the processed term is higher in comparison to the LOWR condition [32].","startOffset":107,"endOffset":111},{"referenceID":29,"context":"P300 amplitude has been shown to be proportional to attentional engagement [30].","startOffset":75,"endOffset":79},{"referenceID":54,"context":"Greater P300 amplitude has also been suggested to reflect the quantity of information transmitted [56], the quantity of useful information [31], relevance to the self [23], or relevance to the task","startOffset":98,"endOffset":102},{"referenceID":30,"context":"Greater P300 amplitude has also been suggested to reflect the quantity of information transmitted [56], the quantity of useful information [31], relevance to the self [23], or relevance to the task","startOffset":139,"endOffset":143},{"referenceID":22,"context":"Greater P300 amplitude has also been suggested to reflect the quantity of information transmitted [56], the quantity of useful information [31], relevance to the self [23], or relevance to the task","startOffset":167,"endOffset":171},{"referenceID":16,"context":"[17, 62], or to judgements of relevance specifically [2].","startOffset":0,"endOffset":8},{"referenceID":60,"context":"[17, 62], or to judgements of relevance specifically [2].","startOffset":0,"endOffset":8},{"referenceID":1,"context":"[17, 62], or to judgements of relevance specifically [2].","startOffset":53,"endOffset":56},{"referenceID":14,"context":"[15], when assessing term relevance, did not find a difference in the P300 between relevant and irrelevant words.","startOffset":0,"endOffset":4},{"referenceID":4,"context":"The underlying processes also might not be wholly independent [5].","startOffset":62,"endOffset":65},{"referenceID":14,"context":"The N400 results fit closely with the study by Eugster and colleagues [15], who found a reduced N400 for relevant words, compared to irrelevant words.","startOffset":70,"endOffset":74},{"referenceID":14,"context":"[15] found that their relevant words elicited larger P600 components than irrelevant words.","startOffset":0,"endOffset":4},{"referenceID":67,"context":"A late positive complex has been observed during memory recognition, it is higher for old versus new stimuli, occurs at around 600ms after a stimulus and also has a central posterior topography [69].","startOffset":194,"endOffset":198}],"year":2020,"abstractText":"Relevance is an essential concept in Information Retrieval (IR). Recent studies using brain imaging have significantly contributed towards the understanding of this concept, but only as a binary notion, i.e. a document being judged as relevant or non-relevant. While such a binary division is prevalent in IR, seminal theories have proposed relevance as a graded variable; i.e. having different degrees. In this paper, we aim to investigate the brain activity associated with relevance when it is treated as a graded concept. Twenty-five participants provided graded relevance judgements in the context of a Question Answering (Q/A) Task, during assessment with an electroencephalogram (EEG). Our findings show that significant differences in event-related potentials (ERPs) were observed in response to information segments processed in the context of high-relevance, low-relevance and no-relevance, supporting the concept of graded relevance. We speculate that differences in attentional engagement, semantic mismatch (between the question and answer) and memory processing underpin the electrophysiological responses to the graded relevance judgements. We believe our conclusions constitute an important step in unravelling the nature of graded relevance and knowledge of the electrophysiological modulation to each grade of relevance will help to improve the design and evaluation of IR systems.","creator":"LaTeX with acmart 2019/12/18 v1.66 Typesetting articles for the Association for Computing Machinery and hyperref 2019/11/10 v7.00c Hypertext links for LaTeX"}}
{"name":"3397271.3401207.pdf","metadata":{"source":"CRF","title":"Retrieving Potential Causes from aQuery Event","authors":["Suchana Datta","Debasis Ganguly","Dwaipayan Roy","Francesca Bonin","Charles Jochim","Mandar Mitra"],"emails":["suchana.datta@ucdconnect.ie","debasis.ganguly1@ie.ibm.com","dwaipayan.roy@gesis.org","fbonin@ie.ibm.com","charlesj@ie.ibm.com","mandar@isical.ac.in","permissions@acm.org."],"sections":[{"heading":null,"text":"ACM Reference Format: Suchana Datta, Debasis Ganguly, Dwaipayan Roy, Francesca Bonin, Charles Jochim, and Mandar Mitra. 2020. Retrieving Potential Causes from a Query Event . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25– 30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3397271.3401207"},{"heading":"1 INTRODUCTION","text":"In this paper, we explore a novel direction of augmenting the classic ranked list of topically relevant search results with an additional list of causally relevant results. A causally relevant document is one that provides information on the likely causes leading to an event specified within the user query. As a concrete example, on submitted queries pertaining to a specific news event (e.g., ‘drop of pound’ or ‘housing crisis’), a causal search system retrieves potentially relevant information required to construct further analysis for the purpose of automated (or semi-automated with humans-in-loop)\n∗Research conducted during author’s affiliation at Indian Statistical Institute, Kolkata.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401207\ndecision and policy making. Moreover, information extracted from causally related documents could also serve as the necessary explanations in order to support an automatically generated decision prescribing ways to eradicate a likely cause.\nThe study of causality in other domains, e.g. information extraction, is typically useful for finding explicitly mentioned causes, e.g. specified with typical patterns within a single sentence, such as ‘X leads to Y’ etc. However, the notion of causality that we address in the context of our work is more subtle; given a query regarding an event, the causal information need of the user is to find out the precursors of the event that lead to the event being taken place. Another point of difference of our work with existing ones is that we intend to retrieve causal information in the form of documents (query-document causal relations), the scale of which is typically much larger (and more subtle) than sentence level causality, such as ‘Heavy rain causes flood’.\nOur work involves finding out the causally relevant information in an unsupervised way given a user-specified query describing an event with chronological dependency (e.g. ‘pound drop’ a potential cause of which is ‘Brexit’). In particular, we propose a pseudorelevance feedback model based on the assumption that causally relevant documents are likely to have only a partial term overlap with the topically relevant ones, e.g., although the top-retrieved documents retrieved for the query ‘pound drop’ are likely not to contain terms, such as ‘Brexit’ or ‘EU’, it is likely that a number of documents beyond the top-ranks would indeed contain such terms. To address this, we propose a relevance feedback model that intends to estimate the distribution of causal terms that are relatively infrequent but associated with high weights in the topical relevance model. We observe that the association of these terms is significantly high in the topically relevant distribution, which\nin effect leads to deviating away from topical relevance towards potential causal relevance.\nThe schematics in Figure 1 illustrates our idea that topical relevance (T ) corresponds to the top-ranked documents, whereas the presence of causally relevant documents (C) may stretch further down the ranked list. In our proposed factored model, we use term distribution information from both these sets to build up a query with a broader aspect. Finally, we favour those documents which comprise terms that have higher weights in C and lower in T ."},{"heading":"2 RELATEDWORK","text":"A number of existing approaches extract cause-effect patterns using lexical, syntactic, and more recently, semantic relations [3–5, 12], primarily taken from headlines or single sentences (refer to [1] for a survey). The cause-effect pattern approach was extended in [7], where a set of patterns are initially used to create a network of causes and effects, and then a relational embedding method (similar to TransE [2]) is used to jointly embed causes and effects. Causal IR works in the reverse direction, where a query describes a cause (e.g., current situation or proposed action) and the results provide a list of possible effects was studied in [9, 10]."},{"heading":"3 FACTORED CAUSAL RELEVANCE MODEL","text":"We estimate a causal relevance model θC as a function of the topical relevance model θT , which represents a set of terms that are directly related to the core topic of the query. This eventually expands the set of initial query terms because a small number of initial query terms is likely not to contain adequate information to help find the causally relevant terms (and eventually the documents). We estimate a standard topical relevance model, θT as:\nP(w |θT ) = M∑ i=1 P(w |Di ) ∏ q∈Q P(q |Di ), (1)\nwhere the weights P(w |θT ) capture the co-occurrences between terms in theM top-retrieved documents, and query terms qi (∈ Q).\nWe consider these estimated terms in θT has the ability of making the query more general, which according to our hypothesis is more useful to find causal relevance (as compared to the keyword query pointing to an effect event). Our intention is to balance the trade-off that terms in the causal model should, on one hand, be correlated well with the general concepts ofθT , and should not be concentrated on solely the query (effect event) specific terms on the other. Thus, we formulate causal relevance model θC as:\nP(w |θC ,θT ) = M∑ i=1 P(w |Di ) ∏\n∀t϶P (t |θT ),0\nP(t |Di ) P(t |θT ) . (2)\nSpecifically, Equation 2 reveals that in contrast to topical relevance model θT [11] estimation, we employ an odds-ratio between the probability of a term occurring within a document retrieved (with a more general query), and the probability of the same term estimated by θT (more specific effect event query). Inspecting the ratio term inside the product sign indicates that this way of estimation favours words that are: a) relatively infrequent in θT (i.e., terms not too specific to the effect event itself) through a decrease in the denominator; and b) frequent in the top-ranked documents retrieved with\nthe more general query, (i.e., terms related to more general aspects of the query, which as per our hypothesis constitutes a number of potential causes) through an increase in the numerator.\nFinally, by substituting Equation 1 into 2, we get the final estimation model as:\nP(w |θC ,θT ) = M∑ i=1 P(w |Di ) ∏ t ∈θT P(t |Di )∑M j=1 P(t |D j ) ∏ q∈Q P(q |D j ) .\n(3) The qualifier ‘factored’ in the proposed model name indicates that the causal model is factorized into topical which then leads to estimating the causal one, as seen from Equation 3. Following the exposition of [8], the query model is linearly interpolated with the factored model in order to finalize the expansion term weights."},{"heading":"4 EVALUATION","text":"Dataset Characteristics. Causality-based queries are associated with information needs with chronological dependency and having cause-effect relationship e.g. ‘outbreak of a war between nations’, ‘major economic crisis’ etc. Events for which there is a single cause, which is rather evident in nature (e.g., the cause is revealed in the article about the effect itself), are not interesting from the perspective of the causal retrieval task definition. In contrast to the direct cause-event relationships, we, in this work, are rather interested in those cases where pieces of causal relations are distributed across a number of different articles. The notion of causal relevance is determined by whether the information in a document relates to a potential cause of the effect specified in the given query.\nTable 1 illustrates the differences between the two types of relevance for a sample query on the ‘assassination of Osama bin Laden’. It can be observed that while the notion of traditional relevance (RT ) corresponds to the topic itself (the sample topically relevant document reports the possibility of Laden’s death), the sample causally relevant (RC ) document comprises information about events that might have led to bin Laden’s death (as shown in the bold-faced text).\nDocument Collection. Documents from the FIRE ad-hoc track English [6] was used as the target collection. It is comprised of 303, 291 news articles crawled from Telegraph India1.\nTopics. As topics, we used our prior knowledge about the news events in the collection to select a set of events, such that for each event it can be reasoned that a number of factors could have been responsible in leading towards it. We exclude those cases where the causality factor is either too obvious (mentioned in the same document that describes the query event itself) or the number of such factors is too small in number (≤ 1). We also ensured that a query (topic)2 is representative of an event that occurred during 1https://www.telegraphindia.com/ 2Available at https://github.com/suchanadatta/Factored-Causal-RLM\nthe period covered by the target collection, i.e. between 2001-2011. We compiled a total of 20 topics for our study.\nRelevance Assessments. A pool of documents for manual relevance assessments in standard (i.e., topical relevance) IR is usually constructed by combining top-ranked documents retrieved by a number of systems (various IR models with different settings) [13]. In case of causal retrieval, this conventional way to construct a pool is likely not to work well because of two main reasons. Firstly, in contrast to topical IR, there exist no empirically well established model for causal IR (in fact, the purpose of developing manual assessments is to fill the void). However, relying on the proposed causal relevance model and a number of standard topical IR models (e.g. BM25, LM etc.) alone cannot ensure inclusion of the truly relevant documents in the pool. Secondly, unlike topical relevance judgement, assessors must have some prior knowledge on the event(mentioned in the query) without which it would be difficult to assess a causally connected event. To alleviate these issues, we treated causation finding for a topic as an exploratory task involving a series of query formulations and reformulations.\nTo aid our exploratory task, we used an interactive system that allowed bookmarking documents for future use (e.g., start exploring along a particular aspect of a potential cause of the main topic). At the end of the exploratory task, these bookmarked documents, being indicative of potentially relevant documents to establish the causal links with the query topic, were added to an assessment pool. Further, top-100 documents, retrieved with standard IR and feedback models, (specifically, LM, BM25 and RLM) were added to this pool. Documents from this pool were then assessed (binary judgments) using both prior knowledge on the topic and the knowledge gained during the exploratory session.\nMethods investigated. Our first baseline is a standard IR model (specifically, linear smoothed language model) without feedback. Since our proposed approach is a factored feedback model, we employ the conventional relevance model based expansion technique RM3 [8] as our second baseline. As another baseline, we employ RM3 feedback to estimate θC with a modification: instead of assuming that the top-retrieved documents as relevant, we rather assume that the documents beyond the top-retrieved ones would be useful to estimate causal relevance. The intuition is as presented in Figure 1. For this, we swap the topM documents with a different set of M documents, Cr = {Dp , . . . ,Dp+M }, where p > M (i.e. an interval of documents of sizeM following the top-M). This baseline makes use of only the causal relevance assumption and it disregards information from the top-retrieved ones. We name this baseline ‘CRLM’ (RLM with causal relevance).\nThe next methodology that we investigate is to make queries more specific towards a causal information need. To illustrate with an example, the query ‘drop in pound value’ can be made more specific to a causal information need by explicitly adding causeindicative keywords such as ‘causes’ or ‘reasons’ to the query. The purpose of reformulating the queries this way is to investigate\nif existing retrieval models could adequately address the causal information need if queries themselves explicitly indicate that information is sought on the causes related to an event and not the event itself, e.g. ‘reasons for the drop in pound value’ or ‘causes for the drop in pound value’.\nFor our experiments, we automatically constructed a set of such causality related keywords, which we add to make an initial query on an event more specific to seeking the causes for the event. A possible way of selecting these terms is to leverage synonyms of the word ’cause’, as enumerated in Table 2. To find out if standard retrieval models on causally reformulated queries (with terms from Table 2) proves useful for retrieving causal information, we repeat all the previously mentioned baseline approaches on causally reformulated queries, which we denote by the suffix ‘CSR’ (abbreviation for Causality Specific Reformulation), e.g., ‘No-QE-CSR’ indicates LM based retrieval that uses queries augmented with the causality indicating terms shown in Table 2.\nParameter Settings. The common parameters in the methods investigated are a) the number of top-ranked documents,M , to consider as pseudo relevant, b) T , the number of top scored terms (P(w |R) values), used for computing the KL divergence for reranking within a standard RLM setup [8], and c) the mixture parameter, λ, used in query likelihood. We optimize these parameters separately for each feedback method by grid search. The purpose of parameter tuning is not to claim that the best performing method also generalizes in the best possible way for other topics (typical of learning parameters in a supervised task), but rather to observe the best results achievable by each individual method. Grid search also ensures that the comparisons between the unsupervised methods are fair. Optimal results for each method, along with the optimal parameter values, are shown in Table 3.\nResults. Table 3 shows comparison between the results obtained with the proposed method and the baselines. We enlist a number of observations as follows. Firstly, it can be noted that a standard (topical) feedback approach, such as RM3, results in a marginal improvement over the initial retrieval step (No-QE). This shows that applying off-the-shelf relevance feedback approaches may not prove effective in retrieving causally relevant information. So applying term expansions to diversify the query in order to find causally relevant terms may not help either because, as per our hypothesis, such terms are likely to have low likelihood of being generated\nfrom top-ranked documents only (which is the assumption in a standard feedback model).\nSecondly, it can be seen that CRLM with its relatively simple heuristics of using pseudo-relevant information from documents deeper down the ranked list, is not able to improve results, which means terms not too related to the main information need (terms from mid-ranged documents) are rather more likely to be topically non-relevant than being causally relevant.\nThirdly, the approach of making a query more specific by explicitly adding the cause indicating terms (Table 2) usually contributes to decreasing retrieval effectiveness for all the above mentioned baseline approaches. This can be attributed to the fact that most causally relevant documents are likely not to explicitly mention the potential causes for an event, through the use of cause indicating words, e.g. it can be noted that the causally relevant documents in the example on Bin Laden’s assassination do not contain words such as ‘cause’, ‘reason’ etc. constituting Table 2.\nFrom Table 3, we can further observe that the proposed FCRLM method turns out to be significantly better than all baseline techniques in terms of various evaluation metrics. From the improvement in precision at rank 5 and reciprocal rank based metric, it can be concluded that FCRLM is competent in retrieving more relevant documents at top ranks. Further, the improvement in average precision and cumulative gain based measurements indicate the overall proficiency of FCRLM in terms of all retrieved documents. These observations confirm the hypothesis that terms that are relatively infrequent in the topical feedback model, but at the same time, having a relatively high likelihood of occurrence within top-retrieved documents during a second step feedback, estimate the best notion of causal relevance. The factored nature of FCRLM and the use of the odds-ratio between the two factors prove to be useful to retrieve documents that are not related to the query directly, but rather represent the potential set of causally related precursors.\nIn addition to the aggregate results, we also present the per-query results in Figure 2, which shows that the improvements obtained with FCRLM are mostly consistent across a number of topics.\nAnalysis. We now present a qualitative comparison between the retrieval effectiveness of RM3 (best performing baseline) and FCRLM by enlisting the top set (by highest weights) of expansion terms selected by each techniques in Table 4 for a topic from our dataset. The topic alludes to a semi-political scandal that resulted in the resignation of Shashi Tharoor, a member of Indian parliament. Lalit Modi, the then IPL chairman, tweeted that Shashi’s friend Sunanda\nPushkar (with whom Shashi was allegedly having an affair) received free equity by team Kochi. It is seen from the highlighted words of Table 4 that FCRLM was successful in estimating terms that are relevant to the chain of events, such as ‘equiti’, ‘affair’, ‘corrupt’ etc., which its counterpart, RM3, was unable to find."},{"heading":"5 CONCLUSIONS AND FUTUREWORK","text":"We proposed a different notion of ad-hoc search, where the information need is not directly related to the core topic of a query, but rather requires to find information that could have led to the query event. We constructed a collection for causality based IR research and proposed an unsupervised pseudo-relevance feedback algorithm that relies on a simple yet effective heuristic that causally relevant terms are not directly related to the core topic of the query. To leverage such terms, we rely on high term sampling probabilities from documents that are deeper down a retrieved list of topically relevant documents, and high co-occurrence likelihoods with the query terms to filter out potential noise.\nAs future work, we intend to explore ways of constructing causal chains of events in a recursive manner, i.e., instead of outputting a ranked list of documents we intend to extract events from the retrieved articles, treat them as queries in turn, and retrieve a list of further causes."}],"references":[{"title":"Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey","author":["Nabiha Asghar"],"venue":"CoRR abs/1605.07895","citeRegEx":"1","shortCiteRegEx":"1","year":2016},{"title":"Translating Embeddings for ModelingMulti-relational Data","author":["Antoine Bordes"],"venue":"In Proc. of NIPS’13","citeRegEx":"2","shortCiteRegEx":"2","year":2013},{"title":"Causal Relation Extraction Using Cue Phrase and Lexical Pair Probabilities","author":["Du-Seong Chang"],"venue":"In Proc. of First IJCNLP","citeRegEx":"3","shortCiteRegEx":"3","year":2005},{"title":"Causal Relation Extraction","author":["Eduardo Blanco"],"venue":"LREC","citeRegEx":"4","shortCiteRegEx":"4","year":2008},{"title":"Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features","author":["Hashimoto"],"venue":"In Proc. of ACL’14","citeRegEx":"5","shortCiteRegEx":"5","year":2014},{"title":"Overview of FIRE","author":["S. Palchowdhury"],"venue":"In Proc. of FIRE’11","citeRegEx":"6","shortCiteRegEx":"6","year":2011},{"title":"Constructing and Embedding Abstract Event Causality Networks from Text Snippets","author":["Se. Zhao"],"venue":"In Proc. of WSDM’17","citeRegEx":"7","shortCiteRegEx":"7","year":2017},{"title":"UMass at TREC 2004: Novelty and HARD","author":["N.A. Jaleel","J. Allan","W.B. Croft","F. Diaz","L.S. Larkey","X. Li","M.D. Smucker","C. Wade"],"venue":"In Proc. TREC ’04","citeRegEx":"8","shortCiteRegEx":"8","year":2004},{"title":"Causal Inference over Longitudinal Data to Support Expectation Exploration","author":["Emre Kiciman"],"venue":"In Proc. of SIGIR","citeRegEx":"9","shortCiteRegEx":"9","year":2018},{"title":"Answering What If, Should I, and Other Expectation Exploration Queries Using Causal Inference over Longitudinal Data","author":["Emre Kiciman","Jorgen Thelin"],"venue":"In Proc. of DESIRES’18","citeRegEx":"10","shortCiteRegEx":"10","year":2018},{"title":"Relevance Based Language Models","author":["Victor Lavrenko","W. Bruce Croft"],"venue":"In Proc. of SIGIR’01","citeRegEx":"11","shortCiteRegEx":"11","year":2001},{"title":"Learning Causality for News Events Prediction","author":["Kira Radinsky","Sagie Davidovich","Shaul Markovitch"],"venue":"In Proc. of WWW","citeRegEx":"12","shortCiteRegEx":"12","year":2012},{"title":"Overview of TREC-8","author":["Ellen Voorhees","Donna Harman"],"venue":"In Proc. of TREC-8","citeRegEx":"13","shortCiteRegEx":"13","year":2000}],"referenceMentions":[{"referenceID":0,"context":"A number of existing approaches extract cause-effect patterns using lexical, syntactic, and more recently, semantic relations [3–5, 12], primarily taken from headlines or single sentences (refer to [1] for a survey).","startOffset":198,"endOffset":201},{"referenceID":6,"context":"The cause-effect pattern approach was extended in [7], where a set of patterns are initially used to create a network of causes and effects, and then a relational embedding method (similar to TransE [2]) is used to jointly embed causes and effects.","startOffset":50,"endOffset":53},{"referenceID":1,"context":"The cause-effect pattern approach was extended in [7], where a set of patterns are initially used to create a network of causes and effects, and then a relational embedding method (similar to TransE [2]) is used to jointly embed causes and effects.","startOffset":199,"endOffset":202},{"referenceID":8,"context":", current situation or proposed action) and the results provide a list of possible effects was studied in [9, 10].","startOffset":106,"endOffset":113},{"referenceID":9,"context":", current situation or proposed action) and the results provide a list of possible effects was studied in [9, 10].","startOffset":106,"endOffset":113},{"referenceID":10,"context":"Specifically, Equation 2 reveals that in contrast to topical relevance model θT [11] estimation, we employ an odds-ratio between the probability of a term occurring within a document retrieved (with a more general query), and the probability of the same term estimated by θT (more specific effect event query).","startOffset":80,"endOffset":84},{"referenceID":7,"context":"Following the exposition of [8], the query model is linearly interpolated with the factored model in order to finalize the expansion term weights.","startOffset":28,"endOffset":31},{"referenceID":5,"context":"Documents from the FIRE ad-hoc track English [6] was used as the target collection.","startOffset":45,"endOffset":48},{"referenceID":12,"context":", topical relevance) IR is usually constructed by combining top-ranked documents retrieved by a number of systems (various IR models with different settings) [13].","startOffset":158,"endOffset":162},{"referenceID":7,"context":"Since our proposed approach is a factored feedback model, we employ the conventional relevance model based expansion technique RM3 [8] as our second baseline.","startOffset":131,"endOffset":134},{"referenceID":7,"context":"The common parameters in the methods investigated are a) the number of top-ranked documents,M , to consider as pseudo relevant, b) T , the number of top scored terms (P(w |R) values), used for computing the KL divergence for reranking within a standard RLM setup [8], and c) the mixture parameter, λ, used in query likelihood.","startOffset":263,"endOffset":266}],"year":2020,"abstractText":"Different to traditional IR, which retrieves a set of topically relevant documents given a user query, we investigate causal retrieval, which involves retrieving a set of documents that describe a set of potential causes leading to an effect specified in the query. We argue that the nature of causal relevance should be different to that of traditional topical relevance. This is because although the causally relevant documents would have partial term overlap with the ones that are topically relevant for a query, yet it is expected that a majority of these documents would use a different set of terms to describe a number of causes possibly leading to their effects. To address this, we propose a feedback model to estimate a distribution of terms which are relatively infrequent but associated with high weights in the topically relevant distribution, leading to potential causal relevance. Our experiments demonstrate that such a feedback model turns out to be substantially more effective than traditional IR models and a number of other causality heuristic baselines. ACM Reference Format: Suchana Datta, Debasis Ganguly, Dwaipayan Roy, Francesca Bonin, Charles Jochim, and Mandar Mitra. 2020. Retrieving Potential Causes from a Query Event . In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25– 30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3397271.3401207","creator":"LaTeX with acmart 2020/01/11 v1.67 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX"}}
{"name":"3397271.3401188.pdf","metadata":{"source":"META","title":"Choppy: Cut Transformer for Ranked List Truncation","authors":["Dara Bahri","Yi Tay","Che Zheng","Andrew Tomkins"],"emails":["dbahri@google.com","yitay@google.com","chezheng@google.com","metzler@google.com","tomkins@google.com","permissions@acm.org."],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems → Presentation of retrieval results; Retrieval effectiveness.\nKEYWORDS ranked list truncation; neural networks; Transformer; information retrieval; deep learning\nACM Reference Format: Dara Bahri, Yi Tay, Che Zheng, Donald Metzler, and Andrew Tomkins. 2020. Choppy: Cut Transformer for Ranked List Truncation. In 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401188"},{"heading":"1 INTRODUCTION","text":"While much of the work in information retrieval has been centered around ranking, there is growing interest in methods for ranked list truncation - the problem of determining the appropriate cutoff \uD835\uDC58 of candidate results [1, 9]. This problem has garnered attention\n∗Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401188\nin fields like legal search [14] and sponsored search [3, 16], where there could be a monetary cost for users looking into an irrelevant tail of documents or where showing too many irrelevant ads could result in ad blindness. The fundamental importance of this problem has led to development of methods that are automatically able to learn \uD835\uDC58 in a data-driven fashion [9]. The focus of this paper is to design more effective models for accurate and dynamic truncation of ranked lists.\nThe present state-of-the-art for this task is BiCut [9], a recurrentbased neural model that formulates the problem as a sequential decision process over the list. BiCut trains a bidirectional LSTM [8] model with a predefined loss that serves as a proxy to the userdefined target evaluation metric. At every position in the ranked list, BiCut makes a binary decision conditioned on both forward and backward context: to continue to the next position, or to end the output list.\nWhile BiCut outperforms non-neural methods [1], we argue it has several drawbacks. Firstly, the model is trained with teacherforcing, i.e. with ground truth context, but it is deployed autoregressively at test time, where it is conditioned on its own predictions. Thus, the model suffers from a train / test distribution mismatch, often referred to as exposure bias [12], resulting in poor model generalization. Secondly, the loss function used does not capture the mutual exclusivity among the candidate cut positions. In other words, the loss does not capture the condition that the list can only be cut in at most one position. Furthermore, the proposed training loss is unaligned with the user-defined evaluation metric. Last but not least, BiCut employs BiLSTMs which are not only slow and non-parallelizable, but also do not take into account global long-range dependencies.\nThis paper proposes Choppy, a newmethod that not only ameliorates the limitations of the BiCut model but also achieves state-ofthe-art performance on the ranked list truncation task. Our method comprises two core technical contributions. The first is a Transformer model [15] that is able to capture long-range dyadic interactions between relevance scores. To the best of our knowledge, this is the first successful application of self-attentive models on scalar ranking scores. The second technical contribution is the development of a loss function that optimizes the expected metric value over all candidate cut positions. Overall, Choppy not only improves the predictive performance on this task but also improves the model inference speed by > 3 times.\nOur contributions. The key contributions of the paper are summarized as follows:\n• We frame the ranked list truncation task as modeling the joint distribution among all candidate cut positions, and we\nconstruct our training loss to be the expected metric value over cut positions, for any choice of user-definedmetric, such as F1, precision, or discounted cumulative gain. As such, our training loss and evaluation metric are fully aligned. • We propose Choppy, a Cut Transformer model that achieves a state-of-the-art 11.5% relative improvement over the BiCut model. To predict the joint distribution over candidate cut positions, our method learns a positional embedding and leverages expressive bidirectional multi-headed attention."},{"heading":"2 RELATEDWORK","text":"Across the rich history of information retrieval research, there has been extensive work focused on modeling score distributions of IR systems. Early work in this area primarily focused on fitting parametric probability distributions to score distributions [1, 10]. This is often achieved by making the assumption that the overall distribution can be expressed as a mixture of a relevant and a non-relevant distribution. The expectation-maximization (EM) algorithm is often adopted to learn the parameters.\nThere has been considerable recent interest in adopting machine learning based models to optimize and improve the ranked list truncation problem. For instance, cascade-style IR systems [17] seek to achieve a balance between efficiency and effectiveness. Notably, [5] investigates a number of machine learning approaches for learning dynamic cutoffs within cascade-style ranking systems. Another recent study investigated how to leverage bidirectional Long Short-Term Memory (LSTM) models to identify the best position to truncate a given list [9]. This model, BiCut, can be considered the present state-of-the-art approach.\nOur work is closely related to the task of query performance prediction [4]. In this task, the objective is to automatically determine the effectiveness of a given query. This could be leveraged\nto determine the optimal set of results to the user for any given measure. Methods for query performance prediction include preretrieval-based approaches [7], relevance-based approaches [4, 19], and neural approaches [18].\nA system that determines the best number of results to display to users has the potential to benefit a wide number of applications. For example, in sponsored search, displaying too many irrelevant ads to users may cause frustration, resulting in so-called query blindness. This motivated research that investigated whether any ads should be displayed at all [3]. It is also easy to see that a similar and related problem formulation is to determine how many ads should be displayed to the users [16]. Moreover, determining the optimal number of ranked results is also important in a number of other IR applications such as legal e-discovery [14], where there is an significant financial or labor cost associated with reviewing results. Finally, the ability to calibrate scores across queries and different corpora has also been studied in the context of federated search tasks [13] such as meta-search [11]."},{"heading":"3 CHOPPY","text":"We now describe Choppy, our proposed Cut Transformer approach for ranked list truncation.\nLet (r1, . . . , r\uD835\uDC5B) denote the sequence of results, ranked in decreasing order of relevance, and let r\uD835\uDC56 have relevance score s\uD835\uDC56 and ground truth relevance label y\uD835\uDC56 (1 if relevant, −1 if non-relevant). We now describe our model piecemeal."},{"heading":"3.1 Transformer Layer","text":"We briefly review the Transformer layer. In our work, we let all model dimensions be \uD835\uDC51 . Let \uD835\uDC4B ∈ R\uD835\uDC5B×\uD835\uDC51 represent the input to the layer and\uD835\uDC4A\uD835\uDC5E,\uD835\uDC4A\uD835\uDC58 ,\uD835\uDC4A\uD835\uDC63 ∈ R\uD835\uDC51×\uD835\uDC51 . We define\nAttn ( \uD835\uDC4B ;\uD835\uDC4A\uD835\uDC5E,\uD835\uDC4A\uD835\uDC58 ,\uD835\uDC4A\uD835\uDC63 ) = Act ( \uD835\uDC4B\uD835\uDC4A\uD835\uDC5E\uD835\uDC4A \uD835\uDC47 \uD835\uDC58 \uD835\uDC4B\uD835\uDC47 ) (\uD835\uDC4B\uD835\uDC4A\uD835\uDC63)\nwhere Act is an activation function. As done in [15], we take it to perform row-wise softmax and normalize by √ \uD835\uDC51 . Attention is often augmented using multiple heads. In multi-headed attention, the model dimension is split into multiple heads, each one performing attention independently, and the result is concatenated together. Let ℎ be the number of heads and suppose \uD835\uDC51 is divisible by ℎ. Then,\nMultiAttn ( \uD835\uDC4B ;\uD835\uDC4A\uD835\uDC5E,\uD835\uDC4A\uD835\uDC58 ,\uD835\uDC4A\uD835\uDC63 ) = rConcat\n1≤\uD835\uDC56≤ℎ\n[ Attn ( \uD835\uDC4B ;\uD835\uDC4A (\uD835\uDC56)\uD835\uDC5E ,\uD835\uDC4A (\uD835\uDC56) \uD835\uDC58 ,\uD835\uDC4A (\uD835\uDC56) \uD835\uDC63 )] where\uD835\uDC4A (\uD835\uDC56)\uD835\uDC5E ,\uD835\uDC4A (\uD835\uDC56) \uD835\uDC58 ,\uD835\uDC4A (\uD835\uDC56) \uD835\uDC63 ∈ R\uD835\uDC51×(\uD835\uDC51/ℎ) . rConcat performs row-wise concatenation. The output of multi-headed attention is\n\uD835\uDC34 = LayerNorm ( \uD835\uDC4B +MultiAttn ( \uD835\uDC4B ;\uD835\uDC4A\uD835\uDC5E,\uD835\uDC4A\uD835\uDC58 ,\uD835\uDC4A\uD835\uDC63 ) ) where LayerNorm is layer normalization [2]. Finally, the output of the Transformer layer is\n\uD835\uDC42trans = LayerNorm (\uD835\uDC34 + rFF (\uD835\uDC34)) where rFF applies a single learnable feed-forward layer with ReLU activation to each row of \uD835\uDC34."},{"heading":"3.2 Positional Embedding","text":"The vanilla Transformer incorporates positional encoding by adding fixed sinusoidal values to the input token embeddings. As the token embeddings are trained, they have the flexibility to learn how to\nbest utilize the fixed positional information. In our setting however, the inputs are fixed 1-dimensional relevance scores. Attempting to apply a Transformer layer directly on the raw scores can limit its complexity. To that end, we introduce a learnable positional embedding \uD835\uDC43 ∈ R\uD835\uDC5B×(\uD835\uDC51−1) and feed in \uD835\uDC4B = rConcat [s, \uD835\uDC43] to the the first Transformer layer only, where s is the column vector of relevance scores."},{"heading":"3.3 Loss","text":"So far, Choppy takes the \uD835\uDC5B-length vector of scores, augments it with a positional embedding, and feeds the result into \uD835\uDC5Blayers Transformer layers. This produces\uD835\uDC42trans ∈ R\uD835\uDC5B×\uD835\uDC51 . We arrive at the output of Choppy by applying a final linear projection followed by a softmax over positions:\no = Softmax (\uD835\uDC42trans\uD835\uDC4A\uD835\uDC5C )\nwhere\uD835\uDC4A\uD835\uDC5C ∈ R\uD835\uDC51×1. We interpret the output o to be a probability distribution over candidate cutoff positions. More concretely, we take o\uD835\uDC56 = Prob [(r1, . . . , r\uD835\uDC56 )]. Let \uD835\uDC36 be any user-defined evaluation metric that should be maximized, such as F1 or precision. For each training example \uD835\uDC57 and every candidate cutoff position \uD835\uDC56 we compute \uD835\uDC36\uD835\uDC56 (y( \uD835\uDC57) ), the value of themetric if the result list were to be truncated at position \uD835\uDC56 , using the ground-truth relevance labels. Our proposed loss follows as:\n\uD835\uDC3F ( s( \uD835\uDC57) , y( \uD835\uDC57) ) = − \uD835\uDC5B∑ \uD835\uDC56=1 o\uD835\uDC56 ( s( \uD835\uDC57) ) \uD835\uDC36\uD835\uDC56 ( y( \uD835\uDC57) ) = −E\uD835\uDC4D∼Categorical(o(s( \uD835\uDC57 ) )) \uD835\uDC36\uD835\uDC4D ( y( \uD835\uDC57) ) .\nWith this loss, our model learns the conditional joint distribution over candidate cut positions that maximizes the expected evaluation metric on the training samples. We depict the loss and the predicted distribution for a few training samples in Figure 1. We see that the model tends to weight positions according to their corresponding metric value. At test time we choose to cut at the argmax position. Note that unlike BiCut, our loss has no tune-able hyperparameters."},{"heading":"4 EXPERIMENTS","text":"This section describes our experimental setup and results."},{"heading":"4.1 Dataset","text":"We evaluate our method using the TREC collection Robust04, used in the TREC 2004 Robust Track. It consists of 250 queries over 528k news articles, where each query has 1000 total results and an average of 70 relevant ones. This is the same dataset used in [9]. We use a random 80/20 train/test that achieves comparable performance to the reported results in [9]. We evaluate the efficacy of our truncation model using two different retrieval approaches - BM25, a traditional tf-idf based model, and DRMM [6], a neural model."},{"heading":"4.2 Baselines","text":"We evaluate our method against the following baselines:\n• Fixed-\uD835\uDC58 returns the top-\uD835\uDC58 results for a single value of \uD835\uDC58 across test queries.\n• Greedy-\uD835\uDC58 chooses the single \uD835\uDC58 that maximizes \uD835\uDC36 over the training set. • Oracle uses knowledge of each test query’s true label to optimize \uD835\uDC58 . It represents an upper-bound on the metric performance that can be achieved. • BiCut [9] learns a multi-layer bidirectional LSTM model on the entire training set, taking the score sequence as inputs. At position \uD835\uDC56 of the result list, the model predicts probability \uD835\uDC5D\uD835\uDC56 to continue and probability 1 − \uD835\uDC5D\uD835\uDC56 to end. At inference time, the cutoff is made before the first occurrence of end."},{"heading":"4.3 Setting","text":"We report F1 and Discounted Cumulative Gain (DCG) scores where we define DCG to penalize negative, or non-relevant results:\nDCG\uD835\uDC5B = \uD835\uDC5B∑ \uD835\uDC56=1\n\uD835\uDC66\uD835\uDC56\nlog2 (\uD835\uDC56 + 1) ,\nWe need to deviate from the usual definition of DCG since the usual definition always increases monotonically with the length of the returned ranked list and so the optimal solution under this definition would be to not truncate at all. For methods that optimize F1 or DCG, we report the performance of the model when it is optimized specifically for that metric. Note that DCG is unsupported by BiCut.\nFor Choppy, we blithely set \uD835\uDC5Blayers = 3, ℎ (# heads) = 8, and \uD835\uDC51 = 128 across all settings, without any tuning. We optimize the aforementioned custom loss function using Adam with default learning rate 0.001, and a batch size of 64. As in [9], we only consider the top-300 candidate results of each query."},{"heading":"5 RESULTS AND DISCUSSION","text":""},{"heading":"5.1 Results","text":"As shown in Table 1, Choppy achieves a significant improvement over BiCut for both metrics and both retrieval types. This improvement arises from Choppy’s ability to model the joint distribution over all candidate cut positions and its direct optimization of the evaluation metric. Furthermore, the attention mechanism is able to effectively capture correlations between scores far apart in ranked order. This is in contrast to LSTMs, as used in BiCut, whose degradation with larger sequence length is well known."},{"heading":"5.2 Ablation Study","text":"Choppy has three hyperparameters: the model dimension \uD835\uDC51 , the number of heads ℎ, and the number of Transformer layers \uD835\uDC5Blayers.\nIn Figure 2 we plot the impact of \uD835\uDC51 and ℎ on predictive performance (while keeping \uD835\uDC5Blayers fixed to 3). We see that both F1 and DCG are strong and stable across settings. Extrapolating beyond Robust04, we expect Choppy to work out-of-the-box on many datasets. Unlike BiCut, which required much tuning in our experience, Choppy seems to work well while requiring little-to-no tuning."},{"heading":"6 CONCLUSION","text":"We propose Choppy, a Transformer architecture for ranked list truncation, that learns the score distribution of relevant and nonrelevant documents and is able to directly optimize any user-defined metric. We show that Choppy achieves state-of-the-art F1 and DCG performance on Robust04, under both a traditional tf-idf as well as modern neural ranking system. We then dig deeper into Choppy’s architecture settings, showing strong and stable performance across a range of values. We thus conclude that when faced with a ranked list truncation task, one can apply Choppy and expect competitive performance."},{"heading":"ACKNOWLEDGMENTS","text":"We would like to thank the authors of BiCut for providing us with the BM25 and DRMM ranked lists that were used for evaluation."}],"references":[{"title":"Where to Stop Reading a Ranked List?: Threshold Optimization Using Truncated Score Distributions","author":["Avi Arampatzis","Jaap Kamps","Stephen Robertson"],"venue":"In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval","citeRegEx":"1","shortCiteRegEx":"1","year":2009},{"title":"To Swing or Not to Swing: Learning when (Not) to Advertise","author":["Andrei Broder","Massimiliano Ciaramita","Marcus Fontoura","Evgeniy Gabrilovich","Vanja Josifovski","Donald Metzler","Vanessa Murdock","Vassilis Plachouras"],"venue":"In Proceedings of the 17th ACM Conference on Information and Knowledge Management (Napa Valley, California,","citeRegEx":"3","shortCiteRegEx":"3","year":2008},{"title":"Predicting Query Performance","author":["Steve Cronen-Townsend","Yun Zhou","W. Bruce Croft"],"venue":"In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Tampere,","citeRegEx":"4","shortCiteRegEx":"4","year":2002},{"title":"Dynamic Cutoff Prediction inMulti-Stage Retrieval Systems. In Proceedings of the 21st Australasian Document Computing Symposium (Caulfield, VIC, Australia) (ADCS ’16)","author":["J. Shane Culpepper","Charles L.A. Clarke","Jimmy Lin"],"venue":null,"citeRegEx":"5","shortCiteRegEx":"5","year":2016},{"title":"A deep relevance matching model for ad-hoc retrieval","author":["Jiafeng Guo","Yixing Fan","Qingyao Ai","W Bruce Croft"],"venue":"In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management","citeRegEx":"6","shortCiteRegEx":"6","year":2016},{"title":"A Survey of Pre-retrieval Query Performance Predictors","author":["Claudia Hauff","Djoerd Hiemstra","Franciska de Jong"],"venue":"In Proceedings of the 17th ACM Conference on Information and Knowledge Management (Napa Valley, California,","citeRegEx":"7","shortCiteRegEx":"7","year":2008},{"title":"Long short-termmemory","author":["Sepp Hochreiter","Jürgen Schmidhuber"],"venue":"Neural computation 9,","citeRegEx":"8","shortCiteRegEx":"8","year":1997},{"title":"An Assumption- Free Approach to the Dynamic Truncation of Ranked Lists","author":["Yen-Chieh Lien","Daniel Cohen","W. Bruce Croft"],"venue":"In Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (Santa Clara, CA, USA) (ICTIR ’19)","citeRegEx":"9","shortCiteRegEx":"9","year":2019},{"title":"Modeling Score Distributions for Combining the Outputs of Search Engines","author":["R. Manmatha","T. Rath","F. Feng"],"venue":"In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,","citeRegEx":"10","shortCiteRegEx":"10","year":2001},{"title":"Relevance Score Normalization for Metasearch","author":["Mark Montague","Javed A. Aslam"],"venue":"In Proceedings of the Tenth International Conference on Information and Knowledge Management (Atlanta, Georgia,","citeRegEx":"11","shortCiteRegEx":"11","year":2001},{"title":"Sequence level training with recurrent neural networks","author":["Marc’Aurelio Ranzato","Sumit Chopra","Michael Auli","Wojciech Zaremba"],"venue":"arXiv preprint arXiv:1511.06732","citeRegEx":"12","shortCiteRegEx":"12","year":2015},{"title":"Attention is all you need","author":["Ashish Vaswani","Noam Shazeer","Niki Parmar","Jakob Uszkoreit","Llion Jones","Aidan N Gomez","Łukasz Kaiser","Illia Polosukhin"],"venue":"In Advances in neural information processing systems","citeRegEx":"15","shortCiteRegEx":"15","year":2017},{"title":"Learning to Advertise: How Many Ads Are Enough","author":["Bo Wang","Zhaonan Li","Jie Tang","Kuo Zhang","Songcan Chen","Liyun Ru"],"venue":null,"citeRegEx":"16","shortCiteRegEx":"16","year":2011},{"title":"A Cascade Ranking Model for Efficient Ranked Retrieval. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Beijing, China) (SIGIR ’11)","author":["Lidan Wang","Jimmy Lin","Donald Metzler"],"venue":null,"citeRegEx":"17","shortCiteRegEx":"17","year":2011},{"title":"Neural Query Performance Prediction Using Weak Supervision from Multiple Signals","author":["Hamed Zamani","W. Bruce Croft","J. Shane Culpepper"],"venue":"In The 41st International ACM SIGIR Conference on Research","citeRegEx":"18","shortCiteRegEx":"18","year":2018},{"title":"Query Performance Prediction in Web Search Environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Amsterdam, The Netherlands) (SIGIR ’07)","author":["Yun Zhou","W. Bruce Croft"],"venue":"Short Research Papers I SIGIR","citeRegEx":"19","shortCiteRegEx":"19","year":2007}],"referenceMentions":[{"referenceID":0,"context":"While much of the work in information retrieval has been centered around ranking, there is growing interest in methods for ranked list truncation - the problem of determining the appropriate cutoff k of candidate results [1, 9].","startOffset":221,"endOffset":227},{"referenceID":7,"context":"While much of the work in information retrieval has been centered around ranking, there is growing interest in methods for ranked list truncation - the problem of determining the appropriate cutoff k of candidate results [1, 9].","startOffset":221,"endOffset":227},{"referenceID":1,"context":"3401188 in fields like legal search [14] and sponsored search [3, 16], where there could be a monetary cost for users looking into an irrelevant tail of documents or where showing too many irrelevant ads could result in ad blindness.","startOffset":62,"endOffset":69},{"referenceID":12,"context":"3401188 in fields like legal search [14] and sponsored search [3, 16], where there could be a monetary cost for users looking into an irrelevant tail of documents or where showing too many irrelevant ads could result in ad blindness.","startOffset":62,"endOffset":69},{"referenceID":7,"context":"The fundamental importance of this problem has led to development of methods that are automatically able to learn k in a data-driven fashion [9].","startOffset":141,"endOffset":144},{"referenceID":7,"context":"The present state-of-the-art for this task is BiCut [9], a recurrentbased neural model that formulates the problem as a sequential decision process over the list.","startOffset":52,"endOffset":55},{"referenceID":6,"context":"BiCut trains a bidirectional LSTM [8] model with a predefined loss that serves as a proxy to the userdefined target evaluation metric.","startOffset":34,"endOffset":37},{"referenceID":0,"context":"While BiCut outperforms non-neural methods [1], we argue it has several drawbacks.","startOffset":43,"endOffset":46},{"referenceID":10,"context":"Thus, the model suffers from a train / test distribution mismatch, often referred to as exposure bias [12], resulting in poor model generalization.","startOffset":102,"endOffset":106},{"referenceID":11,"context":"The first is a Transformer model [15] that is able to capture long-range dyadic interactions between relevance scores.","startOffset":33,"endOffset":37},{"referenceID":0,"context":"Early work in this area primarily focused on fitting parametric probability distributions to score distributions [1, 10].","startOffset":113,"endOffset":120},{"referenceID":8,"context":"Early work in this area primarily focused on fitting parametric probability distributions to score distributions [1, 10].","startOffset":113,"endOffset":120},{"referenceID":13,"context":"For instance, cascade-style IR systems [17] seek to achieve a balance between efficiency and effectiveness.","startOffset":39,"endOffset":43},{"referenceID":3,"context":"Notably, [5] investigates a number of machine learning approaches for learning dynamic cutoffs within cascade-style ranking systems.","startOffset":9,"endOffset":12},{"referenceID":7,"context":"Another recent study investigated how to leverage bidirectional Long Short-Term Memory (LSTM) models to identify the best position to truncate a given list [9].","startOffset":156,"endOffset":159},{"referenceID":2,"context":"Our work is closely related to the task of query performance prediction [4].","startOffset":72,"endOffset":75},{"referenceID":5,"context":"Methods for query performance prediction include preretrieval-based approaches [7], relevance-based approaches [4, 19], and neural approaches [18].","startOffset":79,"endOffset":82},{"referenceID":2,"context":"Methods for query performance prediction include preretrieval-based approaches [7], relevance-based approaches [4, 19], and neural approaches [18].","startOffset":111,"endOffset":118},{"referenceID":15,"context":"Methods for query performance prediction include preretrieval-based approaches [7], relevance-based approaches [4, 19], and neural approaches [18].","startOffset":111,"endOffset":118},{"referenceID":14,"context":"Methods for query performance prediction include preretrieval-based approaches [7], relevance-based approaches [4, 19], and neural approaches [18].","startOffset":142,"endOffset":146},{"referenceID":1,"context":"This motivated research that investigated whether any ads should be displayed at all [3].","startOffset":85,"endOffset":88},{"referenceID":12,"context":"It is also easy to see that a similar and related problem formulation is to determine how many ads should be displayed to the users [16].","startOffset":132,"endOffset":136},{"referenceID":9,"context":"Finally, the ability to calibrate scores across queries and different corpora has also been studied in the context of federated search tasks [13] such as meta-search [11].","startOffset":166,"endOffset":170},{"referenceID":7,"context":"We use a random 80/20 train/test that achieves comparable performance to the reported results in [9].","startOffset":97,"endOffset":100},{"referenceID":4,"context":"We evaluate the efficacy of our truncation model using two different retrieval approaches - BM25, a traditional tf-idf based model, and DRMM [6], a neural model.","startOffset":141,"endOffset":144},{"referenceID":7,"context":"• BiCut [9] learns a multi-layer bidirectional LSTM model on the entire training set, taking the score sequence as inputs.","startOffset":8,"endOffset":11},{"referenceID":7,"context":"As in [9], we only consider the top-300 candidate results of each query.","startOffset":6,"endOffset":9}],"year":2020,"abstractText":"Work in information retrieval has traditionally focused on ranking and relevance: given a query, return some number of results ordered by relevance to the user. However, the problem of determining how many results to return, i.e. how to optimally truncate the ranked result list, has received less attention despite being of critical importance in a range of applications. Such truncation is a balancing act between the overall relevance, or usefulness of the results, with the user cost of processing more results. In this work, we propose Choppy, an assumption-free model based on the widely successful Transformer architecture, to the ranked list truncation problem. Needing nothing more than the relevance scores of the results, the model uses a powerful multi-head attention mechanism to directly optimize any user-defined IR metric. We show Choppy improves upon recent state-of-the-art methods.","creator":"LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2020/01/14 v7.00d Hypertext links for LaTeX"}}
{"name":"3397271.3401103.pdf","metadata":{"source":"META","title":"An Eye Tracking Study of Web Search by People With and Without Dyslexia","authors":["Srishti Palani","Adam Fourney","Shane Williams","Kevin Larson","Irina Spiridonova","Meredith Ringel Morris","Irina Spiri"],"emails":["srpalani@ucsd.edu","adamfo@microsoft.com","shanewil@microsoft.com","kevlar@microsoft.com","irinasp@microsoft.com","merrie@microsoft.com"],"sections":[{"heading":null,"text":"CCS CONCEPTS · Information systems → Search interfaces; · Human-centered computing → Empirical studies in accessibility.\nKEYWORDS Web search; Information Retrieval; Dyslexia; Eye tracking\nACM Reference Format: Srishti Palani, Adam Fourney, Shane Williams, Kevin Larson, Irina Spiridonova, and Meredith Ringel Morris. 2020. An Eye Tracking Study of Web Search by People With and Without Dyslexia. In 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25ś30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401103"},{"heading":"1 INTRODUCTION","text":"Dyslexia is a cognitive diference that impacts about 15% of English speakers [7] (incidence rates vary by language [30]). People with dyslexia tend to experience challenges in tasks involving reading, writing, spelling, and memory, despite having normal intelligence. These challenges often manifest in a slower reading rate and lower reading comprehension [35]. However, because dyslexia is a spectrum disorder, diferent people may experience diferent subsets and degrees of symptoms [7].\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25ś30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401103\nRelatively little is known about how dyslexia impacts web search, but several recent studies have begun to shed some light on this topic. Interviews with people with dyslexia suggest that search engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43]. Online experiments comparing search behaviors of people with and without dyslexia have identifed some features of webpages, such as average line length and the ratio of images to text, that impact page readability for people with dyslexia [25].\nIn this paper, we build on this knowledge by conducting an eye tracking study to investigate the following research questions: RQ1: Are there any diferences between gaze patterns of searchers with and without dyslexia? RQ2: What does the gaze data reveal about the challenges experienced and strategies employed by these two searcher groups?\nTo address these research questions, we conducted an eye tracking study with 27 participants (14 with dyslexia, 13 control). Each participant completed 6 informational search tasks using a modern, English-language, interactive search engine. Our analysis of eye tracking data, together with participants’ self-reports about their experiences, extend fndings from prior studies and contribute new insight into the diferences in search behavior between people with and without dyslexia. Our fndings validate prior self-report fndings that Searchers with Dyslexia (SWD) struggle with all stages of the search process: query formulation, search results triage, and information extraction [37, 43]. Adding to prior work, we fnd SWD visually attend to pages in a markedly diferent fxation pattern\nthan searchers without dyslexia (for e.g. see Figure 1). We conclude the paper by discussing the design implications of our fndings to improve cognitive accessibility of search for people with dyslexia."},{"heading":"2 RELATED WORK","text":"This research builds on prior work done to capture and understand search behavior, and on studies of searchers with dyslexia."},{"heading":"2.1 Methods for Understanding Web Search","text":"Researchers in information retrieval and HCI have extensively studied user interaction with web search systems, particularly for complex informational [16] or exploratory [55] search tasks. Such tasks typically comprise three stages [3, 43]: query formulation (i.e., generating and refning search keywords), search results triage (i.e., determining which parts of the search engine results page ś the SERP ś are most relevant to the task at hand, and which link to open), and information extraction (i.e., gathering and making sense of the sought-after content). In this study, we gather data about this complete query-triage-extraction search pipeline for searchers with and without dyslexia.\nResearchers have employed a variety of methods to study web search, including analyzing search engine and web browser logs (e.g., [32, 53, 54]), gathering self-report data from surveys, interviews, or diary studies of end-users (e.g., [42, 43]), and recruiting participants to perform controlled search tasks (e.g., [3, 25, 44]). As with any methodology, there are trade-ofs: logs can provide in-situ data for a large set of users, but lack qualitative depth; self-report data may have gaps or inconsistencies with actual observed behavior; controlled, in-lab task performance may difer from natural search behavior in unanticipated ways, etc. Several researchers have begun to use eye tracking to understand which aspects of the SERP users attend to [22, 23, 36, 52]. Eye tracking allows us to log and track the amount of attention paid to specifc parts of the pages and interactions at a granular level of space and time. For example, eye tracking studies have revealed that searchers usually fxate on SERPs and webpages in an F-shape pattern [45, 46]. These studies have also shown that searchers distribute their visual attention diferently across organic and ad results on a SERP [24], and that one can determine a webpage’s most salient parts by looking at the amount of visual attention paid to its diferent elements [17]. This paper builds on prior eye-tracking studies to understand how diferent searchers attend to diferent elements of SERPs and webpages. We complement our eye tracking data with participants’ comments immediately after searching to gain qualitative insight into the meaning of the eye tracking results."},{"heading":"2.2 Dyslexia and Web Search","text":"In this section, we discuss prior work on challenges faced by searchers with dyslexia (SWD) at each stage of the search process.\n2.2.1 Qery Formulation: Since spelling and query formulation are closely related, this stage of search has been reported to be challenging for those with dyslexia. From their interview study, Morris et al. [43] reported SWD have trouble spelling words at the phonetic level. They also found SWD report a heavy reliance on voice input and on autocomplete when forming queries. In 2015, Berget and Sandness [13] compared search logs of 21 students with\nand 21 without dyslexia who had formulated Norwegian queries for a Norwegian academic library system with no query-formulation aids. They found that SWD took longer to search, possibly because of spelling errors in their queries. Consequently, they also issued more queries overall. In their 2016 qualitative study, Cole et al. [19] confrmed these fndings for English queries in a system without any query formulation aids. They reported that SWD found choosing keywords, spelling, and forming/refning complex queries using Boolean search strategies of AND and OR operators to be more challenging than a control group. However, in 2016, Berget and Sandness [14] found that when SWD used Google, a modern interactive search engine with query formulation aids, for formulating Norwegian queries, there were no diferences in query formulation behavior. This suggests query formulation aids could help those with dyslexia. Through our eye-tracking study, we validate these self-report results for a modern interactive English-language search engine with query-formulation aids.\n2.2.2 Search Results Triage: In 2010, MacFarlane et al. [37] conducted a search log analysis of people with dyslexia interacting with an information retrieval interface (Okapi), and found SWD read fewer documents on average, had fewer search interactions, and took more time to complete searching. Concluding their article, they state the need for a more granular approach to understand search behavior diferences between searchers with and without dyslexia than search log analysis.\nFurthermore, prior work suggests a strong correlation between dyslexia and lower phonological working memory (i.e., the ability to hold words in short-term memory) [10, 38]. Work done to investigate the efects of working memory on search results triage echoes MacFarlane et al.’s fndings. In 2019, a search log study of participants with low and high working memory found that those with lower working memory take more time to frst click on the SERP, open fewer links, and take more time between events [18].\n2.2.3 Information Extraction: In 2007, Al-Wabil et al.’s [4] interview study observing searchers with dyslexia navigate multiple pages within a website found navigational trails (such as site maps, back and forward buttons) and menus helped SWD locate where they were within a site. Similarly, in 2008, an eye tracking study by Al-Wabil et al. [5], observed 7 participants (2 with dyslexia and 5 controls) navigate webpages within 6 websites to extract information. They found SWD took longer to complete the tasks, had more fxations on the page, looked at the page for longer, and their scan paths were markedly diferent than the control group. A follow-up analysis reported SWD changed scan direction more on the webpage than the control group [39]. They suggest this is an indication of short term memory problems and adds to prior research on backtracking [4]. However, the small number of participants (2 SWD) makes it hard to extrapolate defnitive quantitative diferences from these results.\nIn 2018 Fourney et al. [25] found SWD’s relevance ratings of webpages were highly correlated with their readability scores. They identifed several visual and textual features such as line length, number of headings, and ratio of images to text, that impact SWDs’ readability and relevance judgements of webpages. In 2019, Li et al. [35] found using the \"Reader View\" mode in Firefox web browser,\nwhich simplifes a page’s visual structure, improved reading speed for SWD without reducing comprehension.\nOur study builds on these prior eforts by capturing eye gaze patterns of 27 participants (14 SWD, 13 control) using a modern, English-language, interactive search engine during the complete query-triage-extraction search pipeline."},{"heading":"3 METHOD","text":"Our primary research question asks if there are diferences between gaze patterns and search behavior of searchers with and without dyslexia. To answer this question we designed a study that collects search log, eye tracking, and self-report data through all three stages of search: formulating queries, triaging search results, and extracting information from the resultant pages."},{"heading":"3.1 Participants","text":"We recruited participants using paid social media ads targeted towards residents of our (anonmyized, US-based) metropolitan area, who followed the #dyslexia hashtag or any of a set of organizations related to dyslexia, including a local special-education secondary school. As gratuity for participating in the hour-long study, participants received a $50 Amazon.com gift card. The study took place over a three-week period in the summer of 2019.\nWe recruited 32 participants, half of whom had a medical diagnosis of dyslexia. However, eye tracking data could not be reliably collected for 5 participants: in 4 cases this was due to a malfunction of the instrumentation, and in 1 case tracking failed because the participant had an eye condition (amblyopia). These 5 participants are removed from further consideration, leaving 27 total participants: 14 searchers with dyslexia (SWD), and 13 searchers in the control group.\nParticipants’ ages ranged from 13-72 years (x =27.3, ˙ =17.3). 12 participants were between the ages 13-17 (and received parental permission to participate), and 15 participants were 18 or older. Participants with and without dyslexia were well-balanced across age groups with 7 teenagers with dyslexia and 6 without, and 7 adults with dyslexia and 7 without. One adult reported having attention defcit hyperactivity disorder (ADHD) in addition to dyslexia. ADHD, a neurological diference that is characterized by difculty focusing, is a common comorbidity that occurs with dyslexia [26]. Of the 27 participants, 20 reported using web search engines multiple times on the average day, 5 reported using search at least once per day, and 2 reported searching multiple times per week. When asked to rate their search expertise, 8 participants self-rated as expert searchers, 17 as intermediate, and 2 as novice. Adult participants had a diverse set of occupations, including: program managers, designers, engineers, analysts, a school superintendent, and a human-resources director. Teen participants were all enrolled in secondary school.\n9 participants were male and 18 were female. In the general population, the gender ratio for dyslexia is typically near parity, or skewed slightly towards males [29]. Within our sample, 5 participants with dyslexia identifed as male, while 9 identifed as female. In the control group, 4 participants identifed as male and 9 identifed as female. This gender skew likely refects a limitation of our recruitment methods ś women are more likely to use social media, follow mailing lists [27], and be special education teachers [2], so\nmay have been more likely to see our ads. Lastly, all participants were fuent or native English speakers."},{"heading":"3.2 Apparatus","text":"We conducted the study on a 17ž LCD monitor, positioned at desklevel approximately 28ž from the participant’s eyes. This distance and head position were weakly enforced by seating participants in a four-legged chair, and then marking the location of each chair leg on the foor with tape. We used the monitor’s native resolution of 1920 × 1200 pixels, and accepted the operating system’s default display scaling of 150%. With this display setting, three or four search results were visible on the screen before scrolling. Participants could scroll pages freely, but could not adjust the scaling or browser zoom factors. Together, these controls ensured a consistent visual experience for all participants.\nWe used the Tobii 4c eye tracker [http://www.tobii.se/] upgraded with the Tobii Pro SDK to record each participant’s gaze patterns as they interacted with the search engine and other webpages. The eye tracker sampled the position of the participant’s gaze at a rate of 90Hz. From this data stream we identifed fxations, which we defne as a sequence of gaze points where: (1) the sequence accounts for at least 100 milliseconds of time, and (2) all gaze points fall within 15 pixels of a common centroid. A custom Google Chrome browser extension then mapped each centroid, in real-time, to an HTML element on the webpage that the participant was visiting. We logged both information about the fxation (location, duration) and information about the HTML element (e.g., the font size, element type, etc.). Finally, the browser was further instrumented to record page navigation events and search queries. To ensure a degree of ecological validity, all queries were issued to a major commercial web search engine (anonymized)."},{"heading":"3.3 Procedure","text":"As part of the recruitment process, participants answered a brief screening and demographics questionnaire that collected information about age, gender, occupation, language fuency, education, dyslexia diagnosis, dyslexia-related training/tutoring received, and search engine use (search frequency, preferred sites, and any additional devices or apps used to support searching). As noted above, we excluded participants who were not fuent in English, who were under the age of 13, and who were not located in our general metropolitan area. These exclusion criteria, together with the entire study procedure, were approved by our organization’s IRB.\nWhen each participant arrived at the research lab, the experimenter reviewed the study procedure with the participant, then calibrated the eye tracker to the participant using a 20-point calibration process. The study then proceeded as follows: Participants were asked to complete 6 search tasks, with a maximum of 10 minutes to complete each task, for a total study time of approximately one hour (see Table 1). For each task, the experimenter read a backstory out loud to the participant that outlined a particular information need that the participant needed to address in order to successfully complete that task. This procedure ensured that reading difculties did not impact participants’ understanding of the tasks, and also avoided giving participants clues regarding the spelling of search queries.\nParticipants indicated that they were done with a search task by pressing a button on the keyboard that was marked with a red\nsticker. They then dictated their answer out loud to the experimenter. The experimenter then administered an oral version of the NASA-TLX (task load index), which was modifed to allow participants to respond on a 5-point (rather than 21-point) scale [18, 28]. This was also done orally to reduce reading strain while reporting the task load scores. Participants also reported their prior domain knowledge using a 3-point scale ranging from 1 (łI could have answered this without searching for any information at allž) to 3 (łI did not have any prior knowledge, and had to search for all information.ž). Finally, participants described any challenges they faced during the task, and reported any strategies they used to overcome those challenges. 3.3.1 Search Phases. The 6 search tasks (see Table 1) were divided into 3 phases, with each phase having 2 tasks to check if searcher behavior remained consistent across task diferences. Tasks were presented in a randomized order. Each phase placed additional variables under experimental control so as to more efectively probe diferent stages of web search. These phases are as follows: • Phase 1: Participants were presented with the landing page of the search engine, and the query box was left empty (See Supplementary Materials for example https://tinyurl.com/sigir2020supplementarymaterial). After hearing the description of a search task, participants were free to formulate any query, and could reformulate as often as desired. Likewise, participants were free to open as many search results as necessary. This phase was the most ecologically valid, and allowed us to collect diferences at the query formulation stage of search between SWD and the control group. However, the ensuing diversity of queries, search results, and page-clicks rendered it difcult to compare the populations in later stages of web search. We address this by introducing additional experimental controls in phases 2 and 3, below. • Phase 2: After hearing the description of a search task, participants were presented with a pre-populated search results page for a fxed query that could not be changed. From here, participants were again free to open as many search results as was necessary to complete the current task. Since all participants viewed the same SERP, we can more efectively measure diferences in the results-triage stage of search. However, participants may open diferent pages, or may open no pages at all (i.e., completing the task using search results’ snippets), thus rendering it difcult to compare populations in the fnal information extraction phase of search. • Phase 3: Similar to Phase 2, participants were presented with a pre-populated SERP for a fxed query that could not be changed, but were additionally asked to open at least three webpages from the search results. This overlap of many participants looking at particular webpages allows us to compare information extraction behavior from webpages. 3.3.2 Task Types. All 6 tasks (2 per phase) were informational search tasks (i.e., tasks with an intent to acquire information present in one or more webpages), as opposed to transactional or navigational tasks [16]. To observe if searcher behavior remained consistent across tasks, we ensured each of the 3 phases had 2 tasks. Each phase’s tasks occupied a diferent level of cognitive complexity, as defned in Kelly et al.’s framework [33]. Specifcally, one task occupied the Remember level of complexity, and required participants to retrieve at most one piece of relevant knowledge. The other task occupied the Understand level of complexity, and required constructing meaning from multiple sources or documents. Of the two, Understand tasks are reported to be harder [18, 56]. When developing tasks for this study, we chose a set that covered diverse domains to reduce efects of prior domain knowledge [20]."},{"heading":"3.4 Measures","text":"To observe and analyze diferences in search and gaze patterns across SWD and control searchers, we measured the following: 3.4.1 Self-Reported Task Load: Each task’s mental, temporal, performance, efort, and stress level self-rated by participants on a 5 point Likert-type post-task questionnaire (a simplifed NASA TLX)\n[18, 28]. Higher TLX scores indicate more challenging tasks, and an overall task load is computed by summing the responses to all fve questions, yielding a score between 5 and 25. 3.4.2 Search Log Measures: From the search logs we measured: (i) Task completion time: total time (in milliseconds) to complete each search task. (ii) Number of queries issued per task. (iii) Length of query: number of terms in a query. (iv) Number of spelling errors in query: including number of phonetic and typographic spelling errors, calculated using DamerauśLevenshtein edit distance [9] which is the minimum number of insertions, deletions, or substitutions of a single character required to change one query term into the correct term. (v) Number of SERPs and webpages visited. (vi) Number of returns to each SERP and webpage\n3.4.3 Eye-tracking Measures: To observe what searchers looked at we divided the SERPs and webpages into several Areas of Interest (AOIs outlined in red in Supplementary Materials https: //tinyurl.com/sigir2020supplementarymaterial). We measured the following across the AOIs from eye tracker data: (i) Attention to an AOI: is measured in terms of fxation duration. This is the total time that someone fxated on the AOI. (ii) Number of search results viewed: items on the search results list on the SERP with at least one fxation. (iii) Fixation Patterns: To observe how participants spatially distribute their visual attention, we looked at a visualization of their fxations on the page to detect any patterns. Previous work [45, 47] has established that searchers examine webpages using the following patterns (see Figure 2):\n• F-shape Pattern: The F-shape Pattern is similar to the shape of its namesake, the letter F. Text on the left and towards the top of the page is read more than text on the right or towards the bottom of the page. [45] • Spotted Pattern: The spotted scanning pattern involves fxating on specifc words or parts of the page. Searchers choose these spots either because of visual salience (like bold or large text), or because word shapes resemble those of specifc words they are looking for to answer the task. [47] • Layer-Cake Pattern: Searchers fxate mostly on the page’s headings and subheadings. There are few other fxations on the text in between Ð that is, until searchers locate the heading or part of the page that they are interested in. At that point, there are many fxations in that particular part. It is the most efective scanning method. [47] • Commitment Pattern: Searchers fxate on all, or most, content words in the body of the page. This pattern demonstrates traditional reading, not scanning, and is the most efective for reading comprehension. [47]\nWe randomly selected 20% of all 167 SERPs and 20% of all 476 webpages visited for two raters to independently label using the above four category defnitions. If a fxation pattern did not ft any of the four categories, then these were labelled as \"NA\". The raters reached an agreement of 0.78 Cohen’s Kappa. The rest were classifed by one of the coders."},{"heading":"4 RESULTS","text":"In this section we report the fndings of the study, beginning with general metrics of task success, and then discuss fndings specifc\nto each study phase, sequentially focusing on query formulation, search results triage, and information extraction from webpages."},{"heading":"4.1 General Results","text":"All 27 participants successfully completed each of the 6 search tasks within the time limit of 10 minutes per task. Prior research has shown that people with dyslexia are slower when reading and composing text [35, 38, 39], so we hypothesized that SWD would take longer to complete search tasks than the control group. Though the control group was slightly faster for all tasks, no diferences were signifcant at the = 0.05 level (See Table 1).\nWe also hypothesized that SWD would report exerting more cognitive efort when completing tasks, given the documented challenges that SWD experience when reading, writing, and remembering information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43]. We were not able to accept this hypothesis for any task in the frst two phases of the experiment, but found indications that SWD may have experienced a higher task load in the third phase, which required participants to read more webpages (Figure 3). Specifcally, SWD reported higher task load for both tasks, but the diference was signifcant only for the Remember task (p = 0.03, t = 2.27). Given the median task load across all participants and tasks was 7 ś only 2 points higher than the minimum score of 5 ś it is possible that our scale was not sensitive enough at the lower end to detect population diferences. Furthermore, in 2019, Choi, Capra and Arguello found that diferences in working memory did not afect participants’ post-task perceptions about workload during search tasks. Since dyslexia is strongly correlated with lower phonological working memory this result is consistent with their fndings [18]. Also, when asked to report their prior domain knowledge for each task, most participants reported that they \"did not have any prior knowledge and had to search for all information\" (x = 2.62, ˙ = 0.41). Therefore, we do not include it in our analysis of search behavior.\nFinally, we note that in contrast to prior work [33, 56], we did not fnd Remember tasks to be systematically easier, or harder, than Understand tasks ś there were no signifcant diferences in task completion time, nor were there signifcant diferences in reported task load. For the remainder of this paper, we therefore do not diferentiate between task complexity types, and instead treat the pairs of tasks as independent trials within each phase. Next, we present phase-specifc fndings."},{"heading":"4.2 Phase 1 Results: Query Formulation","text":"In Phase 1 of the study, participants completed a pair of search tasks by composing queries, reviewing SERPs, and optionally retrieving relevant documents. In this phase, we are best-positioned to compare diferences in query formulation; other comparisons are obscured by diferences in the results and webpages seen by searchers.\nWe expected SWD would formulate more queries than the control group, as mentioned in previous studies [11, 19, 37]. Our fndings are consistent with this hypothesis: SWD formulated an average of 2.27 (˙ = 1.37) queries per task, versus 1.54 (˙ = 1.07) queries per task in the control population (p = 0.03, t52 = 2.20).\nLikewise, we expected SWD to make more spelling errors when composing queries, as reported in [11, 37, 38]. With this measure, we are able to strongly confrm our hypothesis: SWD made an average of 0.67 errors per query (˙ = 0.69), versus 0.04 errors per query (˙ = 0.13) within the control group. This diference is highly signifcant (p < 0.01, t52 = 3.26). To further explore what type of errors these were, we manually labelled the errors into: phonetic error (an error at the phoneme level like \"calqulation\" instead of \"calculation\" or \"dipest\" instead of \"deepest\") and typographic error (error because of a typing mistake, identifed if characters typed are close together on the keyboard like \"tennois\" instead of \"tennis\"). Since those with dyslexia have been shown to have trouble spelling because of challenges with phonological decoding [18, 38], we expected SWDs to make more phonetic errors than the control group. Our analysis confrmed that SWDs make more phonetic errors (x=0.35, ˙=0.49) than the control group (x=0.07, ˙=0.26, p=0.01).\nHowever, as reported in Table 3, we did not fnd signifcant differences in average query length, time spent fxating on the query input box, or number of typographic errors. We suspect that autocomplete and other query suggestions may serve as a normalizing factor for query length, but further research is necessary to confrm this hypothesis. Overall, from these results we confrm that SWD struggle with query formulation - issuing more queries and making more errors per query, especially phonetically.\nMeasure SWD Control p t52 # of queries 2.27 1.54 0.03 2.20 (1.37) (1.07) Errors per query 0.67 0.04 < 0.01 3.26 (edit distance) (0.69) (0.13) Phonetic Errors per 0.35 0.07 0.01 2.82 query (edit distance) (0.49) (0.26) Typographic Errors per 0.79 0.07 0.07 1.90 query (edit distance) (1.40) (0.27) Terms per query 6.12\n(2.53) 6.04 (2.92)\n0.91 0.11\nAttention to Search 788 744 0.93 0.01 Box (ms) (1800) (1811)\nTable 3: Mean(˙ ) Query-formulation behavior by searchers with and without dyslexia during Phase 1. Signifcant diferences at the = 0.05 level are highlighted in grey."},{"heading":"4.3 Phase 2 Results: Search Results Triage","text":"In Phase 2 of the study, participants were presented with a preformulated query-SERP pair per task. They were required to complete tasks by inspecting results on the SERP and optionally opening linked webpages. Since all participants viewed the same SERPs, we can compare SERP triage behavior between groups.\nWe begin by reporting the fxation patterns observed when inspecting the SERPs, and then examine search behavior across specifc AOI types. We found that SWD were most likely to employ the Commitment pattern, and exhibited this strategy for 63.89% of search result pages (vs. 0.05% in the control group). Conversely,\nMeasure Phase 2 SWD Control p t49\n# returns to the same SERP\n4.33 (3.16)\n2.35 (1.37) 0.00 -0.74\n# of webpages opened\n2.77 (1.33)\n2.59 (1.49) 0.44 -0.78\n# results viewed on SERP\n7.67 (4.93)\n3.20 (2.91) 0.04 -0.80\nAttention to Instant Answers (ms) 13,340 (13979) 7,261 (6394) 0.04 -2.07 Attention to Ad results (ms) 5,187 (8161) 837 (1713) 0.01 -2.79 Attention to Organic results (ms) 20,120 (46631) 4,8059 (20724) 0.01 -2.88 Attention to Result titles (ms) 22,339 (19244) 9,507 (8767) 0.00 -3.18 Attention to Result snippets (ms) 24,898 (28977) 7,692 (7360) 0.00 -2.95 Attention to Related Searches (ms) 22,853 (20437) 12,499 (10020) 0.02 -2.38 Attention to Right Rail (ms) 2,506 (3747) 1,607 (2389) 0.32 -1.04 Attention to Images (ms) 4,032 (5094) 4,592 (9270) 0.80 0.26 Attention to Bold (ms) 3,966 (5139) 4,555 (9112) 0.78 0.27\nTable 4: Mean (˙ ) gaze patterns of searchers with and without dyslexia on SERPs in Phase 2. Signifcant diferences at = 0.05 level are highlighted in grey.\npeople in the control group were more likely to employ the F-shape pattern, exhibiting this strategy for 71% of search results (vs. 13.89% in the SWD group). Other scan patterns were rarely employed (Figure 2). The Fisher’s exact test for diferences in proportions fnds these Commitment and F-shape diferences to be highly statistically signifcant (p < 0.01 in both cases). In the context of web search, the Commitment pattern corresponds with a careful and systematic examination of the document, whereas the F-shape pattern indicates quick assessment of relevance (e.g., based on titles), followed by more focused attention on one or more promising snippets [45, 47]. This result helps explain prior fndings reporting SWD struggle with search results triage [37, 43].\nTo further quantify the diferences in search results triage patterns, we count the number of search results visually inspected by participants. We fnd that SWD inspected an average of 7.67 results (˙ = 4.93), which is signifcantly more than the average of 3.20 results (˙ = 2.91) inspected by people in the control group (p = 0.04, t = 0.80). Since without scrolling the SERP shows only 3-4 results as per our experimental setup, this result suggests that SWD scroll further down the SERP than the control group to less relevant results [1]. They might be scrolling further to fnd things that are more readable to them, as suggested by [25, 43]. On the other hand, this might be to fnd more information to add to what they’ve already found, as suggested by P9, an SWD: \"Even if the answer’s given right there at the top, I will continue to look on the page to fnd more information to support it before I fnish searching.\"\nConcern with fnding additional support for an answer may refect a lack of confdence, as suggested by prior literature [6, 31, 43].\nAdditionally, we found that SWD returned to look at the same SERP more times, on average 4.33 (˙ = 2.35) compared with 2.30 returns by the control group (˙ = 2.00, p = 0.00). Since SWD do not open signifcantly more pages than the control group (See Table 3), this suggests that they return to examine the same page more closely or more of it. SWD might re-visit and re-read information because dyslexia is associated with having a shorter working memory [7, 18, 38]. Furthermore, SWD spent more time examining organic search results, instant answers, and advertised results. We also expected SWD to pay more visual attention to bold fonts and images than the control group, because prior work relates these to higher readability scores and SWD prefer more readable documents [25, 35, 41]; however, we could not accept this hypothesis because we did not fnd any signifcant diferences at the = 0.05 level. Nonetheless, feedback from participants in the post-task questionnaire suggests that they rely on these features. For example, P4, an SWD, describes the importance of images: \"I clicked on this (link) because the image showed me it referred to the fencing I was looking for\". P5, also an SWD, says about bold text: \"the answer is right there in bold, so I think it’s right\". Therefore, it is possible that both groups depend equally on these features, or that diferences arise in saccadic patterns rather than fxations.\nEach of these comparisons is detailed in Table 4. Overall, from these results we confrm that SWD struggle with triage of search results: they inspect more results, more carefully, and take longer in this phase than searchers in the control group."},{"heading":"4.4 Phase 3 Results: Information Extraction","text":"In Phase 3 of the study, participants were presented with a preformulated query-SERP pair per task. They were required to complete tasks by inspecting results on the given SERP and opening at least 3 linked webpages. From the 3 webpages selected by each participant, there were only 2 webpages (1 in each task) that all 27 participants all viewed. Therefore, we chose to focus our analysis on those 2 webpages. Phase 3’s main focus was to studying behaviors on retrieved webpages. While there was an overlap in the SERPs viewed by participants in this phase too, we cannot analyze and report on the SERP triage behavior for this phase because triage behavior could have been afected by our requirement to open at least 3 webpages when one or none might have sufced.\nTo analyze information extraction behavior on webpages, we identifed one page for each task that all participants visited. For the webpage seen by all in the Phase 3 Remember Task 1, SWD usually exhibit a Commitment pattern, with 87.5% of SWD exhibiting this strategy (vs 12.5% of the control group). Conversely, participants in the control group were more likely to employ the F-shape pattern, with 77.78% of the control group exhibiting this strategy (vs 23% of SWD). Other scan patterns were rarely employed. The Fisher’s exact test for diferences in proportions fnds these Commitment and F-shape diferences to be highly signifcant (p < 0.01). On the webpage in the Phase 3 Understand Task 2, SWD were, again, most likely to employ the Commitment pattern, with 74.89% of them\n1http://www.tax-rates.org/north_carolina/orange_county_property_tax 2https://theexoticbean.com/blog/fair-trade-cofee/what-the-fair-trade-cofee-labelreally-means/\nexhibiting this strategy (vs 20% of the control group). Conversely, participants in the control group were more likely to employ the Fshape pattern, with 75% of the control group exhibiting this strategy (vs 25% of SWD). Again, other scan patterns were rarely employed. The Fisher’s exact test for diferences in proportions fnds these to be highly signifcant (p < 0.01).\nThese results reiterate the fxation patterns seen in Phase 2 for inspecting SERPs. Since the Commitment pattern signifes reading each part of the page carefully rather than skimming, this result indicates that SWD may be employing less efcient page-scanning strategies during information extraction. Furthermore, these results confrm that SWD exhibit diferent gaze patterns than the control group, and this may indicate that SWD struggle with information extraction, as intimated by the NASA TLX responses reported earlier.\nResults from the post-task questionnaire enrich our understanding of the challenges that SWD face when extracting information from webpages, as well as some of the strategies that SWD employed at this stage of the search process. Specifcally, participants referred to the usefulness of images, menus, tables, and lists; they also reported a desire to encounter fewer ads, less italicized text, and more bold and large text. Referring to menus, P4, an SWD, said, \"I always go to the table of contents on Wikipedia to direct me\", and P23, an SWD, said, \"I couldn’t fnd what I was looking for so I went to the drop down menu and selected ’sort by’ to flter\". P17, also an SWD, said about lists, \"I just want to see a list of the answers in front of me\". About textual features, P18 from the control group said, \"the bold print led me to where I want to go.\" They also said, \"well, it’s got a box around it and it’s bold so I think that helped.\" P12, an SWD, said, \"I found what I was looking for because it was big and bold\". These qualitative results are consistent with prior self-report results [25, 39, 43]. Conversely, though each of these properties was mentioned by participants, we did not fnd any signifcant quantitative diferences in how SWD and the control group fxated on these features (Table 5). It is possible that both groups depend equally on these features, or that diferences arise in saccadic patterns rather than fxations."},{"heading":"5 DISCUSSION","text":"This paper is the frst to present fndings from gaze data collected from English-speaking searchers with and without dyslexia using a modern interactive search engine during the complete query-triageextraction process of search. The gaze patterns and search behavior demonstrate that searchers with dyslexia have more spelling errors (particularly phonetic errors) during query formulation, look further down the SERP, and use less-efcient page-scanning patterns than searchers in the control group. In this section we outline several design implications of our fndings, discuss the study’s limitations, and suggest opportunities for future work."},{"heading":"5.1 Design Implications","text":"5.1.1 Qery Formulation: During query formulation, SWD issued more queries and made more errors per query (particularly more phonetic errors as opposed to typographic errors) than the control group. To help with phonetic spelling errors, autocomplete could optionally be confgured to more aggressively correct spelling errors and/or to incorporate dyslexia-specifc updates to spellcheck\nalgorithms, such as proposed by Rello et al. in the context of word processing [48]. Furthermore, to generally help with formulating their information need, autocomplete could suggest not only popularly searched queries, but also make sure that suggestions are semantically diverse to cover more possibilities. Also, since SWD issued more queries than the control group to fnd more relevant, readable, and trustworthy results, the browser could show searchers rich, multi-modal, instant answers as they type in the query. This could help searchers fnd relevant information quicker, make the result more glanceable, and help them evaluate and refne the query as they’re typing. Furthermore, since instant answers are usually sourced from Wikipedia, a source deemed trustworthy by SWD [34], these answers will reduce the challenge to triage trustworthiness by SWD and non-SWD alike [40].\n5.1.2 Search Results Triage: When triaging search results, SWD came back to look at the same SERP more times and inspected more results. They examined the SERP exhaustively using the Commitment fxation pattern, and scrolled far below the control group to less relevant results to possibly fnd more readable and trustworthy results. Search engines and their interfaces must make it easier to fnd more readable results and assess trustworthiness quickly. These fndings bolster the need to redesign the search algorithm to re-rank results using readability and trustworthiness more heavily than it currently does [21, 51] and add to previous work [25, 35] that informs what factors (e.g., amount of images, headings, ads) lead to more readable SERPs and webpages. To make it easier to triage\ntrustworthiness and readability of a result, we could crowdsource trustworthiness and readability ratings for each result and present it to searchers as a visualization (like in [50]) or an icon (like in [12]). Additionally, since we now know that SWD examine more of the SERP, in a more committed manner (focusing on organic and advertised search results, instant answers, etc.) we can spatially organize SERP elements accordingly to prioritize more readable, relevant, and trustworthy content. For example, regulatory bodies who wish to ensure equal access to content by people with dyslexia may need to pass regulations limiting layout options for ads, pushing more readable, relevant, and credible content to the top-left.\n5.1.3 Information Extraction: When extracting information from webpages, SWD took longer and reported it to be harder than the control group. We also found that SWD examine webpages in a more committed manner than the control group. This could inform how we design webpages and spatially organize relevant and important information. For example, by providing summaries of each webpage at the top that can be used to help navigate the website and to expand only more relevant parts so that SWD don’t have to committedly read the entire page. Furthermore, the browser interface and plugins could increase readability by automatically enlarging text as proposed by Bigham in [15], and to mitigate memory challenges faced by SWD preserve highlighted and zoomed sections. The browser could enable this information to be extracted easily by either taking a screenshot or allowing easy export of these sections into a notepad.\nWhile we propose several design implications for search engines, browsers, SERPs, and webpages based on our fndings, implementing these design ideas and verifying their efectiveness for searchers with dyslexia is left to future work. While our investigation focused on web search, many of these design suggestions may also enhance the usability and inclusivity of other information systems (e.g., ebooks). We also need to do more research to test if these fndings can help inform better search tools for those with other causes for reading diferences such as English language learners and children who are learning to read."},{"heading":"5.2 Limitations and Future Work","text":"This study has limitations as it tries to balance between ecological validity and the need to impose experimental control. Here, we discuss their potential impact, how we tried to address the limitations, and propose future work. First, we did not allow searchers to use any assistive technology, or change any of the default display settings of the browser. This controlled the visual interface all participants experienced, and allowed us to more easily compare behaviors across diferent participants. However, this altered the user experience for at least two people; one participant reported using a more aggressive spelling auto-correct technology on their personal computer and another reported difculty because the study prevented them from zooming in.\nInstructing participants to open at least 3 webpages from the SERP in phase 3 was another limitation of the study design. This potentially impacted how they triaged the search results and extracted information. We tried to reduce these efects by allowing them to choose which 3 links they wanted to open. This freedom\nin choosing the links resulted in having only two webpages common between all participants. If there had been more overlap in the visited webpages in phase 3, we may have had more statistical power in analysing diferences in visual attention paid to elements on webpages.\nWe recruited only searchers fuent in English, above the age of 13, and within a major metropolitan area using ads targeted towards dyslexia and dyslexia-related organizations. Recruiting via social media biased us to a population with a certain level of technical literacy. Furthermore, self-selection bias may also impact our fndings - maybe the people who responded to our ads had more severe dyslexia-related challenges, or had friends or family who had dyslexia and therefore followed dyslexia-related organizations. In future studies we could employ diferent recruitment and sampling methods to reduce these biases.\nWhile we ensured that all SWD had a medical diagnosis of dyslexia, we recognize that dyslexia is a spectrum disorder, and there could be individual diferences that do not generalize to other searchers with dyslexia. In future studies, when quick diagnostic tests of dyslexia (for example, [49]) become widely or clinically accepted or available in English language [8], we can control for individual diferences and better model related behaviors and strategies to diferent points along the spectrum. Future research is required to overcome these limitations, build out and test suggested design implications of these fndings."},{"heading":"6 CONCLUSION","text":"By conducting an eye-tracking study observing 27 participants (14 with dyslexia, 13 control) use a widely-used modern Englishlanguage interactive search engine, we confrmed that searchers with dyslexia struggle with all stages of the search process: query formulation, search results triage, and information extraction. Furthermore, we established that searchers with and without dyslexia have noticeably diferent ways of visually attending to SERPs and webpages, including to individual page elements and to the page overall. Moreover, we learned that many of the challenges and strategies reported by participants in self-report studies manifest in the eye tracking data. We conclude by refecting on our fndings to propose design implications for improving utility and cognitive accessibility of search systems for people with dyslexia."}],"references":[{"title":"Special education teachers. https://datausa.io/profle/soc/252050/ #demographics","author":["n. d"],"venue":null,"citeRegEx":"2","shortCiteRegEx":"2","year":2050},{"title":"Find it if you can: a game for modeling diferent types of web search success using interaction data","author":["Mikhail Ageev","Qi Guo","Dmitry Lagun","Eugene Agichtein"],"venue":"In Proceedings of SIGIR 2011. ACM","citeRegEx":"3","shortCiteRegEx":"3","year":2011},{"title":"Web navigation for individuals with dyslexia: an exploratory study","author":["Areej Al-Wabil","Panayiotis Zaphiris","Stephanie Wilson"],"venue":"In International Conference on Universal Access in Human-Computer Interaction. Springer,","citeRegEx":"4","shortCiteRegEx":"4","year":2007},{"title":"Examining visual attention of dyslexics on web navigation structures with eye tracking","author":["Areej Al-Wabil","Panayiotis Zaphiris","Stephanie Wilson"],"venue":"In 2008 International Conference on Innovations in Information Technology","citeRegEx":"5","shortCiteRegEx":"5","year":2008},{"title":"How dyslexic teenagers cope: an investigation of self-esteem, coping and depression","author":["Neil Alexander-Passe"],"venue":"Dyslexia 12,","citeRegEx":"6","shortCiteRegEx":"6","year":2006},{"title":"Spelling-error tolerant, order-independent pass-phrases via the Damerau-Levenshtein string-edit distance metric. In Proceedings of the Session 4C: Neural Networks and Embedding SIGIR ’20","author":["Gregory V Bard"],"venue":"July 25–30,","citeRegEx":"9","shortCiteRegEx":"9","year":2007},{"title":"Dyslexic children show short-term memory defcits in phonological storage and serial rehearsal: an fMRI study","author":["Harald Beneventi","Finn Egil Tønnessen","Lars Ersland"],"venue":"International Journal of Neuroscience 119,","citeRegEx":"10","shortCiteRegEx":"10","year":2009},{"title":"Search and fnd? An accessibility study of dyslexia and information retrieval","author":["Gerd Berget"],"venue":null,"citeRegEx":"11","shortCiteRegEx":"11","year":2016},{"title":"The efect of dyslexia on searching visual and textual content: are icons really useful","author":["Gerd Berget","Frode Eika Sandnes"],"venue":"In International Conference on Universal Access in Human-Computer Interaction. Springer,","citeRegEx":"12","shortCiteRegEx":"12","year":2015},{"title":"Searching databases without querybuilding aids: implications for dyslexic users","author":["Gerd Berget","Frode Eika Sandnes"],"venue":"Information Research: An International Electronic Journal 20,","citeRegEx":"13","shortCiteRegEx":"13","year":2015},{"title":"Do autocomplete functions reduce the impact of dyslexia on information-searching behavior? The case of G oogle","author":["Gerd Berget","Frode Eika Sandnes"],"venue":"Journal of the Association for Information Science and Technology 67,","citeRegEx":"14","shortCiteRegEx":"14","year":2016},{"title":"Making the web easier to see with opportunistic accessibility improvement","author":["Jefrey P Bigham"],"venue":"In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM,","citeRegEx":"15","shortCiteRegEx":"15","year":2014},{"title":"A taxonomy of web search","author":["Andrei Broder"],"venue":"SIGIR FORUM 36,","citeRegEx":"16","shortCiteRegEx":"16","year":2002},{"title":"What do you see when you’re surfng?: using eye tracking to predict salient regions of web pages","author":["Georg Buscher","Edward Cutrell","Meredith Ringel Morris"],"venue":"In Proceedings of the SIGCHI conference on human factors in computing systems. ACM,","citeRegEx":"17","shortCiteRegEx":"17","year":2009},{"title":"The Efects of Working Memory during Search Tasks of Varying Complexity","author":["Bogeum Choi","Robert Capra","Jaime Arguello"],"venue":"In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval. ACM,","citeRegEx":"18","shortCiteRegEx":"18","year":2019},{"title":"Does dyslexia present barriers to information literacy in an online environment? A pilot study","author":["Lynne Cole","Andrew MacFarlane","George Buchanan"],"venue":"Library and Information Research 40,","citeRegEx":"19","shortCiteRegEx":"19","year":2016},{"title":"Knowledge efects on document selection in search results pages","author":["Michael J Cole","Xiangmin Zhang","Chang Liu","Nicholas J Belkin","Jacek Gwizdka"],"venue":"In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM,","citeRegEx":"20","shortCiteRegEx":"20","year":2011},{"title":"Personalizing web search results by reading level","author":["Kevyn Collins-Thompson","Paul N Bennett","Ryen W White","Sebastian De La Chica","David Sontag"],"venue":"In Proceedings of the 20th ACM international conference on Information and knowledge management. ACM,","citeRegEx":"21","shortCiteRegEx":"21","year":2011},{"title":"What are you looking for?: an eyetracking study of information usage in web search","author":["Edward Cutrell","Zhiwei Guan"],"venue":"In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM,","citeRegEx":"22","shortCiteRegEx":"22","year":2007},{"title":"Visual hierarchy and viewing behavior: An eye tracking study","author":["Soussan Djamasbi","Marisa Siegel","Tom Tullis"],"venue":"In International Conference on Human- Computer Interaction. Springer,","citeRegEx":"23","shortCiteRegEx":"23","year":2011},{"title":"Individual diferences in gaze patterns for web search","author":["Susan T Dumais","Georg Buscher","Edward Cutrell"],"venue":"In Proceedings of the third symposium on Information interaction in context. ACM,","citeRegEx":"24","shortCiteRegEx":"24","year":2010},{"title":"Assessing the readability of web search results for searchers with dyslexia","author":["Adam Fourney","Meredith Ringel Morris","Abdullah Ali","Laura Vonessen"],"venue":"In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. ACM,","citeRegEx":"25","shortCiteRegEx":"25","year":2018},{"title":"Comorbidity of ADHD and dyslexia","author":["Eva Germanò","Antonella Gagliano","Paolo Curatolo"],"venue":"Developmental neuropsychology 35,","citeRegEx":"26","shortCiteRegEx":"26","year":2010},{"title":"n. d.]. Social Media Update 2016","author":["S Greenwood","A Perrin","M Duggan"],"venue":"Pew Research Center for Internet and Technology. November","citeRegEx":"27","shortCiteRegEx":"27","year":2016},{"title":"NASA-task load index (NASA-TLX); 20 years later","author":["Sandra G Hart"],"venue":"In Proceedings of the human factors and ergonomics society annual meeting,","citeRegEx":"28","shortCiteRegEx":"28","year":2006},{"title":"Gender ratios for reading difculties","author":["Jesse L Hawke","Richard K Olson","Erik G Willcut","Sally J Wadsworth","John C DeFries"],"venue":"Dyslexia 15,","citeRegEx":"29","shortCiteRegEx":"29","year":2009},{"title":"The Myths and Truths of Dyslexia in Diferent Writing Systems. International Dyslexia Association blog","author":["F. Hoeft","P. McCardle","K. Pugh"],"venue":null,"citeRegEx":"30","shortCiteRegEx":"30","year":2015},{"title":"Self-concept and self-esteem in developmental dyslexia","author":["Neil Humphrey","Patricia M Mullins"],"venue":"Journal of Research in Special Educational Needs 2,","citeRegEx":"31","shortCiteRegEx":"31","year":2002},{"title":"Computers and Iphones and Mobile Phones, Oh My!: A Logs-based Comparison of Search Users on Diferent Devices","author":["Maryam Kamvar","Melanie Kellar","Rajan Patel","Ya Xu"],"venue":"In Proceedings of the 18th International Conference on World Wide Web (WWW ’09)","citeRegEx":"32","shortCiteRegEx":"32","year":2009},{"title":"Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework","author":["Diane Kelly","Jaime Arguello","Ashlee Edwards","Wan-ching Wu"],"venue":"In Proceedings of the 2015 International Conference on The Theory of Information Retrieval. ACM,","citeRegEx":"33","shortCiteRegEx":"33","year":2015},{"title":"When trustworthy information becomes inaccessible: The search behaviour of users with dyslexia in an online encyclopedia","author":["Birgit Kvikne","Gerd Berget"],"venue":null,"citeRegEx":"34","shortCiteRegEx":"34","year":2018},{"title":"The Impact of Web Browser Reader Views on Reading Speed and User Experience","author":["Qisheng Li","Meredith Ringel Morris","Adam Fourney","Kevin Larson","Katharina Reinecke"],"venue":"In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","citeRegEx":"35","shortCiteRegEx":"35","year":2019},{"title":"Eye tracking and online search: Lessons learned and challenges ahead","author":["Lori Lorigo","Maya Haridasan","Hrönn Brynjarsdóttir","Ling Xia","Thorsten Joachims","Geri Gay","Laura Granka","Fabio Pellacini","Bing Pan"],"venue":"Journal of the American Society for Information Science and Technology 59,","citeRegEx":"36","shortCiteRegEx":"36","year":2008},{"title":"The efect of dyslexia on information retrieval: A pilot study","author":["Andrew MacFarlane","Areej Al-Wabil","Chloe Ruth Marshall","A Albrair","Susan A Jones","Panayiotis Zaphiris"],"venue":"Journal of Documentation","citeRegEx":"37","shortCiteRegEx":"37","year":2010},{"title":"Phonological working memory impacts on information searching: an investigation of dyslexia","author":["Andrew MacFarlane","A Albrair","CR Marshall","George Buchanan"],"venue":"In Proceedings of the 4th Information Interaction in Context Symposium. ACM,","citeRegEx":"38","shortCiteRegEx":"38","year":2012},{"title":"Visual analysis of dyslexia on search","author":["Andrew MacFarlane","George Buchanan","Areej Al-Wabil","Gennady Andrienko","Natalia Andrienko"],"venue":"In Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval. ACM,","citeRegEx":"39","shortCiteRegEx":"39","year":2017},{"title":"Google Instant Search: The Complete User’s Guide","author":["Matt McGee"],"venue":null,"citeRegEx":"40","shortCiteRegEx":"40","year":2010},{"title":"Guideline-Based Evaluation of Web Readability","author":["Aliaksei Miniukovich","Michele Scaltritti","Simone Sulpizio","Antonella De Angeli"],"venue":"In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","citeRegEx":"41","shortCiteRegEx":"41","year":2019},{"title":"Collaborative Search Revisited","author":["Meredith Ringel Morris"],"venue":"In Proceedings of the 2013 Conference on Computer Supported Cooperative Work (CSCW ’13)","citeRegEx":"42","shortCiteRegEx":"42","year":2013},{"title":"Understanding the needs of searchers with dyslexia","author":["Meredith Ringel Morris","Adam Fourney","Abdullah Ali","Laura Vonessen"],"venue":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","citeRegEx":"43","shortCiteRegEx":"43","year":2018},{"title":"SearchTogether: An Interface for Collaborative Web Search","author":["Meredith Ringel Morris","Eric Horvitz"],"venue":"In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology (UIST ’07)","citeRegEx":"44","shortCiteRegEx":"44","year":2007},{"title":"F-Shaped Pattern For Reading Web Content (original eyetracking research)","author":["Jakob Nielsen"],"venue":null,"citeRegEx":"45","shortCiteRegEx":"45","year":2006},{"title":"Exhaustive Review or \"I Can’t Believe It’s Not There\" Phenomenon","author":["Kara Pernice"],"venue":null,"citeRegEx":"46","shortCiteRegEx":"46","year":2017},{"title":"Text Scanning Patterns: Eyetracking Evidence","author":["Kara Pernice"],"venue":null,"citeRegEx":"47","shortCiteRegEx":"47","year":2019},{"title":"Frequent words improve readability and short words improve understandability for people with dyslexia","author":["Luz Rello","Ricardo Baeza-Yates","Laura Dempere-Marco","Horacio Saggion"],"venue":"In IFIP Conference on Human-Computer Interaction. Springer,","citeRegEx":"48","shortCiteRegEx":"48","year":2013},{"title":"Dytective: Diagnosing risk of dyslexia with a game","author":["Luz Rello","Miguel Ballesteros","Abdullah Ali","Miquel Serra","Daniela Alarcón Sánchez","Jefrey P Bigham"],"venue":"In Proceedings of the 10th EAI International Conference on Pervasive Computing Technologies for Healthcare. ICST (Institute for Computer Sciences,","citeRegEx":"49","shortCiteRegEx":"49","year":2016},{"title":"Augmenting web pages and search results to support credibility assessment","author":["Julia Schwarz","Meredith Ringel Morris"],"venue":"In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM,","citeRegEx":"50","shortCiteRegEx":"50","year":2011},{"title":"Fighting search engine amnesia: Reranking repeated results","author":["Milad Shokouhi","Ryen W White","Paul Bennett","Filip Radlinski"],"venue":"In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM,","citeRegEx":"51","shortCiteRegEx":"51","year":2013},{"title":"Eye movement patterns on single and dual-column web pages","author":["Sav Shrestha","Justin W Owens"],"venue":"Usability News 10,","citeRegEx":"52","shortCiteRegEx":"52","year":2008},{"title":"Information Re-retrieval: Repeat Queries in Yahoo’s Logs","author":["Jaime Teevan","Eytan Adar","Rosie Jones","Michael A.S. Potts"],"venue":"In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’07)","citeRegEx":"53","shortCiteRegEx":"53","year":2007},{"title":"Investigating the Querying and Browsing Behavior of Advanced Search Engine Users","author":["Ryen W. White","Dan Morris"],"venue":"In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’07)","citeRegEx":"54","shortCiteRegEx":"54","year":2007},{"title":"Exploratory Search: Beyond the Query- Response Paradigm","author":["Ryen W. White","Resa A. Roth"],"venue":null,"citeRegEx":"55","shortCiteRegEx":"55","year":2009},{"title":"Grannies, tanning beds, tattoos and NASCAR: Evaluation of search tasks with varying levels of cognitive complexity","author":["Wan-Ching Wu","Diane Kelly","Ashlee Edwards","Jaime Arguello"],"venue":"In Proceedings of the 4th Information Interaction in Context Symposium. ACM, 254ś257. Session 4C: Neural Networks and Embedding SIGIR","citeRegEx":"56","shortCiteRegEx":"56","year":2012}],"referenceMentions":[{"referenceID":26,"context":"Dyslexia is a cognitive diference that impacts about 15% of English speakers [7] (incidence rates vary by language [30]).","startOffset":115,"endOffset":119},{"referenceID":31,"context":"These challenges often manifest in a slower reading rate and lower reading comprehension [35].","startOffset":89,"endOffset":93},{"referenceID":2,"context":"engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43].","startOffset":161,"endOffset":180},{"referenceID":15,"context":"engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43].","startOffset":161,"endOffset":180},{"referenceID":31,"context":"engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43].","startOffset":161,"endOffset":180},{"referenceID":33,"context":"engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43].","startOffset":161,"endOffset":180},{"referenceID":39,"context":"engine use - including query formulation, search result triage and information extraction from target webpages - is particularly challenging for this population [4, 19, 35, 37, 43].","startOffset":161,"endOffset":180},{"referenceID":21,"context":"Online experiments comparing search behaviors of people with and without dyslexia have identifed some features of webpages, such as average line length and the ratio of images to text, that impact page readability for people with dyslexia [25].","startOffset":239,"endOffset":243},{"referenceID":12,"context":"Researchers in information retrieval and HCI have extensively studied user interaction with web search systems, particularly for complex informational [16] or exploratory [55] search tasks.","startOffset":151,"endOffset":155},{"referenceID":51,"context":"Researchers in information retrieval and HCI have extensively studied user interaction with web search systems, particularly for complex informational [16] or exploratory [55] search tasks.","startOffset":171,"endOffset":175},{"referenceID":1,"context":"Such tasks typically comprise three stages [3, 43]: query formulation (i.","startOffset":43,"endOffset":50},{"referenceID":39,"context":"Such tasks typically comprise three stages [3, 43]: query formulation (i.","startOffset":43,"endOffset":50},{"referenceID":28,"context":", [32, 53, 54]), gathering self-report data from surveys, interviews, or diary studies of end-users (e.","startOffset":2,"endOffset":14},{"referenceID":49,"context":", [32, 53, 54]), gathering self-report data from surveys, interviews, or diary studies of end-users (e.","startOffset":2,"endOffset":14},{"referenceID":50,"context":", [32, 53, 54]), gathering self-report data from surveys, interviews, or diary studies of end-users (e.","startOffset":2,"endOffset":14},{"referenceID":38,"context":", [42, 43]), and recruiting participants to perform controlled search tasks (e.","startOffset":2,"endOffset":10},{"referenceID":39,"context":", [42, 43]), and recruiting participants to perform controlled search tasks (e.","startOffset":2,"endOffset":10},{"referenceID":18,"context":"have begun to use eye tracking to understand which aspects of the SERP users attend to [22, 23, 36, 52].","startOffset":87,"endOffset":103},{"referenceID":19,"context":"have begun to use eye tracking to understand which aspects of the SERP users attend to [22, 23, 36, 52].","startOffset":87,"endOffset":103},{"referenceID":32,"context":"have begun to use eye tracking to understand which aspects of the SERP users attend to [22, 23, 36, 52].","startOffset":87,"endOffset":103},{"referenceID":48,"context":"have begun to use eye tracking to understand which aspects of the SERP users attend to [22, 23, 36, 52].","startOffset":87,"endOffset":103},{"referenceID":41,"context":"For example, eye tracking studies have revealed that searchers usually fxate on SERPs and webpages in an F-shape pattern [45, 46].","startOffset":121,"endOffset":129},{"referenceID":42,"context":"For example, eye tracking studies have revealed that searchers usually fxate on SERPs and webpages in an F-shape pattern [45, 46].","startOffset":121,"endOffset":129},{"referenceID":20,"context":"These studies have also shown that searchers distribute their visual attention diferently across organic and ad results on a SERP [24], and that one can determine a webpage’s most salient parts by looking at the amount of visual attention paid to its diferent elements [17].","startOffset":130,"endOffset":134},{"referenceID":13,"context":"These studies have also shown that searchers distribute their visual attention diferently across organic and ad results on a SERP [24], and that one can determine a webpage’s most salient parts by looking at the amount of visual attention paid to its diferent elements [17].","startOffset":269,"endOffset":273},{"referenceID":39,"context":"[43] reported SWD have trouble spelling words at the phonetic level.","startOffset":0,"endOffset":4},{"referenceID":9,"context":"Berget and Sandness [13] compared search logs of 21 students with and 21 without dyslexia who had formulated Norwegian queries for a Norwegian academic library system with no query-formulation aids.","startOffset":20,"endOffset":24},{"referenceID":15,"context":"[19] confrmed these fndings for English queries in a system without any query formulation aids.","startOffset":0,"endOffset":4},{"referenceID":10,"context":"Sandness [14] found that when SWD used Google, a modern interactive search engine with query formulation aids, for formulating Norwegian queries, there were no diferences in query formulation behavior.","startOffset":9,"endOffset":13},{"referenceID":33,"context":"[37] conducted a search log analysis of people with dyslexia interacting with an information retrieval interface (Okapi), and found SWD","startOffset":0,"endOffset":4},{"referenceID":6,"context":", the ability to hold words in short-term memory) [10, 38].","startOffset":50,"endOffset":58},{"referenceID":34,"context":", the ability to hold words in short-term memory) [10, 38].","startOffset":50,"endOffset":58},{"referenceID":14,"context":"In 2019, a search log study of participants with low and high working memory found that those with lower working memory take more time to frst click on the SERP, open fewer links, and take more time between events [18].","startOffset":214,"endOffset":218},{"referenceID":2,"context":"’s [4] interview study observing searchers with dyslexia navigate multiple","startOffset":3,"endOffset":6},{"referenceID":3,"context":"[5], observed 7 participants (2 with dyslexia and 5 controls) navigate webpages within 6 websites to extract information.","startOffset":0,"endOffset":3},{"referenceID":35,"context":"A follow-up analysis reported SWD changed scan direction more on the webpage than the control group [39].","startOffset":100,"endOffset":104},{"referenceID":21,"context":"[25] found SWD’s relevance ratings of webpages were highly correlated with their readability scores.","startOffset":0,"endOffset":4},{"referenceID":31,"context":"[35] found using the \"Reader View\" mode in Firefox web browser, Session 4C: Neural Networks and Embedding SIGIR ’20, July 25–30, 2020, Virtual Event, China","startOffset":0,"endOffset":4},{"referenceID":22,"context":"ADHD, a neurological diference that is characterized by difculty focusing, is a common comorbidity that occurs with dyslexia [26].","startOffset":125,"endOffset":129},{"referenceID":25,"context":"In the general population, the gender ratio for dyslexia is typically near parity, or skewed slightly towards males [29].","startOffset":116,"endOffset":120},{"referenceID":23,"context":"recruitment methods ś women are more likely to use social media, follow mailing lists [27], and be special education teachers [2], so may have been more likely to see our ads.","startOffset":86,"endOffset":90},{"referenceID":0,"context":"recruitment methods ś women are more likely to use social media, follow mailing lists [27], and be special education teachers [2], so may have been more likely to see our ads.","startOffset":126,"endOffset":129},{"referenceID":14,"context":"The experimenter then administered an oral version of the NASA-TLX (task load index), which was modifed to allow participants to respond on a 5-point (rather than 21-point) scale [18, 28].","startOffset":179,"endOffset":187},{"referenceID":24,"context":"The experimenter then administered an oral version of the NASA-TLX (task load index), which was modifed to allow participants to respond on a 5-point (rather than 21-point) scale [18, 28].","startOffset":179,"endOffset":187},{"referenceID":12,"context":", tasks with an intent to acquire information present in one or more webpages), as opposed to transactional or navigational tasks [16].","startOffset":130,"endOffset":134},{"referenceID":14,"context":"Of the two, Understand tasks are reported to be harder [18, 56].","startOffset":55,"endOffset":63},{"referenceID":52,"context":"Of the two, Understand tasks are reported to be harder [18, 56].","startOffset":55,"endOffset":63},{"referenceID":16,"context":"When developing tasks for this study, we chose a set that covered diverse domains to reduce efects of prior domain knowledge [20].","startOffset":125,"endOffset":129},{"referenceID":5,"context":"spelling errors in query: including number of phonetic and typographic spelling errors, calculated using DamerauśLevenshtein edit distance [9] which is the minimum number of insertions, deletions, or substitutions of a single character required to change one query term into the correct term.","startOffset":139,"endOffset":142},{"referenceID":41,"context":"Previous work [45, 47] has established that searchers examine webpages using the following patterns (see Figure 2):","startOffset":14,"endOffset":22},{"referenceID":43,"context":"Previous work [45, 47] has established that searchers examine webpages using the following patterns (see Figure 2):","startOffset":14,"endOffset":22},{"referenceID":31,"context":"has shown that people with dyslexia are slower when reading and composing text [35, 38, 39], so we hypothesized that SWD would take longer to complete search tasks than the control group.","startOffset":79,"endOffset":91},{"referenceID":34,"context":"has shown that people with dyslexia are slower when reading and composing text [35, 38, 39], so we hypothesized that SWD would take longer to complete search tasks than the control group.","startOffset":79,"endOffset":91},{"referenceID":35,"context":"has shown that people with dyslexia are slower when reading and composing text [35, 38, 39], so we hypothesized that SWD would take longer to complete search tasks than the control group.","startOffset":79,"endOffset":91},{"referenceID":6,"context":"ing information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43].","startOffset":16,"endOffset":24},{"referenceID":34,"context":"ing information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43].","startOffset":16,"endOffset":24},{"referenceID":4,"context":"ing information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43].","startOffset":134,"endOffset":145},{"referenceID":27,"context":"ing information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43].","startOffset":134,"endOffset":145},{"referenceID":39,"context":"ing information [10, 38] together with their tendency to express lower levels of self-esteem and confdence when performing such tasks [6, 31, 43].","startOffset":134,"endOffset":145},{"referenceID":29,"context":"Finally, we note that in contrast to prior work [33, 56], we did not fnd Remember tasks to be systematically easier, or harder, than Understand tasks ś there were no signifcant diferences in task completion time, nor were there signifcant diferences in reported task load.","startOffset":48,"endOffset":56},{"referenceID":52,"context":"Finally, we note that in contrast to prior work [33, 56], we did not fnd Remember tasks to be systematically easier, or harder, than Understand tasks ś there were no signifcant diferences in task completion time, nor were there signifcant diferences in reported task load.","startOffset":48,"endOffset":56},{"referenceID":7,"context":"We expected SWD would formulate more queries than the control group, as mentioned in previous studies [11, 19, 37].","startOffset":102,"endOffset":114},{"referenceID":15,"context":"We expected SWD would formulate more queries than the control group, as mentioned in previous studies [11, 19, 37].","startOffset":102,"endOffset":114},{"referenceID":33,"context":"We expected SWD would formulate more queries than the control group, as mentioned in previous studies [11, 19, 37].","startOffset":102,"endOffset":114},{"referenceID":7,"context":"Likewise, we expected SWD to make more spelling errors when composing queries, as reported in [11, 37, 38].","startOffset":94,"endOffset":106},{"referenceID":33,"context":"Likewise, we expected SWD to make more spelling errors when composing queries, as reported in [11, 37, 38].","startOffset":94,"endOffset":106},{"referenceID":34,"context":"Likewise, we expected SWD to make more spelling errors when composing queries, as reported in [11, 37, 38].","startOffset":94,"endOffset":106},{"referenceID":14,"context":"Since those with dyslexia have been shown to have trouble spelling because of challenges with phonological decoding [18, 38], we expected SWDs to make more phonetic errors than the control group.","startOffset":116,"endOffset":124},{"referenceID":34,"context":"Since those with dyslexia have been shown to have trouble spelling because of challenges with phonological decoding [18, 38], we expected SWDs to make more phonetic errors than the control group.","startOffset":116,"endOffset":124},{"referenceID":41,"context":", based on titles), followed by more focused attention on one or more promising snippets [45, 47].","startOffset":89,"endOffset":97},{"referenceID":43,"context":", based on titles), followed by more focused attention on one or more promising snippets [45, 47].","startOffset":89,"endOffset":97},{"referenceID":33,"context":"This result helps explain prior fndings reporting SWD struggle with search results triage [37, 43].","startOffset":90,"endOffset":98},{"referenceID":39,"context":"This result helps explain prior fndings reporting SWD struggle with search results triage [37, 43].","startOffset":90,"endOffset":98},{"referenceID":21,"context":"They might be scrolling further to fnd things that are more readable to them, as suggested by [25, 43].","startOffset":94,"endOffset":102},{"referenceID":39,"context":"They might be scrolling further to fnd things that are more readable to them, as suggested by [25, 43].","startOffset":94,"endOffset":102},{"referenceID":4,"context":"\" Concern with fnding additional support for an answer may refect a lack of confdence, as suggested by prior literature [6, 31, 43].","startOffset":120,"endOffset":131},{"referenceID":27,"context":"\" Concern with fnding additional support for an answer may refect a lack of confdence, as suggested by prior literature [6, 31, 43].","startOffset":120,"endOffset":131},{"referenceID":39,"context":"\" Concern with fnding additional support for an answer may refect a lack of confdence, as suggested by prior literature [6, 31, 43].","startOffset":120,"endOffset":131},{"referenceID":14,"context":"SWD might re-visit and re-read information because dyslexia is associated with having a shorter working memory [7, 18, 38].","startOffset":111,"endOffset":122},{"referenceID":34,"context":"SWD might re-visit and re-read information because dyslexia is associated with having a shorter working memory [7, 18, 38].","startOffset":111,"endOffset":122},{"referenceID":21,"context":"We also expected SWD to pay more visual attention to bold fonts and images than the control group, because prior work relates these to higher readability scores and SWD prefer more readable documents [25, 35, 41]; however, we could not accept this hypothesis because we did not fnd any signifcant diferences at the = 0.","startOffset":200,"endOffset":212},{"referenceID":31,"context":"We also expected SWD to pay more visual attention to bold fonts and images than the control group, because prior work relates these to higher readability scores and SWD prefer more readable documents [25, 35, 41]; however, we could not accept this hypothesis because we did not fnd any signifcant diferences at the = 0.","startOffset":200,"endOffset":212},{"referenceID":37,"context":"We also expected SWD to pay more visual attention to bold fonts and images than the control group, because prior work relates these to higher readability scores and SWD prefer more readable documents [25, 35, 41]; however, we could not accept this hypothesis because we did not fnd any signifcant diferences at the = 0.","startOffset":200,"endOffset":212},{"referenceID":21,"context":"These qualitative results are consistent with prior self-report results [25, 39, 43].","startOffset":72,"endOffset":84},{"referenceID":35,"context":"These qualitative results are consistent with prior self-report results [25, 39, 43].","startOffset":72,"endOffset":84},{"referenceID":39,"context":"These qualitative results are consistent with prior self-report results [25, 39, 43].","startOffset":72,"endOffset":84},{"referenceID":44,"context":"in the context of word processing [48].","startOffset":34,"endOffset":38},{"referenceID":30,"context":"Furthermore, since instant answers are usually sourced from Wikipedia, a source deemed trustworthy by SWD [34], these answers will reduce the challenge to triage trustworthiness by SWD and non-SWD alike [40].","startOffset":106,"endOffset":110},{"referenceID":36,"context":"Furthermore, since instant answers are usually sourced from Wikipedia, a source deemed trustworthy by SWD [34], these answers will reduce the challenge to triage trustworthiness by SWD and non-SWD alike [40].","startOffset":203,"endOffset":207},{"referenceID":17,"context":"These fndings bolster the need to redesign the search algorithm to re-rank results using readability and trustworthiness more heavily than it currently does [21, 51] and add to previous work [25, 35] that","startOffset":157,"endOffset":165},{"referenceID":47,"context":"These fndings bolster the need to redesign the search algorithm to re-rank results using readability and trustworthiness more heavily than it currently does [21, 51] and add to previous work [25, 35] that","startOffset":157,"endOffset":165},{"referenceID":21,"context":"These fndings bolster the need to redesign the search algorithm to re-rank results using readability and trustworthiness more heavily than it currently does [21, 51] and add to previous work [25, 35] that","startOffset":191,"endOffset":199},{"referenceID":31,"context":"These fndings bolster the need to redesign the search algorithm to re-rank results using readability and trustworthiness more heavily than it currently does [21, 51] and add to previous work [25, 35] that","startOffset":191,"endOffset":199},{"referenceID":46,"context":"trustworthiness and readability of a result, we could crowdsource trustworthiness and readability ratings for each result and present it to searchers as a visualization (like in [50]) or an icon (like in [12]).","startOffset":178,"endOffset":182},{"referenceID":8,"context":"trustworthiness and readability of a result, we could crowdsource trustworthiness and readability ratings for each result and present it to searchers as a visualization (like in [50]) or an icon (like in [12]).","startOffset":204,"endOffset":208},{"referenceID":11,"context":"interface and plugins could increase readability by automatically enlarging text as proposed by Bigham in [15], and to mitigate memory challenges faced by SWD preserve highlighted and zoomed sections.","startOffset":106,"endOffset":110},{"referenceID":45,"context":"In future studies, when quick diagnostic tests of dyslexia (for example, [49]) become widely or clinically accepted or available in English language [8], we can control for individual diferences and better model related behaviors and strategies","startOffset":73,"endOffset":77}],"year":2020,"abstractText":"Web search is a key digital literacy skill that can be particularly challenging for people with dyslexia, a common learning disability that afects reading and spelling skills in about 15% of the Englishspeaking population. In this paper, we collected and analyzed eyetracking, search log, and self-report data from 27 participants (14 with dyslexia) to confrm that searchers with dyslexia struggle with all stages of the search process and have markedly diferent gaze patterns and search behavior that refect the strategies used and challenges faced. Based on these fndings, we discuss design implications to improve the cognitive accessibility of web search.","creator":"LaTeX with acmart 2019/04/22 v1.60 Typesetting articles for the Association for Computing Machinery and hyperref 2018/11/30 v6.88e Hypertext links for LaTeX"}}
{"name":"3397271.3401164.pdf","metadata":{"source":"META","title":"Web-to-Voice Transfer for Product Recommendation on Voice","authors":["Rongting Zhang","Jie Yang"],"emails":["rongtz@amazon.com","jiy@amazon.com"],"sections":[{"heading":null,"text":"CCS CONCEPTS • Information systems→Recommender systems; •Computing methodologies → Transfer learning; Neural networks.\nKEYWORDS Voice-based recommendation; Web-to-Voice transfer; Repeated purchase\nACM Reference Format: Rongting Zhang and Jie Yang. 2020. Web-to-Voice Transfer for Product Recommendation on Voice. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401164\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401164"},{"heading":"1 INTRODUCTION","text":"In the last few years, there has been an explosion of interest in voicebased technology across industry, with an estimated 144.3 million shipments of voice-enabled devices last year [43]. An important use case for voice-enabled devices is voice shopping, which has become an increasingly important shopping scenario. A recent survey shows that up to 43% of voice-enabled device owners use their device to shop and by 2022, voice is expected to be a $40 billion channel for shopping [9].\nA fundamental task in voice shopping is product recommendation, where the goal is to recommend relevant products to customers by inferring their preferences. While a growing body of research has addressed the voice-based recommendation problem from the dialogue perspective, i.e., improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].\nDue to the unique characteristics of voice interfaces (e.g., narrow information channel), customers tend to explore fewer products and choose fewer long-tail products, as compared to Web-based channels [45]. Consequently, products purchased through Voice are much more limited in terms of both quantity and diversity. Due to the fact that voice interfaces are new and not yet widely adopted, customers on Voice do not have a long history as compared with Web. These problems pose a bigger-than-ever data sparsity challenge that impedes effective recommendation. To mitigate the data sparsity issue, a promising approach is transfer learning: a customer is likely to share similar shopping behaviors on the Web and Voice in terms of favored product types and purchase patterns; by transferring the shared shopping patterns from Web, the system can readily generate recommendations for customers with limited historical purchases on Voice, or even those new to Voice.\nDespite its obvious potential, transfer learning from Web to Voice is non-trivial due to customers’ distinct shopping behaviors on Voice [19]. For example, customers are more inclined to purchase low-consideration products (e.g., paper towel and toothpaste) than high-consideration ones (e.g., computer monitors) on Voice. This is in part due to the recency of Voice as a shopping medium that customers are not used to making complex shopping decisions by voice, in part due to the lack of technology for supporting effective interactions. On the other hand, the convenience of voice interactions triggers a strong tendency of repeated purchases in voice shopping: many products are purchased by the same customers over and over, especially those consumables that need to be purchased on a regular basis. We note that repeated purchase behaviors have also been observed on the Web, which is mainly driven by customers’ loyalty to certain brands [5, 10, 42].\nIn the recommendation literature, transfer learning has been implemented by extending recommendation models, e.g., factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks. While being different in the underlying recommendation models (see Section 2 for a detailed discussion), both classes of methods are designed for transferring user representations with less emphasis on transferring the interaction patterns, which has to be carefully considered in our Web-to-Voice transfer context. More recent work [18] attempts to address the problem by modeling the transfer of the purchase patterns from one domain to another using a linear transformation. However, it fails to capture the relationships of interaction patterns across domains, such as similarity and dissimilarity, which are essential for Web to Voice transfer.\nThis paper introduces TransV, a novel neural network-based recommendation method for transferring customers’ shopping behaviors from Web to Voice while considering the uniqueness of voice shopping. TransV is designed to learn shared customer and product representations across both channels and to carefully transfer the purchase patterns from Web to Voice by modeling their relationships explicitly. Specifically, our method is built on the state-of-the-art self-attention neural architecture [41, 49] to learn customer and product representations. To enable effective transfer of customers’ purchase patterns from Web to Voice, TransV adopts a multi-level tri-factorization approach that models web and voice purchase patterns with both shared and separated neural parts. By doing so, TransV distinguishes channel-specific purchase patterns from channel-independent ones. To account for customers’ repeated purchase behaviors that are prevalent in voice shopping, TransV adopts a recency-biased copy mechanism, which leverages the copy mechanism [16, 36] and extends it by considering the impact of the recency of historical purchases on repeated purchases.\nTransV generates recommendations from a mixture of general and repeated purchase probability, thereby unifying Web-to-Voice transfer learning with repeated purchase modeling in a holistic neural model. It can be trained in an end-to-end manner that automatically learns the importance of general and repeated purchases for generating the most relevant product recommendations on Voice. In summary, we make the following key contributions:\n• We introduce the problem of Web-to-Voice transfer learning for effective recommendation in voice shopping. • We propose a multi-level tri-factorization approach that allows for effective Web-to-Voice transfer while taking into account customers’ behaviors on Voice. • We present a unified neural transfer network that orchestrates both transfer learning and repeated purchasemodeling for voice-based recommendation.\nTo the best of our knowledge, this is the first work to study transfer learning for voice-based recommendation. Extensive validation on multiple real-world datasets, including two cross-channel datasets fromAmazon.com andAmazonAlexa (one of today’smajor voice shopping channels), shows that our method is able to improve the quality of voice-based recommendation by 26.8% as compared with non-transfer learning methods measured by NDCG@1.\n1We use “user” as a generic term and “customer” to specifically refer to the user in shopping contexts; similar for “item” vs. “product”, and “interaction” vs. “purchase”."},{"heading":"2 RELATEDWORK","text":"This section discusses relevant work from the emerging field of voice-based recommendation, and then reviews existing methods related to ours in transfer learning and repeated purchases."},{"heading":"2.1 Voice-based Recommendation","text":"With the rapid increase of personal assistants, a considerable amount of literature has grown up around conversational recommendation. The focal point of research efforts has been enabling the system to effectively and efficiently infer users’ intents and satisfy their information needs [35, 47]. Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44]. Existing work mainly takes a dialogue perspective with the goal of improving the question-answering process, i.e., asking the most relevant questions to collect user feedback.\nFrom the recommendation perspective, most existing work assumes a cold-start setting that ignores long-term preferences of users. For example, Zhang et al. [47] studies the effect of in-session aspect-based questions for product recommendation using memory networks [39]. Li et al. [25] introduce a neural dialogue model that classifies the sentiment of a user with respect to movies discussed in the conversation session, and based on that, it generates movie recommendations with a pre-trained autoencoder recommender [37]. A recent paper by Sun et al. [40] shows that the integration of users’ past purchasing behaviors boosts the effectiveness of voice-based recommendation. Their work, however, concentrates on methods for integrating recommendation techniques into the dialogue system. Our work takes a step back and aims at bridging the conventional recommendation techniques with the recommendation task on Voice, with a specific focus on Web-to-Voice transfer which is of key importance for successful voice shopping in practice."},{"heading":"2.2 Transfer Learning for Recommendation","text":"Transfer learning has been a popular approach for tackling the data sparsity problem by transferring the knowledge (e.g., user preferences) in a source domain to the task in the target domain [6, 24]. In recommendation, transfer learning is generally implemented through multi-task learning [48], i.e., joint model training for recommendation in source and target domains, with a specific focus on transferring user and item representations, or interaction patterns across the domains.\nEarly work focuses on adapting matrix factorization techniques for transfer learning [28, 29, 33]. Liu et al. [28] introduce a model based on collective matrix factorization [38], where user latent factors are shared across different domains. The observed interactions in the source domain help to train better latent factors, thus transferring the knowledge to the target domain. Pan et al. [32, 33] propose to model the interaction patterns in different domains as independent parameter matrices. Factorization models, however, only learn latent factors and parameter matrices in a linear fashion, which are oversimplified in capturing the complex user-item interaction patterns. More importantly, these methods fail to capture the relationship between user interactions in different domains.\nNeural network-basedmethods are more capable of learning nonlinear latent representations for users and items and potentially also their interactions (see a recent survey [46]). Specific to neural transfer learning for cross-domain recommendation, Elkahky et al. [14] introduce a multi-view deep learning method that learns shared user representations from user-item interactions in different domains. Lian et al. [26] propose to incorporate content information into the multi-view neural network. Kanagawa et al. [21] go further in this direction and formulate cross-domain recommendation as extreme multi-class classification, where only content features are used for adapting a classifier trained in the source domain to the target domain. These methods focus on learning better representations for users and items, while emphasizing less learning their interaction patterns, which is important for voice shopping.\nA recent paper [18], perhaps the most closely related work to ours, models the transfer of interactions across multiple domains as a linear transformation using cross-stitch networks [30]. In the context of Web-to-Voice transfer, since the same products appear in both Web and Voice, we adopt a tri-factorization approach [12, 31] that fixes the product representations across channels, and extend the approach into a multi-level scheme, which allows to capture channel-independent and channel-specific interaction patterns."},{"heading":"2.3 Repeated Purchase","text":"Due to the importance for business profitability, repeated purchases are an important customer behavior studied in marketing as a signal of brand loyalty [20, 34]. In Web-based information systems, repeated item consumption has been shown to be most affected by the recency and quality of the item, with recency being more critical [1]. Benson et al. [4] study such a behavior in more detail and reveals the increasing inter-arrival gaps of repeated item consumption that eventually lead to abandonment. In online shopping, Bhagat et al. [5] study repeated purchases for consumable products, e.g., toothpaste and diapers, and propose a prediction model that helps increase the product click-through rate. Our work extends the study to voice shopping and the model to collaborative filtering that recommends both repeated and novel products.\nRepeated purchases have only been considered in recommender system literature recently. Wan et al. [42] introduce a recommendation algorithm, adaLoyal, a personalized grocery recommender. In adaLoyal, the repeated purchase is leveraged in a post-processing procedure to adapt the prediction of purchase probability over an item using the customer’ historical purchases of the item. The adaptation is implemented through a posterior calculation that accounts for both probabilities of general and repeated purchases. Ren et al. [36] propose a unified neural network model that jointly learns repeated and novel consumption for session-based recommendation. Our method is different in that we consider the general sequential recommendation scenario and further model the impact of recency of historical purchases for voice-based recommendation."},{"heading":"3 THE TRANSV MODEL","text":"This section introduces our proposed method TransV for transferring customers’ shopping patterns from Web to Voice. The overall structure of TransV is illustrated in Figure 1(a). It is composed of the following modules: 1) Sequence Encoder, which learns high-quality\ncustomer and product representations shared across channels given historical customer-product interaction records from both channels; 2) Multi-level Interaction Module, which models the purchase patterns on the Web and Voice as well as their relationships using a multi-level tri-factorization approach; 3) Repeated Purchase Module, which captures customers’ repeated purchase behaviors on both channels; and 4) Mixture Output Module, which generates the output from a mixture of general and repeated purchase probability.\nIn the following, we start by formalizing the problem before introducing each of the modules in a separate subsection. Problem Statement.We model voice-based recommendation as a sequential recommendation problem, i.e., given a sequence of products a customer has previously purchased on theWeb and Voice, we predict the next purchases. Formally, letU andV be the set of customers and products, respectively; let Su = [vu,1,vu,2, . . . ,vu,nu ] andTu = [tu,1, tu,2, . . . , tu,nu ] denote the sequence of products customer u ∈ U purchased on both channels and the corresponding timestamps of the purchases. Note that a timestamp is represented as the difference with respect to a reference time point in terms of the number of weeks; this allows us to map the timestamps to embeddings, so as to model the temporal effect of past purchases.\nSuppose tk is the time of prediction (tu,nu ≤ tk ), our sequential recommendation problem with Web-to-Voice transfer is formulated as predicting the purchase probability of any product on both channels at tk+1:\nP(vdtk+1 = v |Su ,Tu ), v ∈ V,d ∈ D, (1) where D is the set of channels (i.e., Web and Voice in our case)."},{"heading":"3.1 Sequence Encoder","text":"The sequence encoder takes as input the sequence of products historically purchased by a customer and generates customer representations. It starts by encoding the products and the corresponding timestamps of purchases, then encodes the sequence of embeddings with transformer layers that take into account the dependencies between the purchases, and finally generates customer representations through a self-attention pooling layer. Embedding Products and Timestamps. Item encoding is implemented as a single embedding lookup layer. In our case, we also consider the category taxonomy of products available to obtain higher quality embeddings. Specifically, we define the embedding of a product vj ∈ V as the sum of embeddings of its affiliated categories at different levels of the taxonomy:\nvj = L∑ l=1 v(l )j , (2)\nwhere v(l )j ∈ Rm is the embedding of vj ’s category at the l-th level. To capture the temporal dynamics of Su , we embed the timestamp of a purchase tu,i ∈ Tu : tu,i = τ (tk − tu,i ), (3)\nwhere τ represents a embedding lookup layer such that tu,i ∈ Rm . The final representation of a product purchase is given as the sum of the item embedding and time embedding:\neu,i = vu,i + tu,i . (4)\nTransformer Layers. To learn high-quality customer representations, we leverage the transformer layers [41] that capture the dependency between the purchases based on semantic affinity in the embedding space. To do so, we adopt the multi-head self-attention mechanism [27, 41], which allows jointly attending to information from different parts of the purchase sequence. Formally, let E = [eu,1, eu,2, . . . , eu,nu ]T ∈ Rnu×m be the output from the embedding layer, we construct new representations using h attention heads, each learning a specific dependency relationship within the sequence as follows:\nMH(E) = [head1,head2, . . . ,headh ]WO , (5)\nheadi = Attention(EWQi ,EW K i ,EW V i ,m/h), (6)\nwhereWQi ,W K i ,W V i ∈ Rm×m/h are projection matrices for each attention head and WO ∈ Rm×m is the output projection matrix for the heads combined. The attention function is given by:\nAttention(Q,K,V,m) = Softmax(QK T\n√ m )V. (7)\nIn addition, we apply a position-wise feed-forward layer to the output of the multi-head self-attention layers. The output of the feed-forward layer is calculated as\nA = LayerNorm(E + Dropout(MH(E))), (8) S = Dropout(σ F (AWF1 + bF1 ))WF2 + bF2 , (9) F = LayerNorm(A + Dropout(S)), (10)\nwhere WF1 and W F 2 are parameter matrices and b F 1 and b F 2 are bias terms; σ F (x) Softrelu(x) = log(1 + ex ) is applied element-wise. Here we adopt residual connection [17] and layer normalization [2].\nSelf-Attention Pooling LayerWe then construct three types of customer representations, up , uq , uπ , to model the general customer preference, repeated purchase preference and their relative importance for recommendation, respectively. To do so, we apply an attention-based pooling layer [27]:\nu [up , uq , uπ ]T (11) =FTDropout(Softmax(Dropout(σP (FWP1 ))WP2 )),\nwhere σP Softrelu,WP1 ∈ Rm×m andWP2 ∈ Rm×3 are parameter matrices."},{"heading":"3.2 Multi-Level Interaction Module","text":"Now we consider the problem of modeling customer-product interactions induced from customers’ general preference (i.e., nonrepeated purchase) on different channels.\nA standard approach to model interactions on different domains when entities are shared is matrix tri-factorization [31], which models the interaction scoring function on a specific domain d as:\nb d,p u, j = u pWd,pvTj , (12)\nwhereWd,p is a channel-specific parametermatrix capturing customerproduct interaction patterns. Such a formulation, however, cannot capture the similarity between interaction patterns across channels.\nWe introduce the multi-level interaction layers which model customer-product interaction as the sum of two factors, the channelindependent affinity of the product to the customer’s taste and the channel-specific one. Formally, we calculate the interaction score by:\nb d,p i, j = u pvTj + ũ pWd,p ṽTj , (13)\nwhere ũp , ṽj are low-dimensional vectors derived from up , vj through fully connected layers with Softrelu activation. The first term upvTj captures customers’ purchase patterns across channels, while the second term ũpWd,p ṽTj captures channel-specific patterns. The details of our multi-level interaction layers are depicted in Figure 1(b).\nWe then obtain the interaction probability over all the products using softmax:\np̂du = Softmax([b d,p u,1 , . . . ,b d,p u, |V |]), (14)\nwhere p̂du, j models the probability that the customer u would purchase a specific product j among all the products on channel d ."},{"heading":"3.3 Repeated Purchase Module","text":"We now introduce our method for modeling the specific interaction pattern of repeated purchases of the same product. The key idea is to learn a probability of repeating a historical purchase for each customer. To do so, we adopt the copy mechanism [16], which models repeated purchases as copying purchases from the past. In the specific context of product recommendation, the probability is dependent not only on the customer’s repeated purchase preferences, but also on the recency of the historical purchases. We therefore extend the copy mechanism by considering the impact of the recency of historical purchases on repeated purchases.\nSince the repeated purchase behavior can vary across different channels, we model the repeated purchase score of product vu,i ∈ Su for customer u ∈ U by:\nb d,q u,(u,i) = u qvTu,i + ũ qWd,q ṽTu,i , (15)\nwhere vu,i is the vector representation of vu,i and ũq , ṽu,i are lowdimensional vectors derived from uq , vu,i through fully connected layers with Softrelu activation, respectively. Note that here we consider each purchase in the sequence, i.e., vu,i ∈ Su , rather than each product, as a candidate for repetition. The repeated purchase probability for the same product will be summed up.\nIn practice, customers tend to repeatedly purchase products that are recently purchased. We therefore, consider the impact of a historical purchase on current purchase decision as affected by purchase recency. Given the timestamp of the i-th purchase tu,i and the time of prediction tk , we define a bias term for modeling time recency as: дu,i = T (tk − tu,i ), (16) where T (·) is a learnable scalar function which can be represented by a single-dimensional embedding lookup layer. With such a bias term, the distribution of repeated purchase probability over historical purchases is given by:\nqdu,(u,i) = exp(bd,qu,(u,i) + дu,i )∑nu l=1 exp(b d,q u,(u,l ) + дu,l ) . (17)\nConsequently repeated purchase probability of product vj ∈ V is\nqdu, j = nu∑ i=1 1{vj=v(u,i ) }q d u,(u,i), (18)\nwhere 1{·} is the indicator function returning 1 if the statement is True and 0 otherwise. We note that in a degenerate case where the repeated purchase score and time recency bias for all purchased\nproducts are equal, the resulting distribution of repeated purchase probability is equivalent to the empirical purchase frequency."},{"heading":"3.4 Mixture Output and Loss Function","text":"The overall probability of a customer purchasing a product is modeled as a mixture of the general purchase probability and the repeated purchase probability, i.e., p̂d and qd :\npdu = π d u q d u + (1 − πdu )p̂du . (19)\nwhere πdu is the mixture weight that describes the importance of repeated purchase in customers’ shopping decision.\nThe mixture weight is considered to be dependent on specific customers and the recency of their last purchases. Formally, considering the timestamp of the last purchase tu,nu , we define the time recency bias as:\nhu = S(tk − tu,nu ), (20) where S(·) is a learnable scalar function. The mixture weight πdu is given by:\nπdu = exp(wd,π uπ + bd,π + hu )\n1 + exp(wd,π uπ + bd,π + hu ) , (21)\nwhere wd,π ∈ Rm and bd,π ∈ R are parameters to be learned. Loss Function. Our model generates recommendations on both Web and Voice. To consider the importance of recommendation relevance on both channels for model training, we introduce a hyperparameter αd as the weight for loss on channel d ∈ D. Specifically, let cdu, j be the normalized empirical frequency of observed purchases on channel d for customer u ∈ U and product vj ∈ V , the training loss is given by:\nL = |D |∑ d=1 αd ∑ u ∈U |V |∑ j=1 cdu, j log(pdu, j ). (22)"},{"heading":"4 EXPERIMENTS AND RESULTS","text":"In this section, we perform experiments to evaluate the performance of TransV.We aim to answer the following questions:\n• Q1: How much benefit does modeling repeated purchases bring to recommendation performance? • Q2: Howwell does our proposedWeb-to-Voice transfer learning perform for recommendation on Voice? • Q3: How effective is TransV in uncovering the impact of historical purchase recency on reorder and weighting the importance of repeated purchases in voice shopping?\nIn addition, we investigate the impact of data sparsity on the effectiveness of our method. In the following, we start by introducing our experimental setup, before answering each of the above questions in a separate subsection."},{"heading":"4.1 Experimental Setup","text":"Datasets. We evaluate our proposed model on four real-world shopping datasets. Among them, two datasets are publicly available and each contains customers’ purchase records on a single channel. These datasets allow us to evaluate the effectiveness of TransV in modeling repeated purchases.\n• Dunnhumby: This is a grocery shopping dataset released by Dunnhumby.2 It contains shopping history of 2,500 households for around two years. • Instacart: This is another grocery shopping dataset released by Intacart.3 It contains shopping history of more than 200 thousand users. This dataset only records the gap between two consecutive transactions with a cutoff at 30 days where gaps greater than 30 days are logged as 30 days. Thus, we only keep users whose maximum gap between two consecutive orders is less than 30 days so all her shopping timestamps can be reconstructed. To evaluate our model in transfer learning, we construct two cross-channel datasets from Amazon.com4 and Amazon Alexa5 that contain customers’ purchase records on both Web and Voice.6\n• AmazonGrocery:This is a proprietary grocery shopping dataset collected from Amazon. It contains a sample of customers’ grocery shopping histories on both Web and Voice. • Amazon Home: This is a proprietary home products shopping dataset collected from Amazon (e.g., water filter, coffee maker). It contains a sample of customers’ shopping histories of home products on both Web and Voice. For every user, we create sliding windows of Nw + 2 weeks with a step size of 2 weeks from her historical shopping record. TransV consumes purchases from the first Nw weeks and generates recommendations for the last 2 weeks. Each sliding window of a user is considered as a sample. We split the samples into a training and a test set based on the timestamps of the purchases such that the purchases in the last 2 weeks of the test samples do not appear in the training samples. Afterward, we filter the dataset by only keeping items purchased bymore than 10 unique customers. For the Amazon datasets, we only keep samples with at least one purchase on Voice in the test set. Key statistics from public and proprietary datasets are presented in Table 1 and 2 respectively. Comparison Methods. To demonstrate the effectiveness of our method in modeling repeated purchases, we compare with the\n2https://www.dunnhumby.com/careers/engineering/sourcefiles 3https://www.instacart.com/datasets/grocery-shopping-2017 4https://www.amazon.com 5https://developer.amazon.com/alexa 6To protect customers, all data from Amazon.com and Alexa were anonymized and un-identified before the experiment and analysis were performed.\nfollowing state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases. For fair comparison, we use the same attention based pooling layer and the inner product interaction layer for all the above comparison methods and our model. To ablate the effect of transfer learning, the compared methods are all trained on combined samples from Web and Voice – as repeated purchases are a cross-platform behavior – without learning channel-specific interactions. As an additional baseline, we also compare with user-itemPop which only relies on the customerwise empirical frequency of products for recommendation.\nTo investigate the effectiveness of our multi-level interaction module for transfer learning, we compare the following variants of TransV: 1) Non Transfer: the variant that only utilizes voice data for recommendation, 2)Direct Transfer: the variant that does not learn channel-specific interactions, i.e., the same configuration used in comparing methods for modeling repeated purchases, 3) Tri-Factor: the variant where tri-factorization is used to model interactions across Web and Voice, 4) TransV: the variant where our proposed multi-level interaction module is used for the transfer.\nEvaluation Protocols.We measure the performance of the compared methods using two metrics 1) Normalized Discounted Cumulative Gain (NDCG) at K = {1, 5, 10} and 2) Area Under the ROC Curve (AUC). AUC captures the overall ranking performance by comparing customer purchased products with non-purchased ones in a pairwise manner, regardless of the position of the compared pair in the generated ranking list. Unlike AUC, NDCG@K weights the top-ranking positions as more important than the others. Such a difference makes NDCG@K (especially when K = 1) a more suitable metric for voice-based recommendation, where the top recommendations need to be highly precise due to the narrow information channel. We note that results reported in this section for the two cross-channel datasets are all obtained from Voice.\nParameter Settings.We empirically set optimal parameters based on a head-out validation set that contains 10% of the test data. For all methods, the dimension of the embedding is set to 64. For the learning rate and regularization weight, we apply a grid search in {10−4, 10−3, 10−2, 10−1} . The dropout rate is selected from the set {0.0, 0.1, 0.2, 0.3}. To find the optimal default loyalty l0 for adaLoyal, we apply grid search in {0.1, 0.2, ..., 0.9}. For Transformer models, we set the number of attention heads to 8. To keep the most recent purchases, we set the maximum sequence length of historical purchases as follows: N = 300 for Dunnhumby, N = 200 for Instacart and N = 60 for Amazon Grocery and Home. All the models\nare implemented with MXNet7. Model training is performed using Adagrad [13] with mini-batches of size 128. All the gradients are clipped between −10 and 10 to prevent exploding [3]."},{"heading":"4.2 Results on Repeated Purchase (Q1)","text":"We start by investigating the effectiveness of our proposed repeated purchase module by comparing it against user-itemPop, Bi-LSTM, Transformer, adaLoyal Transformer and RepeatNet. Results on public and proprietary datasets are reported in Table 3 and 4, respectively. In Table 4, the performance is shown in terms of relative ratio with respect to that of user-itemPop tested on voice purchases.\nWe observe that Bi-LSTM and Transformer achieve better performance than user-itemPop when measured by AUC however are outperformed by user-itemPop when measured by NDCG@K . Recall that Bi-LSTM and Transformer are general collaborative filtering approaches that do not explicitly model customers’ repeated purchase behaviors. The result indicates that while these methods are generally effective in recommendation, they are not suitable for precise recommendation where the top-ranked products need to be highly relevant. Unlike Bi-LSTM and Transformer, methods that consider repeated purchases such as RepeatNet and TransV, achieve higher performance than user-itemPop. Such a comparison clearly demonstrate the need to account for repeated purchases in recommendation.\n7https://mxnet.apache.org/\nAmong these methods, we observe that RepeatNet and TransV outperform adaLoyal Transformer on Amazon Grocery and Amazon Home across both types of metrics; and on Dunnhumby and Instacart, they outperform adaLoyal Transformer when performance is measured by NDCG@K . Recall that adaLoyal Transformer takes a post-processing approach for modeling repeated purchases: it re-ranks the result from the general collaborative filtering method Transformer by considering customers’ historical product purchases; RepeatNet and TransV, on the other hand, consider repeated purchases as an integral part in modeling customers’ purchase behaviors. The comparison results demonstrate the benefit of the latter approach for precise recommendation. This can be explained by its capability of accurately capturing the effect of repeated purchases in customers’ purchase behaviors, which helps generate recommendations that are more relevant.\nOur proposedmethod TransV achieves the best NDCG@K across all the datasets and highest AUC on the two cross-channel datasets. In particular, TransV consistently outperforms RepeatNet on both datasets across both metrics. This is mainly due to the effectiveness of considering time bias in modeling repeated purchases, which we discuss further in section 4.4. Overall, TransV outperforms RepeatNet by 1.97% for the Web datasets (averaged over NDCG@K for K = {1, 5, 10}; p-value < .001, WilCoxon signed-rank test) and by 2.47% for voice-based recommendation on the cross-channel datasets in terms of NDCG@1 (p-value < .001 on Grocery and < .01 on Home, WilCoxon signed-rank test)."},{"heading":"4.3 Results on Transfer Learning (Q2)","text":"We evaluate our proposed transfer learning module. To understand the impact of the relative sparsity of Web data with respect to that of voice data on the effectiveness of transfer learning, we first divide customers into groups of different historical numbers of purchases on Web; then for each of the group, we further divide customers into groups according to their number of purchases on Voice.\nThe overall results on the two cross-channel datasets, i.e., Amazon Grocery and Home, are reported in Table 5. We observe that Direct Transfer is outperformed by Tri-factor, which is further outperformed by our proposed approach TransV. The result signifies the importance of transferring the interaction patterns between customers and products for voice-based recommendation. More importantly, the superior performance obtained by TransV compared with Tri-factor signifies the advantage of multi-level interaction in Web-to-Voice transfer for taking into account the distinct purchase patterns of customers on Voice.\nCompared with non-transfer learning (Table 5), TransV substantially improves voice-based recommendation by 26.81%, 36.47%, and 37.13% for NDCG@K for K = {1, 5, 10}, respectively.\nImpact of Data Sparsity. Results on customer groups with different numbers of purchases on Web and on Voice are depicted in Figure 2. We first note that applying user-itemPop on Voice (useritemPop voice in the figure) results in different performance for voice-based recommendation than regular user-item Pop which is applied on purchases from both channels; moreover, it generally has better performance. This confirms that customers’ shopping behavior on Voice is different from that on Web and in particular, they tend to have more repeated purchases on Voice. Comparing the transfer learning performance on customers with different numbers of historical purchases on Voice (subgroups in Figure 2(a-d)), we observe that transfer learning is most beneficial for the customers with the least number of historical purchases on Voice. Finally, we observe that for customers with a similar number of voice purchases, transfer learning is more beneficial for customers with more purchases on the Web (Figure 2(a) vs. (b), and (c) vs. (d)). These results demonstrate that Web data indeed can largely alleviate the data sparsity issue in voice-based recommendation.\nImpact of Voice Weight. In transfer learning, TransV weights the importance of the recommendation tasks for different channels with the hyperparamter αd in its loss function. To investigate the impact of the weight, we conduct an experiment with different weight values on Amazon Grocery and Home datasets. We fixed the task weight for Web as 1 and apply a grid search in {1, 2, 5, 10} for the weight for recomendation on Voice. Figure 3 shows the performance measured by NDCG@K . We observe that as the weight for voice increases, the performance first increases then decreases. The best performance is achieved when weight for Voice is 5 for Amazon Grocery and 2 for Amazon Home. This result suggests that with an appropriate setting for the weight of recommendation on Voice, our approach can effectively transfer customers’ purchase patterns from Web-to-Voice while taking into account the unique characteristics of voice shopping. Besides, the similarity in performance variation across αd values on the two datasets shows the robustness of TransV.\n4.4 Properties of TransV (Q3) To further show how TransV works, we conduct an in-depth analysis of the properties of TransV. We show how TransV can strike a balance between modeling general and repeated purchases for effective recommendation and can uncover a time decay phenomenon of the effect of historical purchases on repeated purchases.\nLearning Mixture Weight. TransV learns the mixture weight π to capture the importance of repeated purchases in customers’ shopping decisions. Being able to learn such mixture weight is important for generating recommendations of high-relevance. For comparison, Figure 4 shows the learned mixture weights against the empirical repeated purchase rate for Amazon Grocery and Instacart on the test dataset. We observe that the learned mixture weight correlates positively with the empirical repeated purchase rate. This demonstrates the effectiveness of TransV in striking a balance between modeling general and repeated purchases for recommendation. In particular, we observe that for Amazon Grocery the predicted mixture weight on Voice is higher than that onWeb, which corresponds well with the statistics of the dataset (Table 2). As a remark, we note though that the predicted mixture weight does not reflect the exact empirical repeated purchase rate. This is likely due to the fact that general preference also contributes to repeated purchases and the current model cannot capture all the discriminant factors. We leave the improvement to future work.\nLearning Time Bias. TransV learns time bias functions T (·) and S(·) to weight the importance of the recency of historical purchases on repeated purchases.T (·) captures the relative importance within the historical purchases, while S(·) captures the importance with respect to general collaborative filtering. On our experimental dataset, these functions are learned as piecewise constant functions on equal-spaced bins each representing a week.\nFigure 5(a) shows the learned time bias T (·). We observe a general pattern that the importance of historical purchases decreases when the purchase occurs further in the past. The result implies that recently purchased products are more likely to be repeatedly purchased. We also notice that the time bias function for Amazon Grocery shows a slightly different pattern: it first increases then decreases. This can be explained by the fact that grocery products are generally consumed for a few weeks before being repeatedly purchased, e.g., trash bags, drinks. Interestingly, we observe a specific periodic pattern on the two cross-channel datasets, and the local peaks occur on a monthly basis. This is due to the subscription feature provided by Amazon where customers can elect to subscribe to products monthly.\nSimilar results are observed for S(·) from Figure 5(b): rrecently purchased products are more likely to be repeatedly purchased, thus playing amore important role in the recommenation generation.We note that S(·) of Instacart decreases for the first several weeks and then approaches zero. This is because customers with greater than the maximum gap (30 days) between two consecutive purchases are filtered out. This results in a skewed distribution of time since the last purchase, leading to the diminishing of S(·) after several weeks."},{"heading":"5 CONCLUSION","text":"We presented TransV, a neural transfer network that addresses the data sparsity issue of voice-based recommendation by transferring customers’ shopping patterns from the Web to Voice. It employs multi-level tri-factorization to capture the similarity and dissimilarity of customers’ shopping patterns on the Web and Voice, thereby allowing effective Web-to-Voice transfer, while taking into account distinct voice shopping patterns. TransV is seamlessly integrated with a recency-based copy mechanism to capture the prevalent behavior of repeated purchases on Voice. Our extensive evaluation on multiple real-world datasets, including two cross-channel datasets from Amazon, shows that TransV significantly improves the performance of voice-based recommendation. Our analysis further offers valuable insights into customers’ voice shopping behaviors, e.g. recent purchases are more likely to be repeated. As future work, we plan to study how to integrate TransV with natural language generation for conversational recommendations."}],"references":[{"title":"The dynamics of repeat consumption","author":["Ashton Anderson","Ravi Kumar","Andrew Tomkins","Sergei Vassilvitskii"],"venue":"In Proceedings of the 23rd International Conference on World Wide Web","citeRegEx":"1","shortCiteRegEx":"1","year":2014},{"title":"Advances in optimizing recurrent networks","author":["Yoshua Bengio","Nicolas Boulanger-Lewandowski","Razvan Pascanu"],"venue":"IEEE International Conference on Acoustics, Speech and Signal Processing","citeRegEx":"3","shortCiteRegEx":"3","year":2013},{"title":"Modeling user consumption sequences","author":["Austin R Benson","Ravi Kumar","Andrew Tomkins"],"venue":"In Proceedings of the 25th International Conference on WorldWideWeb. InternationalWorldWideWebConferences Steering Committee,","citeRegEx":"4","shortCiteRegEx":"4","year":2016},{"title":"Buy It Again: Modeling Repeat Purchase Recommendations","author":["Rahul Bhagat","Srevatsan Muralidharan","Alex Lobzhanidze","Shankar Vishwanath"],"venue":"In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","citeRegEx":"5","shortCiteRegEx":"5","year":2018},{"title":"Transfer learning for collective link prediction in multiple heterogenous domains","author":["Bin Cao","Nathan N Liu","Qiang Yang"],"venue":"In Proceedings of the 27th International Conference on Machine Learning","citeRegEx":"6","shortCiteRegEx":"6","year":2010},{"title":"Q&R: A two-stage approach toward interactive recommendation","author":["Konstantina Christakopoulou","Alex Beutel","Rui Li","Sagar Jain","Ed H Chi"],"venue":"In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","citeRegEx":"7","shortCiteRegEx":"7","year":2018},{"title":"Towards conversational recommender systems","author":["Konstantina Christakopoulou","Filip Radlinski","Katja Hofmann"],"venue":"In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,","citeRegEx":"8","shortCiteRegEx":"8","year":2016},{"title":"Has brand loyalty declined? A longitudinal analysis of repeat purchase behavior in the UK and the USA","author":["John Dawes","Lars Meyer-Waarden","Carl Driesener"],"venue":"Journal of Business Research 68,","citeRegEx":"10","shortCiteRegEx":"10","year":2015},{"title":"Towards end-to-end reinforcement learning of dialogue agents for information","author":["Bhuwan Dhingra","Lihong Li","Xiujun Li","Jianfeng Gao","Yun-Nung Chen","Faisal Ahmed","Li Deng"],"venue":null,"citeRegEx":"11","shortCiteRegEx":"11","year":2016},{"title":"Orthogonal nonnegative matrix t-factorizations for clustering","author":["Chris Ding","Tao Li","Wei Peng","Haesun Park"],"venue":"In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining","citeRegEx":"12","shortCiteRegEx":"12","year":2006},{"title":"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization","author":["John Duchi","Elad Hazan","Yoram Singer"],"venue":"The Journal of Machine Learning Research","citeRegEx":"13","shortCiteRegEx":"13","year":2011},{"title":"A multi-view deep learning approach for cross domain usermodeling in recommendation systems","author":["Ali Mamdouh Elkahky","Yang Song","Xiaodong He"],"venue":"In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee,","citeRegEx":"14","shortCiteRegEx":"14","year":2015},{"title":"Hybrid speech recognition with deep bidirectional LSTM","author":["Alex Graves","Navdeep Jaitly","Abdel-rahman Mohamed"],"venue":"IEEE Workshop on Automatic Speech Recognition and Understanding","citeRegEx":"15","shortCiteRegEx":"15","year":2013},{"title":"Incorporating copying mechanism in sequence-to-sequence learning","author":["Jiatao Gu","Zhengdong Lu","Hang Li","Victor OK Li"],"venue":"arXiv preprint arXiv:1603.06393","citeRegEx":"16","shortCiteRegEx":"16","year":2016},{"title":"Deep residual learning for image recognition","author":["Kaiming He","Xiangyu Zhang","Shaoqing Ren","Jian Sun"],"venue":"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","citeRegEx":"17","shortCiteRegEx":"17","year":2016},{"title":"Conet: Collaborative cross networks for cross-domain recommendation","author":["Guangneng Hu","Yu Zhang","Qiang Yang"],"venue":"In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM,","citeRegEx":"18","shortCiteRegEx":"18","year":2018},{"title":"The Challenges of Moving from Web to Voice in Product Search","author":["Amir Ingber","Arnon Lazerson","Liane Lewin-Eytan","Alexander Libov","Eliyahu Osherovich"],"venue":"In Proceedings of the 1st International Workshop on Generalization in Information Retrieval","citeRegEx":"19","shortCiteRegEx":"19","year":2018},{"title":"Brand loyalty vs. repeat purchasing behavior","author":["Jacob Jacoby","David B Kyner"],"venue":"Journal of Marketing research 10,","citeRegEx":"20","shortCiteRegEx":"20","year":1973},{"title":"Cross-domain recommendation via deep domain adaptation","author":["Heishiro Kanagawa","Hayato Kobayashi","Nobuyuki Shimizu","Yukihiro Tagami","Taiji Suzuki"],"venue":"In European Conference on Information","citeRegEx":"21","shortCiteRegEx":"21","year":2019},{"title":"Self-attentive sequential recommendation","author":["Wang-Cheng Kang","Julian McAuley"],"venue":"IEEE International Conference on Data Mining","citeRegEx":"22","shortCiteRegEx":"22","year":2018},{"title":"Attentive memory networks: Efficient machine reading for conversational search","author":["Tom Kenter","Maarten de Rijke"],"venue":"arXiv preprint arXiv:1712.07229","citeRegEx":"23","shortCiteRegEx":"23","year":2017},{"title":"Transfer learning for collaborative filtering via a rating-matrix generative model","author":["Bin Li","Qiang Yang","Xiangyang Xue"],"venue":"In Proceedings of the 26th Annual International Conference on Machine Learning","citeRegEx":"24","shortCiteRegEx":"24","year":2009},{"title":"Towards deep conversational recommendations","author":["Raymond Li","Samira Ebrahimi Kahou","Hannes Schulz","Vincent Michalski","Laurent Charlin","Chris Pal"],"venue":"In Advances in Neural Information Processing Systems","citeRegEx":"25","shortCiteRegEx":"25","year":2018},{"title":"CCCFNet: a content-boosted collaborative filtering neural network for cross domain recommender systems","author":["Jianxun Lian","Fuzheng Zhang","Xing Xie","Guangzhong Sun"],"venue":"In Proceedings of the 26th International Conference onWorld Wide Web companion. International World WideWeb Conferences Steering Committee,","citeRegEx":"26","shortCiteRegEx":"26","year":2017},{"title":"A structured self-attentive sentence embedding","author":["Zhouhan Lin","Minwei Feng","Cicero Nogueira dos Santos","Mo Yu","Bing Xiang","Bowen Zhou","Yoshua Bengio"],"venue":null,"citeRegEx":"27","shortCiteRegEx":"27","year":2017},{"title":"Unifying explicit and implicit feedback for collaborative filtering","author":["Nathan N Liu","Evan W Xiang","Min Zhao","Qiang Yang"],"venue":"In Proceedings of the 19th ACM International Conference on Information and Knowledge Management. ACM,","citeRegEx":"28","shortCiteRegEx":"28","year":2010},{"title":"Cross-domain collaborative filtering with factorization machines","author":["Babak Loni","Yue Shi","Martha Larson","Alan Hanjalic"],"venue":"In European Conference on Information","citeRegEx":"29","shortCiteRegEx":"29","year":2014},{"title":"Cross-stitch networks for multi-task learning","author":["Ishan Misra","Abhinav Shrivastava","Abhinav Gupta","Martial Hebert"],"venue":"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","citeRegEx":"30","shortCiteRegEx":"30","year":2016},{"title":"A Three-Way Model for Collective Learning on Multi-Relational Data","author":["Maximilian Nickel","Volker Tresp","Hans-Peter Kriegel"],"venue":null,"citeRegEx":"31","shortCiteRegEx":"31","year":2011},{"title":"Transfer learning to predict missing ratings via heterogeneous user feedbacks","author":["Weike Pan","Nathan N Liu","EvanWXiang","Qiang Yang"],"venue":"In Twenty-Second International Joint Conference on Artificial Intelligence","citeRegEx":"32","shortCiteRegEx":"32","year":2011},{"title":"Transfer Learning in Collaborative Filtering for Sparsity Reduction","author":["Weike Pan","Evan Wei Xiang","Nathan Nan Liu","Qiang Yang"],"venue":"In Twenty-fourth AAAI Conference on Artificial Intelligence,","citeRegEx":"33","shortCiteRegEx":"33","year":2010},{"title":"Product involvement/brand loyalty: is there a link","author":["Pascale Quester","Ai Lin Lim"],"venue":"Journal of Product & Brand Management 12,","citeRegEx":"34","shortCiteRegEx":"34","year":2003},{"title":"A theoretical framework for conversational search","author":["Filip Radlinski","Nick Craswell"],"venue":"In Proceedings of the 2017 Conference on Human Information Interaction and Retrieval","citeRegEx":"35","shortCiteRegEx":"35","year":2017},{"title":"RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-based Recommendation","author":["Pengjie Ren","Zhumin Chen","Jing Li","Zhaochun Ren","Jun Ma","Maarten de Rijke"],"venue":null,"citeRegEx":"36","shortCiteRegEx":"36","year":2018},{"title":"Autorec: Autoencoders meet collaborative filtering","author":["Suvash Sedhain","Aditya Krishna Menon","Scott Sanner","Lexing Xie"],"venue":"In Proceedings of the 24th International Conference on World Wide Web","citeRegEx":"37","shortCiteRegEx":"37","year":2015},{"title":"Relational learning via collective matrix factorization","author":["Ajit P Singh","Geoffrey J Gordon"],"venue":"In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","citeRegEx":"38","shortCiteRegEx":"38","year":2008},{"title":"End-to-end memory networks","author":["Sainbayar Sukhbaatar","JasonWeston","Rob Fergus"],"venue":"In Advances in Neural Information Processing Systems","citeRegEx":"39","shortCiteRegEx":"39","year":2015},{"title":"Conversational recommender system","author":["Yueming Sun","Yi Zhang"],"venue":"In The 41st International ACM SIGIR Conference on Research &Development in Information Retrieval","citeRegEx":"40","shortCiteRegEx":"40","year":2018},{"title":"Attention is all you need","author":["Ashish Vaswani","Noam Shazeer","Niki Parmar","Jakob Uszkoreit","Llion Jones","Aidan N Gomez","Łukasz Kaiser","Illia Polosukhin"],"venue":"In Advances in neural information processing systems","citeRegEx":"41","shortCiteRegEx":"41","year":2017},{"title":"Representing and Recommending Shopping Baskets with Complementarity, Compatibility and Loyalty","author":["Mengting Wan","Di Wang","Jie Liu","Paul Bennett","Julian McAuley"],"venue":"In Proceedings of the 27th ACM International Conference on Information and Knowledge Management","citeRegEx":"42","shortCiteRegEx":"42","year":2018},{"title":"Double-Digit Growth Expected in the Smart HomeMarket, Says IDC","author":["Adam Wright","Jitesh Ubrani","Michael Shirer"],"venue":"IDC [online; posted 29-March-2018]","citeRegEx":"43","shortCiteRegEx":"43","year":2019},{"title":"Response ranking with deep matching networks and external knowledge in information-seeking conversation systems","author":["Liu Yang","Minghui Qiu","Chen Qu","Jiafeng Guo","Yongfeng Zhang","W Bruce Croft","Jun Huang","Haiqing Chen"],"venue":"In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval","citeRegEx":"44","shortCiteRegEx":"44","year":2018},{"title":"Understanding user interactions with podcast recommendations delivered via voice","author":["Longqi Yang","Michael Sobolev","Christina Tsangouri","Deborah Estrin"],"venue":"In Proceedings of the 12th ACM Conference on Recommender Systems","citeRegEx":"45","shortCiteRegEx":"45","year":2018},{"title":"Deep learning based recommender system: A survey and new perspectives","author":["Shuai Zhang","Lina Yao","Aixin Sun","Yi Tay"],"venue":"Comput. Surveys","citeRegEx":"46","shortCiteRegEx":"46","year":2019},{"title":"Towards conversational search and recommendation: System ask, user respond","author":["Yongfeng Zhang","Xu Chen","Qingyao Ai","Liu Yang","W Bruce Croft"],"venue":"In Proceedings of the 27th ACM International Conference on Information and Knowledge Management","citeRegEx":"47","shortCiteRegEx":"47","year":2018},{"title":"A survey on multi-task learning","author":["Yu Zhang","Qiang Yang"],"venue":"arXiv preprint arXiv:1707.08114","citeRegEx":"48","shortCiteRegEx":"48","year":2017},{"title":"ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation","author":["Chang Zhou","Jinze Bai","Junshuai Song","Xiaofei Liu","Zhengchao Zhao","Xiusi Chen","Jun Gao"],"venue":"arXiv preprint arXiv:1711.06632","citeRegEx":"49","shortCiteRegEx":"49","year":2017}],"referenceMentions":[{"referenceID":40,"context":"3 million shipments of voice-enabled devices last year [43].","startOffset":55,"endOffset":59},{"referenceID":5,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":6,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":8,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":20,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":22,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":41,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":42,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":83,"endOffset":109},{"referenceID":37,"context":", improving the effectiveness of questionanswering between the customer and system [7, 8, 11, 23, 25, 44, 45], relatively little work has been focused on addressing the specific challenges arising in recommendation on Voice [40].","startOffset":224,"endOffset":228},{"referenceID":42,"context":", narrow information channel), customers tend to explore fewer products and choose fewer long-tail products, as compared to Web-based channels [45].","startOffset":143,"endOffset":147},{"referenceID":16,"context":"Despite its obvious potential, transfer learning from Web to Voice is non-trivial due to customers’ distinct shopping behaviors on Voice [19].","startOffset":137,"endOffset":141},{"referenceID":3,"context":"We note that repeated purchase behaviors have also been observed on the Web, which is mainly driven by customers’ loyalty to certain brands [5, 10, 42].","startOffset":140,"endOffset":151},{"referenceID":7,"context":"We note that repeated purchase behaviors have also been observed on the Web, which is mainly driven by customers’ loyalty to certain brands [5, 10, 42].","startOffset":140,"endOffset":151},{"referenceID":39,"context":"We note that repeated purchase behaviors have also been observed on the Web, which is mainly driven by customers’ loyalty to certain brands [5, 10, 42].","startOffset":140,"endOffset":151},{"referenceID":25,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":23,"endOffset":35},{"referenceID":26,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":23,"endOffset":35},{"referenceID":30,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":23,"endOffset":35},{"referenceID":11,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":55,"endOffset":67},{"referenceID":18,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":55,"endOffset":67},{"referenceID":23,"context":", factorization models [28, 29, 33] or neural networks [14, 21, 26], with shared user1 representations for cross-domain recommendation tasks.","startOffset":55,"endOffset":67},{"referenceID":15,"context":"More recent work [18] attempts to address the problem by modeling the transfer of the purchase patterns from one domain to another using a linear transformation.","startOffset":17,"endOffset":21},{"referenceID":38,"context":"Specifically, our method is built on the state-of-the-art self-attention neural architecture [41, 49] to learn customer and product representations.","startOffset":93,"endOffset":101},{"referenceID":46,"context":"Specifically, our method is built on the state-of-the-art self-attention neural architecture [41, 49] to learn customer and product representations.","startOffset":93,"endOffset":101},{"referenceID":13,"context":"To account for customers’ repeated purchase behaviors that are prevalent in voice shopping, TransV adopts a recency-biased copy mechanism, which leverages the copy mechanism [16, 36] and extends it by considering the impact of the recency of historical purchases on repeated purchases.","startOffset":174,"endOffset":182},{"referenceID":33,"context":"To account for customers’ repeated purchase behaviors that are prevalent in voice shopping, TransV adopts a recency-biased copy mechanism, which leverages the copy mechanism [16, 36] and extends it by considering the impact of the recency of historical purchases on repeated purchases.","startOffset":174,"endOffset":182},{"referenceID":32,"context":"The focal point of research efforts has been enabling the system to effectively and efficiently infer users’ intents and satisfy their information needs [35, 47].","startOffset":153,"endOffset":161},{"referenceID":44,"context":"The focal point of research efforts has been enabling the system to effectively and efficiently infer users’ intents and satisfy their information needs [35, 47].","startOffset":153,"endOffset":161},{"referenceID":8,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":128,"endOffset":140},{"referenceID":20,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":128,"endOffset":140},{"referenceID":22,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":128,"endOffset":140},{"referenceID":5,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":169,"endOffset":176},{"referenceID":42,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":169,"endOffset":176},{"referenceID":6,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":236,"endOffset":243},{"referenceID":41,"context":"Due to the complexity of the problem, it has been studied by several research communities including natural language processing [11, 23, 25], human-computer interaction [7, 45], and information retrieval (including recommender systems) [8, 44].","startOffset":236,"endOffset":243},{"referenceID":44,"context":"[47] studies the effect of in-session aspect-based questions for product recommendation using memory networks [39].","startOffset":0,"endOffset":4},{"referenceID":36,"context":"[47] studies the effect of in-session aspect-based questions for product recommendation using memory networks [39].","startOffset":110,"endOffset":114},{"referenceID":22,"context":"[25] introduce a neural dialogue model that classifies the sentiment of a user with respect to movies discussed in the conversation session, and based on that, it generates movie recommendations with a pre-trained autoencoder recommender [37].","startOffset":0,"endOffset":4},{"referenceID":34,"context":"[25] introduce a neural dialogue model that classifies the sentiment of a user with respect to movies discussed in the conversation session, and based on that, it generates movie recommendations with a pre-trained autoencoder recommender [37].","startOffset":238,"endOffset":242},{"referenceID":37,"context":"[40] shows that the integration of users’ past purchasing behaviors boosts the effectiveness of voice-based recommendation.","startOffset":0,"endOffset":4},{"referenceID":4,"context":", user preferences) in a source domain to the task in the target domain [6, 24].","startOffset":72,"endOffset":79},{"referenceID":21,"context":", user preferences) in a source domain to the task in the target domain [6, 24].","startOffset":72,"endOffset":79},{"referenceID":45,"context":"In recommendation, transfer learning is generally implemented through multi-task learning [48], i.","startOffset":90,"endOffset":94},{"referenceID":25,"context":"Early work focuses on adapting matrix factorization techniques for transfer learning [28, 29, 33].","startOffset":85,"endOffset":97},{"referenceID":26,"context":"Early work focuses on adapting matrix factorization techniques for transfer learning [28, 29, 33].","startOffset":85,"endOffset":97},{"referenceID":30,"context":"Early work focuses on adapting matrix factorization techniques for transfer learning [28, 29, 33].","startOffset":85,"endOffset":97},{"referenceID":25,"context":"[28] introduce a model based on collective matrix factorization [38], where user latent factors are shared across different domains.","startOffset":0,"endOffset":4},{"referenceID":35,"context":"[28] introduce a model based on collective matrix factorization [38], where user latent factors are shared across different domains.","startOffset":64,"endOffset":68},{"referenceID":29,"context":"[32, 33] propose to model the interaction patterns in different domains as independent parameter matrices.","startOffset":0,"endOffset":8},{"referenceID":30,"context":"[32, 33] propose to model the interaction patterns in different domains as independent parameter matrices.","startOffset":0,"endOffset":8},{"referenceID":43,"context":"Neural network-basedmethods are more capable of learning nonlinear latent representations for users and items and potentially also their interactions (see a recent survey [46]).","startOffset":171,"endOffset":175},{"referenceID":11,"context":"[14] introduce a multi-view deep learning method that learns shared user representations from user-item interactions in different domains.","startOffset":0,"endOffset":4},{"referenceID":23,"context":"[26] propose to incorporate content information into the multi-view neural network.","startOffset":0,"endOffset":4},{"referenceID":18,"context":"[21] go further in this direction and formulate cross-domain recommendation as extreme multi-class classification, where only content features are used for adapting a classifier trained in the source domain to the target domain.","startOffset":0,"endOffset":4},{"referenceID":15,"context":"A recent paper [18], perhaps the most closely related work to ours, models the transfer of interactions across multiple domains as a linear transformation using cross-stitch networks [30].","startOffset":15,"endOffset":19},{"referenceID":27,"context":"A recent paper [18], perhaps the most closely related work to ours, models the transfer of interactions across multiple domains as a linear transformation using cross-stitch networks [30].","startOffset":183,"endOffset":187},{"referenceID":9,"context":"In the context of Web-to-Voice transfer, since the same products appear in both Web and Voice, we adopt a tri-factorization approach [12, 31] that fixes the product representations across channels, and extend the approach into a multi-level scheme, which allows to capture channel-independent and channel-specific interaction patterns.","startOffset":133,"endOffset":141},{"referenceID":28,"context":"In the context of Web-to-Voice transfer, since the same products appear in both Web and Voice, we adopt a tri-factorization approach [12, 31] that fixes the product representations across channels, and extend the approach into a multi-level scheme, which allows to capture channel-independent and channel-specific interaction patterns.","startOffset":133,"endOffset":141},{"referenceID":17,"context":"Due to the importance for business profitability, repeated purchases are an important customer behavior studied in marketing as a signal of brand loyalty [20, 34].","startOffset":154,"endOffset":162},{"referenceID":31,"context":"Due to the importance for business profitability, repeated purchases are an important customer behavior studied in marketing as a signal of brand loyalty [20, 34].","startOffset":154,"endOffset":162},{"referenceID":0,"context":"In Web-based information systems, repeated item consumption has been shown to be most affected by the recency and quality of the item, with recency being more critical [1].","startOffset":168,"endOffset":171},{"referenceID":2,"context":"[4] study such a behavior in more detail and reveals the increasing inter-arrival gaps of repeated item consumption that eventually lead to abandonment.","startOffset":0,"endOffset":3},{"referenceID":3,"context":"[5] study repeated purchases for consumable products, e.","startOffset":0,"endOffset":3},{"referenceID":39,"context":"[42] introduce a recommendation algorithm, adaLoyal, a personalized grocery recommender.","startOffset":0,"endOffset":4},{"referenceID":33,"context":"[36] propose a unified neural network model that jointly learns repeated and novel consumption for session-based recommendation.","startOffset":0,"endOffset":4},{"referenceID":38,"context":"To learn high-quality customer representations, we leverage the transformer layers [41] that capture the dependency between the purchases based on semantic affinity in the embedding space.","startOffset":83,"endOffset":87},{"referenceID":24,"context":"To do so, we adopt the multi-head self-attention mechanism [27, 41], which allows jointly attending to information from different parts of the purchase sequence.","startOffset":59,"endOffset":67},{"referenceID":38,"context":"To do so, we adopt the multi-head self-attention mechanism [27, 41], which allows jointly attending to information from different parts of the purchase sequence.","startOffset":59,"endOffset":67},{"referenceID":14,"context":"Here we adopt residual connection [17] and layer normalization [2].","startOffset":34,"endOffset":38},{"referenceID":24,"context":"To do so, we apply an attention-based pooling layer [27]:","startOffset":52,"endOffset":56},{"referenceID":28,"context":"A standard approach to model interactions on different domains when entities are shared is matrix tri-factorization [31], which models the interaction scoring function on a specific domain d as:","startOffset":116,"endOffset":120},{"referenceID":13,"context":"To do so, we adopt the copy mechanism [16], which models repeated purchases as copying purchases from the past.","startOffset":38,"endOffset":42},{"referenceID":12,"context":"following state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases.","startOffset":47,"endOffset":51},{"referenceID":19,"context":"following state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases.","startOffset":120,"endOffset":128},{"referenceID":38,"context":"following state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases.","startOffset":120,"endOffset":128},{"referenceID":39,"context":"following state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases.","startOffset":290,"endOffset":294},{"referenceID":33,"context":"following state-of-the-art methods: 1) Bi-LSTM [15]: a deep learning model based on bi-directional LSTM, 2) Transformer [22, 41]: a recommendation model based on Transformer that adopts the self-attention mechanism, 3) adaLoyal Transformer: a variant of Transformer where we apply adaLoyal [42] on top of Transformer for modeling repeated purchases, 4) RepeatNet [36]: an RNNbasedmethod thatmodels repeated purchases using an explorerepeatmode switchwhich integrates a copymechanism that chooses item from historical purchases.","startOffset":363,"endOffset":367},{"referenceID":10,"context":"Model training is performed using Adagrad [13] with mini-batches of size 128.","startOffset":42,"endOffset":46},{"referenceID":1,"context":"All the gradients are clipped between −10 and 10 to prevent exploding [3].","startOffset":70,"endOffset":73}],"year":2020,"abstractText":"While product recommendation algorithms on the Web are wellsupported by a vast amount of interaction data, the same is not true on Voice. A promising approach to mitigate the issue is transfer learning, i.e., transferring the knowledge of customers’ shopping behaviors learned from their shopping activities on the Web to Voice. Such aWeb-to-Voice transfer is challenging due to customers’ distinct shopping behaviors on Voice: customers are inclined to purchase more low-consideration products and are more likely to purchase certain products repeatedly. This paper presents TransV, a novel Web-to-Voice neural transfer network that allows for effective transfer of customers’ shopping patterns from theWeb to Voice, while taking into account customers’ distinct purchase patterns on Voice. Our method extends the state-of-the-art self-attention neural architecture with a multi-level tri-factorization neural component, which allows to explicitly capture the similarity and dissimilarity of customers’ shopping patterns on the Web and Voice. To model repeated purchases, TransV adopts a recency-based copy mechanism that considers the impact of the recency of historical purchases on customers’ behavior of repeated purchases. Extensive validation on multiple real-world datasets, including two cross-platform datasets from Amazon.com and Amazon Alexa, shows that our method is able to improve voice-based recommendation substantially by 26.8% as compared with non-transfer learning methods.","creator":"LaTeX with acmart 2019/05/27 v1.61 Typesetting articles for the Association for Computing Machinery and hyperref 2017/03/14 v6.85a Hypertext links for LaTeX"}}
