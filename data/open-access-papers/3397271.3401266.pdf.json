{
  "name" : "3397271.3401266.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Reranking for Efficient Transformer-based Answer Selection",
    "authors" : [ "Yoshitomo Matsubara", "Thuy Vu", "Alessandro Moschitti" ],
    "emails" : [ "yoshitom@uci.edu", "thuyvu@amazon.com", "amosch@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS CONCEPTS • Information systems→Retrievalmodels and ranking;Question answering.\nKEYWORDS QuestionAnswering, TransformerModels, Neural Networks, Reranking, Information Retrieval, Natural Language Processing\nACM Reference Format: Yoshitomo Matsubara, Thuy Vu, and Alessandro Moschitti. 2020. Reranking for Efficient Transformer-based Answer Selection. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China.ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401266"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "QA research has received a renewed attention in recent years thanks to advent of virtual assistants in industrial sectors. For example, Alexa, Google Home, and Siri provide general information inquiry services. One key task for QA, is the Answer Sentence Selection (AS2), which has been widely studied by the TREC challenges. AS2, given a question and a set of answer sentence candidates, consists in selecting sentences (e.g., retrieved by a search engine) that correctly answer the question. Neural models have significantly contributed with new techniques, e.g., [8, 11] to AS2. More recently, neural language models, e.g., ELMO [13], GPT [14], BERT [5], RoBERTa [10],\n∗This work was done while the author was an intern at Amazon Alexa.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8016-4/20/07. https://doi.org/10.1145/3397271.3401266\nXLNet [3] have led to major advancements in NLP. These methods capture dependencies between words and their compounds by pre-training neural networks on large amounts of data. Unfortunately, the Transformer-based architectures use a huge number of parameters (e.g., 340 million for BERT Large). This poses important challenges for their deployment in production: first, the latency of the approach highly increases with the number of candidates. For instance, to target the required Recall, it might be necessary to classify of hundreds of candidates, which can take several seconds even when using powerful GPUs. Secondly, although the classification of candidates can be scaled horizontally, the number of GPUs required to fulfill a target transaction-per-second will prohibitively increase the operational cost. Finally, their high energy consumption is an environmental threat, as pointed out by [18] and the NeurIPS workshop, Tackling Climate Change with ML.\nIn this paper, we study and propose solutions to improve the efficiency and cost of modern QA systems based on search engines and Transformer models. Though we mainly focus on AS2, the proposed solution is general, and can be applied to other QA paradigms, including machine reading tasks. Our main idea follows the successful cascade approach for ad-hoc document retrieval [19], which considers fast but less accurate rerankers together with more accurate but slower models. In particular, we use (i) simple models, e.g., Jaccard similarity, as well as light neural models such as CompareAggregate [22], for reranking answer sentence candidates; and (ii) BERT models as our final AS2 step.\nTo carry out our experiments, we created three different AS2 datasets, all including a large number of answer sentence candidates. Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended. We tested the combinations of fast rerankers, Jaccard similarity, Rel-CNN1 and CA with an accurate BERT selector, and compared them with the upper bound, obtained with the expensive approach of classifying all candidates with the BERT model. The key finding of our paper is to show that the inference cost of Transformer models, e.g., BERT, can be reduced by selecting only a promising candidate subset to be processed, still preserving the original Accuracy. That is, the candidates selected by shallow neural rerankers are compatible with those selected by Transformer models. This enables the design of accurate, fast and cost-effective QA systems, based on sequential rerankers."
    }, {
      "heading" : "2 RELATEDWORK",
      "text" : "Neural models for AS2 typically apply a series of non-linear transformations to the input question and answer, represented as compositions of word or character embeddings and then measure the 1For lack of space we do not report its result, but it is between Jaccard and CA models.\nsimilarity between the obtained representations. For example, the Rel-CNN [16] has two separate embedding layers for the question and answer, and relational embedding, which aims at connecting them. Recent work has shown that Transformer-based models, e.g., BERT [5], can highly improve inference. Yang et al. [23] applied it to Ad Hoc Document Retrieval, obtaining significant improvement. Garg et al. [8] fine-tuned BERT for AS2, achieving the state of the art. However, BERT’s high computational cost prevents its use in most real-word applications. Some solutions rely on leveraging knowledge distillation in the pre-training step, e.g., [15]. In contrast, we propose an alternative (and compatible with the initiatives above) approach following previous work in document retrieval, e.g., the use of sequential rerankers [19]. Wang et al. [21] focused on quickly identifying a set of good candidate documents to be passed to the second and further rerankers of the cascade. Dang et al. [4] proposed two stage approaches using a limited set of textual features and a final model trained using a larger set of queryand document-dependent features. Gallagher et al. [7] presented a new general framework for learning an end-to-end cascade of rankers using backpropagation. Asadi and Lin [1] studied effectiveness/efficiency trade-offs with three candidate selection approaches. All the methods above are in line with our study, but they target very different settings: document retrieval, linear models or just basic neural models. In contrast, the main contribution of our paper is to show that simple and efficient neural sentence rerankers are compatible with expensive Transformer models, enabling their efficient use."
    }, {
      "heading" : "3 EFFICIENT PASSAGE RERANKING",
      "text" : "In this section, we introduce the most famous neural models for AS2, as well as a state-of-the-art model based on BERT, and then, we present sequential architecture based on fast neural models and more expensive (but more accurate) models."
    }, {
      "heading" : "3.1 Neural rerankers",
      "text" : "A reranker can be simply designed as a model for estimating the probability of correctness of a question and answer sentence pair, \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60). Given a set of questions, \uD835\uDC44 , and a set of sentences, \uD835\uDC46 , we define a reranker as \uD835\uDC45 : \uD835\uDC44 × P(\uD835\uDC46) → P(\uD835\uDC46), which takes a subset Σ of the sentences in \uD835\uDC46 , and returns a subset of size \uD835\uDC58 < |Σ|, i.e., \uD835\uDC45(\uD835\uDC5E, Σ) = (\uD835\uDC60\uD835\uDC561, ..., \uD835\uDC60\uD835\uDC56\uD835\uDC58 ), where \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56ℎ) > \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56\uD835\uDC59 ), if ℎ < \uD835\uDC59 . We use the Compare-Aggregate (CA) model [22], which implements an attention mechanism between the question and the answer candidate with two-step operations, i.e., comparison and aggregation.\nFinally, we study the efficient use of Transformer models, which are neural networks designed to capture dependencies between words, i.e., their interdependent context. The input is encoded in embeddings for tokens, segments and their positions. These are given in input to several blocks (up to 24), containing layers for multi-head attention, normalization and feed forward processing. The result of this transformation is an embedding, \uD835\uDC65 , representing a text pair thatmodels the dependencies betweenwords and segments\nof the two sentences. We train these models for AS2 by feeding \uD835\uDC65 to a fully connected layer, and fine-tuning the Transformer as shown by Devlin et al. [5]. For this step, we used question and sentence pairs labeled as positive or negative, whether or not the sentence correctly answers the question."
    }, {
      "heading" : "3.2 Sequential rerankers",
      "text" : "Given a sequence of rerankers sorted by their computational efficiency, (\uD835\uDC451, \uD835\uDC452,.., \uD835\uDC45\uD835\uDC41 ), we assume that the ranking Accuracy A (e.g., in terms of MAP and MRR), increases in reverse order of the efficiency, i.e., A(\uD835\uDC45 \uD835\uDC57 ) > A(\uD835\uDC45\uD835\uDC56 ) iff \uD835\uDC57 > \uD835\uDC56 .\nWe define a Sequential Reranker of order \uD835\uDC5B, SR\uD835\uDC5B , as the composition of \uD835\uDC5B rerankers: \uD835\uDF0C (\uD835\uDC46) = \uD835\uDC45\uD835\uDC41 ◦ \uD835\uDC45\uD835\uDC41−1 ◦ .. ◦ \uD835\uDC451 (\uD835\uDC46). Each \uD835\uDC45\uD835\uDC56 is associated with a different \uD835\uDC58\uD835\uDC56 = |\uD835\uDC45\uD835\uDC56 (·) |, i.e., the number of sentences the reranker returns (see Fig. 1). Setting different values for \uD835\uDC58\uD835\uDC56 can produce many valuable SRs with different trade-off between Accuracy and efficiency. The design of an end-to-end neural architecture that learns the optimal parameter set for the target trade-off is an interesting future research. In this paper, we focus on a simple SR2 constituted by just two rerankers, SR2 = \uD835\uDC452 ◦ \uD835\uDC451 (\uD835\uDC46), where the first ranker is either a Jaccard similarity-based model or neural rankers such as CA, whereas the second reranker (selector) is the more expensive BERT model, i.e., Base or Large versions."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We show that fast rerankers can effectively feed more expensive Transformer-based selectors, enabling the trade off between Accuracy and efficiency. We compare with state-of-the-art AS2 baselines on two different test sets, created with different question types from Alexa Virtual Assistant traffic. We also built an academic dataset to enable model replicability and comparison with future work."
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "4.1.1 Metrics. The performance for QA systems is typically measured with Accuracy in providing correct answers, i.e., the percentage of correct responses. Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers. We also measure the system latency using the average endto-end processing time for each question, including preprocessing, loading files, etc.2.\n4.1.2 Alexa Datasets. We built training and test sets using questions sampled from Alexa traffic (see statistics in Table 1). We only considered information inquiry questions. These ask about general knowledge, ranging from “Who is the president of United States?” to “How did Los Angeles Lakers score in the match yesterday?”. We generate candidates by selecting sentences from the documents retrieved using an elastic search system, where the referring index\n2Measured on an EC2-instance p3.2xlarge machine (8 Intel Xeon Scalable (Skylake) vCPUs, 62GB RAM, 1 GPU: NVIDIA Tesla V100)\nis ingested with several web domains, ranging from Wikipedia to reference.com, coolantarctica.com, www.cia.gov/library and so on.\nGeneral Purpose Dataset (GPD):We used the following strategy to retrieve correct candidates with high confidence: (i) retrieve top 100 documents; (ii) automatically extract the top 100 sentences ranked by a BERT model over all sentences of the documents; and (iii) manually annotate the top 100 sentences as correct or incorrect answers. This process does not guarantee that we have all correct answers but the chance to miss them is much smaller than for other datasets. Indeed, Table 1 shows an average of seven correct answers for each question. As some of the correct answers can be missed, we consider the GPD evaluation as a strict lower bound (SLB).\nSentence-Based Training Set (SBTS): A training set requires labels for all answer candidates. We limited the rank to just the top 10 sentence per question, to save annotation costs. SBTS consists of 25, 226 questions for a total of 134, 765 candidates, where 125, 779 were labelled as positive and 8, 986 as negative examples.\n4.1.3 Academic Benchmark dataset. Answer Sentence Natural Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario. Thus, we used ASNQ3 by Garg et al. [8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task. ASNQ contains 57,242 and 2,672 distinct questions in the training and test sets, respectively: this is an order of magnitude larger than most public AS2 datasets. In our study, we only use it for testing. Finally, as the number of candidates is still lower than what can typically appear in an industrial scenario, we created ASNQ++ by simply adding negative candidates associated with different questions to reach 1,300 sentences for each question. Table 1 shows some statistics.\n4.1.4 Models. We used the following models: Jaccard Similarity, CA, BERT-Base and BERT-Large to estimate \uD835\uDC5D (\uD835\uDC5E, \uD835\uDC60\uD835\uDC56 ) and used SBTS for training them (except for Jaccard similarity, which is an unsupervised approach). BERT-based models use SBTS for fine-tuning. We trained all the models using the log cross-entropy loss function: L = −∑\uD835\uDC59 ∈{0,1} \uD835\uDC66\uD835\uDC59 × \uD835\uDC59\uD835\uDC5C\uD835\uDC54(\uD835\uDC66\uD835\uDC59 ) on pairs of text. For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, \uD835\uDC59\uD835\uDC5F = 0.0001, and (iii) the batch size of 32. Additionally, we use early stopping to monitor MAP on the validation set. We set the hidden dimensions of the biLSTM in CA to 300. As for BERT, we tuned the learning rate in the interval [1\uD835\uDC52-6, 2\uD835\uDC52-5].\n3ASNQ and ASNQ++ are available at https://github.com/alexa/wqa_tanda\nTable 3: Performance of multi-reranker approach using BERT-Base as the selector derived on GPD\nReranker Jaccard CA BERT #Select. sent. 100 200 250 300 100 all Latency (sec/q) 0.564 0.840 0.977 1.110 0.962 4.590 Prec.@1 0.270 0.376 0.382 0.411 0.450 0.476\nFigure 2: Accuracy of SR using BERT-Large and \uD835\uDC58 sentences selected by the first reranker on ASNQ."
    }, {
      "heading" : "4.2 Results",
      "text" : "We first assess our models on public datasets, then test the individual models as well as the combined models in terms of reranking performance on different datasets. In particular, we compute a SLB and the exact measures on Alexa and ASNQ datasets, respectively.\n4.2.1 Model Assessment. As the datasets for testing the efficiency of the rerankers are new or partially new, to assess where our systems collocate in previous work, we compare our models with the best AS2 systems on standard datasets, WikiQA and TRECQA. Table 2 shows the results derived according to the standard clean setting on the official test sets. We can see that our CA implementation performs slightly lower than models from previous work. This depends on the fact that there is a large variation of the results according to different random seeds. In contrast, our BERT implementation is aligned to the state of the art, thus our comparison using Sequential Rerankers (SRs) is very meaningful.\n4.2.2 Evaluation of SR on GPD. We tested SRs constituted by different models on GPD. Table 3 reports the Accuracy, where: (i) the first row indicates the approach for reranking, either Jaccard or CA, (ii) the second row shows several values for the number of sentences selected by Jaccard, while for CA, we selected a competitive number, i.e., 100, (iii) the third row shows the latency of the SR models including, reranker time and selector applied to the \uD835\uDC58 candidates; and finally, (iv) the last row shows the Precision@1 (i.e., the SLB Accuracy) of SR. The BERT column just shows the upper bound Accuracy, i.e., applying BERT to all candidates.\nWe note that: (i) the latency of the Jaccard-based SR is better than the SR based on CA, when we consider the same number of candidates, e.g., 100, since CA execution time is not negligible, while Jaccard latency is tiny; (ii) the Accuracy of the Jaccard-based SR is half the one based on CA; (iii) also when increasing the number of candidates, the Jaccard-based SR cannot catch up with the Accuracy of CA-based SR using 100 candidates, whereas the latency quickly increases; (iv) the BERT upper bound is just 2.6% better, but the latency is 4.5 seconds, which is prohibitively high; and (v) we did\nnot optimize CA in terms of efficiency and a better trade off to select a more appropriate \uD835\uDC58 can be found. 4.2.3 Results on ASNQ. The evaluation on ASNQ is in principle more accurate as all candidates have been annotated4. Figure 2 shows the plot of three models: the blue line is the BERT-Large (Accuracy when it is applied to all sentence candidates), the green curve is the SR Accuracy when using \uD835\uDC58 sentences from CA, and the orange curve is the Accuracy of SR-based on Jaccard similarity. Again, CA highly outperforms the Jaccard-based model. Interestingly, using 100 sentences or fewer allows for reaching the upper bound. Due to limited space, we cannot show a similar plot for BERT-Base, which reflects the same behaviour of BERT-Large (but with lower Accuracy).\nTo better show the potential of the SR approach, we report a configuration with 100 candidates selected by the first reranker in Table 4. We note that (i) SR based on Jaccard is almost 10 points below the upper bound (all candidates are used), for both selectors, BERT Base and Large; (ii) in contrast, SR based on CA is less than 0.3% below the upper bound. (iii) BERT-Large-based models are around three points better than those based on BERT-Base, as expected. (iv) Finally, there is a clear advantage in terms of latency, but the average number of candidates of ASNQ is lower than in a real setting, thus the speedup is just about 2-3 times for BERT-Base and 7-8 times for BERT-Large.\nTo better appreciate the gain in latency, we tested our models on ASNQ++ dataset, on which we added a larger number of negative examples. Table 5 shows that the latency of BERT-Base increases to 4.23 seconds, while the execution time of SR based on CA is still under 1 second. It is interesting to note that the Accuracy of the upper bound and SR models decreases comparing to the one obtained on ASNQ, e.g., BERT-based obtains 0.571 vs. 0.536 on ASNQ and ASNQ++, respectively. However, SR based on CA still reaches the target upper bound. Although, ASNQ++ is partially artificial, it provides results inline with GPD."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "Very recent research work has shown that powerful neural models such as Transformer models can highly improve retrieval Accuracy. In particular, we show (confirming the results from ad hoc retrieval and our previous work [8]) that BERT can improve passage/sentence reranking in a QA setting. Unfortunately, in a realworld scenario requiring the processing of hundreds or thousands of candidates, this also leads to a latency of 4-5 seconds or more per question, even using an entire GPU. To tackle this problem, we applied SR models by limiting our study to two types of classifiers fast models such as, Jaccard similarity and CA, and slower but more accurate models, i.e., the Transformer (BERT Base and Large). We showed two important findings: (i) Simple neural rerankers such as CA can select candidates containing correct answers BERT would 4Considering that annotation process is always subject to annotator agreement.\nselect. This is important as the data representation of the former is very different from the one of the latter. (ii) SR can be used to parameterize the trade off between efficiency and Accuracy, enabling the use of transformer models for real-world scenarios. Future ideas connected to our work are already in progress, e.g., [2, 17]."
    } ],
    "references" : [ {
      "title" : "Effectiveness/Efficiency Tradeoffs for Candidate Generation in Multi-stage Retrieval Architectures",
      "author" : [ "Nima Asadi", "Jimmy Lin" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "A Study on Efficiency, Accuracy and Document Structure for Answer Sentence Selection",
      "author" : [ "Daniele Bonadiman", "Alessandro Moschitti" ],
      "venue" : "CoRR abs/2003.02349 (2020)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2020
    }, {
      "title" : "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G Carbonell", "Quoc Le", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2019
    }, {
      "title" : "Two-Stage Learning to Rank for Information Retrieval",
      "author" : [ "Van Dang", "Michael Bendersky", "W. Bruce Croft" ],
      "venue" : "ECIR",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2019
    }, {
      "title" : "Natural Questions: A Benchmark for Question Answering Research",
      "author" : [ "Tom Kwiatkowski" ],
      "venue" : "TACL",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2019
    }, {
      "title" : "Joint Optimization of Cascade Ranking Models",
      "author" : [ "Luke Gallagher", "Ruey-Cheng Chen", "Roi Blanco", "J. Shane Culpepper" ],
      "venue" : "InWSDM",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2019
    }, {
      "title" : "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection",
      "author" : [ "Siddhant Garg", "Thuy Vu", "Alessandro Moschitti" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2019
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR abs/1412.6980",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : "CoRR abs/1907.11692",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2019
    }, {
      "title" : "Passage Re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho" ],
      "venue" : "CoRR abs/1901.04085 (2019)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning" ],
      "venue" : "In EMNLP’14",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI Blog 1,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2019
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2019
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks. In SIGIR’15",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "The Cascade Transformer: an Application for Efficient Answer Sentence Selection",
      "author" : [ "Luca Soldaini", "Alessandro Moschitti" ],
      "venue" : "CoRR abs/2005.02534",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2020
    }, {
      "title" : "Energy and Policy Considerations for Deep Learning in NLP",
      "author" : [ "E. Strubell", "A. Ganesh", "A. McCallum" ],
      "venue" : "In ACL",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2019
    }, {
      "title" : "A Cascade Ranking Model for Efficient Ranked Retrieval",
      "author" : [ "Lidan Wang", "Jimmy Lin", "Donald Metzler" ],
      "venue" : "In SIGIR",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "What is the Jeopardy model? A quasi-synchronous grammar for QA",
      "author" : [ "MengqiuWang", "Noah A Smith", "Teruko Mitamura" ],
      "venue" : "In EMNLP-CoNLL",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Fast First-Phase Candidate Generation for Cascading Rankers",
      "author" : [ "Qi Wang", "Constantinos Dimopoulos", "Torsten Suel" ],
      "venue" : "In SIGIR’16",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "A compare-aggregate model for matching text sequences",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "In Fifth International Conference on Learning Representations",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2017
    }, {
      "title" : "Simple Applications of BERT for Ad Hoc Document Retrieval",
      "author" : [ "Wei Yang", "Haotian Zhang", "Jimmy Lin" ],
      "venue" : "CoRR abs/1903.10972 (2019)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2019
    }, {
      "title" : "WikiQA: A Challenge Dataset for Open-Domain Question Answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "A compare-aggregate model with latent clustering for answer selection",
      "author" : [ "Seunghyun Yoon", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Kyomin Jung" ],
      "venue" : "In CIKM. Short Research Papers I SIGIR",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : ", ELMO [13], GPT [14], BERT [5], RoBERTa [10],",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : ", ELMO [13], GPT [14], BERT [5], RoBERTa [10],",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : ", ELMO [13], GPT [14], BERT [5], RoBERTa [10],",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "3401266 XLNet [3] have led to major advancements in NLP.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "Finally, their high energy consumption is an environmental threat, as pointed out by [18] and the NeurIPS workshop, Tackling Climate Change with ML.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Our main idea follows the successful cascade approach for ad-hoc document retrieval [19], which considers fast but less accurate rerankers together with more accurate but slower models.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : ", Jaccard similarity, as well as light neural models such as CompareAggregate [22], for reranking answer sentence candidates; and (ii) BERT models as our final AS2 step.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "Two of them are built using different samples of the anonymized questions fromAlexa Information Traffic, while the third, ASNQ [8], is a sample from the Google Natural Question dataset [6], adapted for the AS2 task, which we further extended.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "For example, the Rel-CNN [16] has two separate embedding layers for the question and answer, and relational embedding, which aims at connecting them.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : ", BERT [5], can highly improve inference.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 21,
      "context" : "[23] applied it to Ad Hoc Document Retrieval, obtaining significant improvement.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8] fine-tuned BERT for AS2, achieving the state of the art.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : ", the use of sequential rerankers [19].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "[21] focused on quickly identifying a set of good candidate documents to be passed to the second and further rerankers of the cascade.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] proposed two stage approaches using a limited set of textual features and a final model trained using a larger set of queryand document-dependent features.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] presented a new general framework for learning an end-to-end cascade of rankers using backpropagation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Asadi and Lin [1] studied effectiveness/efficiency trade-offs with three candidate selection approaches.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "We use the Compare-Aggregate (CA) model [22], which implements an attention mechanism between the question and the answer candidate with two-step operations, i.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "Questions (ASNQ): Famous benchmarks for AS2 such as TRECQA [20] andWikiQA [24] do not comewith an large enough number of annotated candidates to simulate a real application scenario.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "[8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[8], who transformed Google Natural Question (NQ) benchmark [6] for the AS2 task.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, lr = 0.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "For CA, we applied the same design choices from the original papers except for (i) the use of 300 dimensional Glove embeddings [12], (ii) the Adam optimizer [9] with a learning rate, lr = 0.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "In particular, we show (confirming the results from ad hoc retrieval and our previous work [8]) that BERT can improve passage/sentence reranking in a QA setting.",
      "startOffset" : 91,
      "endOffset" : 94
    } ],
    "year" : 2020,
    "abstractText" : "IR-based Question Answering (QA) systems typically use a sentence selector to extract the answer from retrieved documents. Recent studies have shown that powerful neural models based on the Transformer can provide an accurate solution to Answer Sentence Selection (AS2). Unfortunately, their computation cost prevents their use in real-world applications. In this paper, we show that standard and efficient neural rerankers can be used to reduce the amount of sentence candidates fed to Transformer models without hurting Accuracy, thus improving efficiency up to four times. This is an important finding as the internal representation of shallower neural models is dramatically different from the one used by a Transformer model, e.g., word vs. contextual embeddings.",
    "creator" : "LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2019/11/10 v7.00c Hypertext links for LaTeX"
  }
}