{
  "name" : "3397271.3401467.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Reinforcement Learning for Information Retrieval:Fundamentals and Advances",
    "authors" : [ "Weinan Zhang", "Shanghai Jiao Tong", "Xiangyu Zhao", "Li Zhao", "Dawei Yin", "Grace Hui Yang", "Alex Beutel" ],
    "emails" : [ "wnzhang@sjtu.edu.cn", "zhaoxi35@msu.edu", "lizo@microsoft.com", "yindawei@acm.org", "huiyang@cs.georgetown.edu", "alexbeutel@google.com", "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Deep Reinforcement Learning, Information Retrieval\nACM Reference Format: Weinan Zhang, Xiangyu Zhao, Li Zhao, Dawei Yin, Grace Hui Yang, and Alex Beutel. 2020. Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401467"
    }, {
      "heading" : "1 BACKGROUND AND MOTIVATIONS",
      "text" : "Recent years have witnessed the increased popularity and explosive growth of the World Wide Web, which has generated huge amounts of data and leaded to progressively severe information\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 https://doi.org/10.1145/3397271.3401467\noverload problem [4]. In consequence, how to extract information (products or services) that satisfy users’ information requirements at the proper time and place has become increasingly important. This motivates several information retrieval mechanisms, such as search, recommendation, and online advertising. These retrieval mechanisms could generate a set of objects that best match users’ explicit or implicit preferences [10]. Efforts have been made on developing supervised or unsupervised methods for these information retrieval mechanisms [21]. However, since the widely use of mobile applications during the recent years, more and more information retrieval services have provided interactive functionality and products [41], these conventional techniques typically face several common challenges. First, most of traditional approaches consider information retrieval tasks in a static environment and extract information via a fixed greedy strategy, whichmay fail to catch the dynamic characteristics of users’ preferences (or environment). Second, the majority of existing methods aims to maximize user’s immediate satisfaction, while completely overlooking whether the generate information can benefit more to user’s preference in the long run [24]. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning [27].\nDriven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14]. By integrating deep learning into reinforcement learning, DRL is not only capable of continuing sensing and learning to act, but also capturing complex patterns with the power of deep learning. Under the DRL schema, complex problems are addressed by acquiring experiences through interactions with a dynamic environment. The result is an optimal policy that can provide decision making solutions to complex tasks without any specific instructions [13]. Introducing DRL to information retrieval community can naturally tackle the above-mentioned challenges. First, DRL considers information retrieval tasks as sequential interactions between an RL agent (system) and users (environment), where the agent continuously update the information retrieval strategies based on users’ real-time feedback, so as to generate information best match users’ dynamic preferences. Second, the DRL-based techniques targets to optimize users’ long-term satisfaction or engagement. Therefore, the agent could identify information to achieve the trade-off between users’ short-term and long-term satisfaction.\nGiven the advantages of reinforcement learning, there have been tremendous interests in developing RL based information retrieval techniques [3, 5, 11, 12, 15, 38–40]. While these successes show the promise of DRL, applying learning from game-based DRL to information retrieval is fraught with unique challenges, including, but not limited to, extreme data sparsity, power-law distributed samples, and large state and action spaces.\nTherefore, in this workshop, we will provide a venue for both academia researchers and industry practitioners to discuss the fundamental principles, technical and practice limitations, and observations and lessons learned from applications of DRL for (interactive) information retrieval. We also aim to foster research on novel information retrieval algorithms, techniques and applications of DRL."
    }, {
      "heading" : "2 REVIEW OF EXISTINGWORK",
      "text" : "In this section, we briefly review related work of deep reinforcement learning in main IR scenarios i.e., search, recommendation and online advertising.\nSearch targets at retrieving and ranking a set of items (e.g. documents, records) according to a user query [7, 29]. For query understanding, Nogueira et al. [19] proposed a reinforcement learning based query reformulation task, which maximizes the number of recalled relevant documents via rewriting a query. In [18], a multiagent reinforcement learning framework is proposed to increase the diverse query reformulation efficiency, where each agent learns a local policy that performs well on a subset of examples, and all agents are trained with parallelism to make the learning faster. For relevance ranking, conventional methods typically optimize the evaluation metric before a predefined position (e.g. NDCG@K), which ignores the information after rank K . MDPRank [31] is proposed to address this problem by using the metrics calculated upon all the positions as reward function, and the model parameters are be optimized via maximizing the accumulated rewards for all decisions. Beyond relevance ranking, another important goal is to increase the diversity of search results [8, 22], which needs to capture the utility of information users have perceived from the preceding documents in a sequential document selection. MDPDIV [34] formalized diverse ranking as a continuous state Markov decision process, and policy gradient algorithm of REINFORCE is leveraged to maximize the accumulated long-term rewards in terms of the diversity metric.\nRecommender systems aim to learn users’ preferences based on their feedback and suggest items to match their preferences. User’s preference is assumed to be static in traditional recommendation algorithms such as collaborative filtering, which is usually not true in real-world recommender systems where users’ preferences are highly dynamic. Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions). Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over time. In [39], a user’s dynamic preference (state) is learned from her browsing history and feedback. Each time a user provides feedback (skip, click or purchase) to an item, the recommender systemwill update the state to capture user’s new preferences. Conventional recommender algorithms also suffer from the exploitation-exploration\ndilemma, where exploitation is to suggest items that best match users’ preferences, while exploration is to randomly suggest items to mine more users’ possible preferences. The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ϵ-greedy [30], EXP3 [2], and UCB1 [1].\nOnline advertising is to suggest the right ads to the right users so as to maximize the click-through rate (CTR) or return on investment (ROI) of the advertising campaign, which consists of two main marketing strategy, i.e., guaranteed delivery (GD) and real-time bidding (RTB). In guaranteed delivery setting, ads that grouped into campaigns are charged on a pay-per-campaign basis for the pre-specified number of deliveries [23]. A multi-agent reinforcement learning approach [32] is proposed to derive cooperative policies for the publisher, where impression allocation problem is formulated as an auction problem, and publishers can submit virtual bids for impressions. In Real-Time Bidding setting, an advertiser submits a bid for each impression in a very short time frame. The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out. Thus, the MDP setting is introduced. For example, a model-based RL framework is proposed in RTB setting [3], where the state value is approximated by neural network to address the scalability problem of large auction amounts and the limited budget. In [12], a multi-agent bidding model is proposed to jointly consider all the advertisers’ biddings in the system, and a clustering approach is introduced to deal with a large number of advertisers."
    }, {
      "heading" : "3 KEY CHALLENGES AND OPEN QUESTIONS",
      "text" : "While there has been a plenty of research work on deep reinforcement learning for information retrieval published during the recent years, several problems are still unsolved or remain as key challenges. Here we list some of them for broad discussion.\nSample Efficiency. Sample efficiency has been of key problem for DRL since most DRL methods are model-free. Given sufficient training data from the agent interacting with the environment, the paradigm of model-free RL is suitable for training deep neural network based value functions or policies. However, most IR systems directly interact with the users and collect the training data, which is normally insufficient to train the model-free RL solutions.\nSparse & Biased Feedback Data. Feedback data is commonly biased. In RL view, the experience data is sampled from the occupancy measure distribution of the policy-environment interaction. Although off-policy training method can still help improve the policy based on biased data, the sample efficiency is seriously reduced. Moreover, in some IR scenarios such as online advertising, the positive reward is highly sparse (e.g., 0.3% click-through rate for display ads), which results in very low efficiency or failure of RL.\nOnline Deployment. From industry perspective, deploying DRL solution onto production IR platform is challenging. The common model pipelines in IR platforms center on relevance estimation models, e.g., relevance or CTR estimation, while the DRL model pipelines center on the policy module. Bridging gap between two\ngenerations of model pipelines should be positioned as high priority for applied research team in this field."
    }, {
      "heading" : "4 PROGRAM SKETCH",
      "text" : ""
    }, {
      "heading" : "4.1 Workshop Format",
      "text" : "The workshop is planned to be host a whole day, with 2 keynotes, 4 invited talks and 6 oral research talks. The keynote speakers should be well-recognized professors or scientists working on the area. There are two encouraged types of invited talks and peer reviewed oral research talks: (i) the academic talk on fundamental research on reinforcement learning with an attempt of application on IR; (ii) the industrial talk on practice of designing or applying deep reinforcement learning techniques for real-world IR tasks.\nEach talk is expected to be presented as a lecture with slides. There will be a QA session at the end of each talk."
    }, {
      "heading" : "4.2 Online Materials",
      "text" : "A website (http://drl4ir.github.io) for this workshop will be made available online right before the lecture is presented. All the relevant materials will be made available on this website, including the talk information, presentation slides, referred papers, speaker information and related open source projects etc."
    }, {
      "heading" : "5 RELATEDWORKSHOPS",
      "text" : "The Deep Reinforcement Learning Workshop at NeurIPS (2015- 2019)1 and IJCAI (2016)2 focused on the techniques to combine neural networks with reinforcement learning, and domains like robotics, strategy games, and multi-agent interaction. The Deep Reinforcement Learning Meets Structured Prediction at ICLR (2019)3 focused on leveraging reinforcement learning paradigm on tasks of structured predictions. The workshops at ICML (2019)4 and KDD (2019)5 focus on a wide range of real life reinforcement learning applications. The proposedworkshop is the first to focus on deep reinforcement learning for information retrieval. This workshop will bring together experts in information retrieval and reinforcement learning. Our proposed workshop will have invited keynotes and talks, paper presentation, poster session, and panel discussion to help interested researchers gain a high-level view about the current state of the art and potential directions for future contributions. Real datasets and codes will also be released for attendees to practice in the future."
    }, {
      "heading" : "6 ORGANIZERS INFORMATION AND QUALIFICATION",
      "text" : "Dr. Weinan Zhang, the workshop lead organizer, is currently a tenure-track associate professor in Shanghai Jiao Tong University. His research interests include machine learning and big data mining, particularly, deep learning and reinforcement learning techniques for real-world data mining scenarios, such as computational advertising, recommender systems, text mining, web search and\n1https://sites.google.com/view/deep-rl-workshop-neurips-2019/home 2https://sites.google.com/site/deeprlijcai16/ 3https://sites.google.com/view/iclr2019-drlstructpred/ 4https://sites.google.com/view/RL4RealLife 5http://www.cse.msu.edu/~zhaoxi35/DRL4KDD/\nknowledge graphs. He has published over 80 papers on first-tier international conferences and journals, including KDD, SIGIR, ICML, ICLR, JMLR, IJCAI, AAAI,WSDM, CIKM etc. He won the Best Paper Honorable Mention Award in SIGIR 2017, the Best Paper Award in DLP Workshop in KDD 2019, ACM Rising Star Award, Alibaba DAMO Young Scholar Award etc. Weinan has organized workshops and tutorials in SIGIR, KDD, CIKM and ECIR etc.\nXiangyu Zhao is a senior Ph.D. student of computer science and engineering at Michigan State University (MSU). His supervisor is Dr. Jiliang Tang. Before joining MSU, he completed his MS (2017) at USTC and BS (2014) at UESTC. He is the student member of IEEE, SIGIR, and SIAM. His current research interests include data mining and machine learning, especially (1) Reinforcement Learning and AutoML for E-commerce; (2) Urban Computing and Spatio-Temporal Data Analysis. After joiningMSU, he has published his work in top journals (e.g. SIGKDD, SIGWeb) and conferences (e.g., KDD, SIGIR, CIKM, ICDM, RecSys). He was the recipients of the KDD’18/19, RecSys’18, SDM’18, and CIKM’17 Student Travel Award.\nDr. Li Zhao is currently a Senior Researcher in Machine Learning Group, Microsoft Research Asia (MSRA). Her research interests mainly lie in deep learning and reinforcement learning, and their applications for text mining, recommendation, finance and games. She has co-organized the 3rd Asian Workshop on Reinforcement Learning (AWRL’18), and is one of the invited speakers for AWRL’19. She obtained her Ph.D. degree majoring in Computer Science in July, 2016, from Tsinghua University, supervised by Professor Xiaoyan Zhu. During her Ph.D. studies, she has conducted research on sentiment extraction, text mining and weakly supervised learning. She published several research papers in top conferences, including NeurIPS, KDD, IJCAI, AAAI, EMNLP and CIKM.\nDr. Dawei Yin is Engineering Director at Baidu inc.. He is managing the search science team at Baidu, leading BaiduâĂŹs science efforts of web search, question answering, video search, image search, news search, app search, etc.. Previously, he was Senior Director, managing the recommendation engineering team at JD.com between 2016 and 2020. Prior to JD.com, he was Senior Research Manager at Yahoo Labs, leading relevance science team and in charge of Core Search Relevance of Yahoo Search. He obtained Ph.D. (2013), M.S. (2010) from Lehigh University and B.S. (2006) from Shandong University. From 2007 to 2008, he was an M.Phil. student in The University of Hong Kong. His research interests include data mining, applied machine learning, information retrieval and recommender system. He published more than 80 research papers in premium conferences and journals, and was the recipients of WSDM2016 Best Paper Award, KDD2016 Best Paper Award, WSDM2018 Best Student Paper Award, and ICHI 2019 Best Paper Honorable Mention.\nDr. Grace Hui Yang is an Associate Professor in the Department of Computer Science at Georgetown University. Dr. Yang is leading the InfoSense (Information Retrieval and Sense-Making) group at Georgetown University, Washington D.C., U.S.A. Dr. Yang obtained her Ph.D. from the Language Technologies Institute, Carnegie Mellon University in 2011. Dr. Yang’s current research interests include deep reinforcement learning, dynamic information retrieval, search\nengine evaluation, privacy-preserving information retrieval, internet of things, and information organization. Prior to this, she has conducted research on question answering, ontology construction, near-duplicate detection, multimedia information retrieval, and opinion and sentiment detection. Dr. Yang has co-chaired SIGIR 2013 and 2014 Doctoral Consortiums, SIGIR 2017Workshop,WSDM 2017 Workshop, ICTIR 2017 Workshop, CIKM 2015 Tutorial, ICTIR 2018 Short Paper and SIGIR 2018 Demonstration Paper Program Committees. Dr. Yang served on the editorial board of Information Retrieval Journal from 2014 to 2017.\nDr. Alex Beutel is a Staff Research Scientist in Google Brain SIR, leading a team working on responsible and fair ML, as well as researching neural recommendation and ML for Systems. He received his Ph.D. in 2016 from Carnegie Mellon University’s Computer Science Department, and previously received his B.S. from Duke University in computer science and physics. His Ph.D. thesis on large-scale user behavior modeling, covering recommender systems, fraud detection, and scalable machine learning, was given the SIGKDD 2017 Doctoral Dissertation Award Runner-Up. He also received the Best Paper Award at KDD 2016 and ACM GIS 2010, was a finalist for best paper in KDD 2014 and ASONAM 2012, and was awarded the Facebook Fellowship in 2013 and the NSF Graduate Research Fellowship in 2011. More details can be found at alexbeutel.com.\nAcknowledgement. Weinan Zhang is supported by \"New Generation of AI 2030\" Major Project (2018AAA0100900) and NSFC (61632017, 61702327, 61772333)."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning 47,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire" ],
      "venue" : "SIAM journal on computing 32,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Real-time bidding by reinforcement learning in display advertising",
      "author" : [ "Han Cai", "Kan Ren", "Weinan Zhang", "Kleanthis Malialis", "Jun Wang", "Yong Yu", "Defeng Guo" ],
      "venue" : "WSDM",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "A survey of web information extraction systems",
      "author" : [ "Chia-Hui Chang", "Mohammed Kayed", "Moheb R Girgis", "Khaled F Shaalan" ],
      "venue" : "TKDE 18,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Large-scale interactive recommendation with tree-structured policy gradient",
      "author" : [ "Haokun Chen", "Xinyi Dai", "Han Cai", "Weinan Zhang", "Xuejian Wang", "Ruiming Tang", "Yuzhou Zhang", "Yong Yu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2019
    }, {
      "title" : "Top-K Off-Policy Correction for a REINFORCE Recommender System",
      "author" : [ "Minmin Chen", "Alex Beutel", "Paul Covington", "Sagar Jain", "Francois Belletti", "Ed H Chi" ],
      "venue" : "In WSDM",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2019
    }, {
      "title" : "Query representation and understanding workshop",
      "author" : [ "W Bruce Croft", "Michael Bendersky", "Hang Li", "Gu Xu" ],
      "venue" : "In ACM SIGIR Forum,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Search result diversification",
      "author" : [ "Marina Drosou", "Evaggelia Pitoura" ],
      "venue" : "ACM SIGMOD Record 39,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning for relation classification from noisy data",
      "author" : [ "Jun Feng", "Minlie Huang", "Li Zhao", "Yang Yang", "Xiaoyan Zhu" ],
      "venue" : "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2018
    }, {
      "title" : "Information seeking: convergence of search, recommendations, and advertising",
      "author" : [ "Hector Garcia-Molina", "Georgia Koutrika", "Aditya Parameswaran" ],
      "venue" : "Commun. ACM 54,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement Learning",
      "author" : [ "Li He", "Liang Wang", "Kaipeng Liu", "Bo Wu", "Weinan Zhang" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2018
    }, {
      "title" : "Real-time bidding with multi-agent reinforcement learning in display advertising",
      "author" : [ "Junqi Jin", "Chengru Song", "Han Li", "Kun Gai", "Jun Wang", "Weinan Zhang" ],
      "venue" : "In CIKM",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2018
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore" ],
      "venue" : "Journal of artificial intelligence research",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1996
    }, {
      "title" : "Socially compliant mobile robot navigation via inverse reinforcement learning",
      "author" : [ "Henrik Kretzschmar", "Markus Spies", "Christoph Sprunk", "Wolfram Burgard" ],
      "venue" : "The International Journal of Robotics Research 35,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning based recommendation with explicit user-item interactions modeling",
      "author" : [ "Feng Liu", "Ruiming Tang", "Xutao Li", "Weinan Zhang", "Yunming Ye", "Haokun Chen", "Huifeng Guo", "Yuzhou Zhang" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2018
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In ICML",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Learning to coordinate multiple reinforcement learning agents for diverse query reformulation",
      "author" : [ "Rodrigo Nogueira", "Jannis Bulian", "Massimiliano Ciaramita" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2018
    }, {
      "title" : "Task-oriented query reformulation with reinforcement learning",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho" ],
      "venue" : "arXiv preprint arXiv:1704.04572",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2017
    }, {
      "title" : "A combinatorial-bandit algorithm for the online joint bid/budget optimization of pay-per-click advertising campaigns",
      "author" : [ "Alessandro Nuara", "Francesco Trovo", "Nicola Gatti", "Marcello Restelli" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2018
    }, {
      "title" : "Product-based neural networks for user response prediction over multi-field categorical data",
      "author" : [ "Yanru Qu", "Bohui Fang", "Weinan Zhang", "Ruiming Tang", "Minzhe Niu", "Huifeng Guo", "Yong Yu", "Xiuqiang He" ],
      "venue" : "TOIS 37,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2018
    }, {
      "title" : "A unified optimization framework for auction and guaranteed delivery in online advertising",
      "author" : [ "Konstantin Salomatin", "Tie-Yan Liu", "Yiming Yang" ],
      "venue" : "In CIKM",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "An MDP-based recommender system",
      "author" : [ "Guy Shani", "David Heckerman", "Ronen I Brafman" ],
      "venue" : "JMLR 6,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Mastering the game of Go without human knowledge",
      "author" : [ "David Silver", "Julian Schrittwieser", "Karen Simonyan", "Ioannis Antonoglou", "Aja Huang", "Arthur Guez", "Thomas Hubert", "Lucas Baker", "Matthew Lai", "Adrian Bolton" ],
      "venue" : "Nature 550,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2017
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2018
    }, {
      "title" : "Automatic ad format selection via contextual bandits",
      "author" : [ "Liang Tang", "Romer Rosales", "Ajit Singh", "Deepak Agarwal" ],
      "venue" : "In CIKM",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "A Reinforcement Learning Approach for Dynamic Search",
      "author" : [ "Zhiwen Tang", "Grace Hui Yang" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ ],
      "venue" : "Ph.D. Dissertation. King’s College,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1989
    }, {
      "title" : "Reinforcement learning to rank with Markov decision process",
      "author" : [ "Zeng Wei", "Jun Xu", "Yanyan Lan", "Jiafeng Guo", "Xueqi Cheng" ],
      "venue" : "In SIGIR",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2017
    }, {
      "title" : "A multi-agent reinforcement learning method for impression allocation in online display advertising",
      "author" : [ "Di Wu", "Cheng Chen", "Xun Yang", "Xiujun Chen", "Qing Tan", "Jian Xu", "Kun Gai" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2018
    }, {
      "title" : "Learning contextual bandits in a non-stationary environment",
      "author" : [ "Qingyun Wu", "Naveen Iyer", "Hongning Wang" ],
      "venue" : "In SIGIR",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2018
    }, {
      "title" : "Adapting Markov decision process for search result diversification",
      "author" : [ "Long Xia", "Jun Xu", "Yanyan Lan", "Jiafeng Guo", "Wei Zeng", "Xueqi Cheng" ],
      "venue" : "In SIGIR",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2017
    }, {
      "title" : "Estimation bias in multi-armed bandit algorithms for search advertising",
      "author" : [ "Min Xu", "Tao Qin", "Tie-Yan Liu" ],
      "venue" : "In NIPS",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "Dynamic contextual multi arm bandits in display advertisement",
      "author" : [ "Hongxia Yang", "Quan Lu" ],
      "venue" : "In ICDM",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Online context-aware recommendation with time varying multi-armed bandit",
      "author" : [ "Chunqiu Zeng", "Qing Wang", "Shekoofeh Mokhtari", "Tao Li" ],
      "venue" : "In KDD",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Deep Reinforcement Learning for Page-wise Recommendations",
      "author" : [ "Xiangyu Zhao", "Long Xia", "Liang Zhang", "Zhuoye Ding", "Dawei Yin", "Jiliang Tang" ],
      "venue" : "In Proceedings of the 12th ACM Recommender Systems Conference",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2018
    }, {
      "title" : "Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning",
      "author" : [ "Xiangyu Zhao", "Liang Zhang", "Zhuoye Ding", "Long Xia", "Jiliang Tang", "Dawei Yin" ],
      "venue" : "In KDD",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2018
    }, {
      "title" : "Deep Reinforcement Learning for List-wise Recommendations",
      "author" : [ "Xiangyu Zhao", "Liang Zhang", "Zhuoye Ding", "Dawei Yin", "Yihong Zhao", "Jiliang Tang" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2017
    }, {
      "title" : "Interactive collaborative filtering",
      "author" : [ "Xiaoxue Zhao", "Weinan Zhang", "Jun Wang" ],
      "venue" : "In CIKM",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2013
    }, {
      "title" : "Jointly Learning to Recommend and Advertise",
      "author" : [ "Xiangyu Zhao", "Xudong Zheng", "Xiwang Yang", "Xiaobing Liu", "Jiliang Tang" ],
      "venue" : "arXiv preprint arXiv:2003.00097",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "These retrieval mechanisms could generate a set of objects that best match users’ explicit or implicit preferences [10].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "Efforts have been made on developing supervised or unsupervised methods for these information retrieval mechanisms [21].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "However, since the widely use of mobile applications during the recent years, more and more information retrieval services have provided interactive functionality and products [41], these conventional techniques typically face several common challenges.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "Second, the majority of existing methods aims to maximize user’s immediate satisfaction, while completely overlooking whether the generate information can benefit more to user’s preference in the long run [24].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 25,
      "context" : "Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning [27].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].",
      "startOffset" : 238,
      "endOffset" : 246
    }, {
      "referenceID" : 24,
      "context" : "Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].",
      "startOffset" : 238,
      "endOffset" : 246
    }, {
      "referenceID" : 15,
      "context" : "Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].",
      "startOffset" : 260,
      "endOffset" : 268
    }, {
      "referenceID" : 16,
      "context" : "Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].",
      "startOffset" : 260,
      "endOffset" : 268
    }, {
      "referenceID" : 13,
      "context" : "Driven by recent advances in reinforcement learning theories and the prevalence of deep learning technologies, there has been tremendous interest in resolving complex problems by deep reinforcement leaning methods, such as the game of Go [25, 26], video games [16, 17], and robotics [14].",
      "startOffset" : 283,
      "endOffset" : 287
    }, {
      "referenceID" : 6,
      "context" : "documents, records) according to a user query [7, 29].",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 27,
      "context" : "documents, records) according to a user query [7, 29].",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : "[19] proposed a reinforcement learning based query reformulation task, which maximizes the number of recalled relevant documents via rewriting a query.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "In [18], a multiagent reinforcement learning framework is proposed to increase the diverse query reformulation efficiency, where each agent learns a local policy that performs well on a subset of examples, and all agents are trained with parallelism to make the learning faster.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 29,
      "context" : "MDPRank [31] is proposed to address this problem by using the metrics calculated upon all the positions as reward function, and the model parameters are be optimized via maximizing the accumulated rewards for all decisions.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "Beyond relevance ranking, another important goal is to increase the diversity of search results [8, 22], which needs to capture the utility of information users have perceived from the preceding documents in a sequential document selection.",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "MDPDIV [34] formalized diverse ranking as a continuous state Markov decision process, and policy gradient algorithm of REINFORCE is leveraged to maximize the accumulated long-term rewards in terms of the diversity metric.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 31,
      "context" : "Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions).",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "Bandit methods [33, 37] usually utilizes a variable reward function to delineate the dynamic nature of the environment (reward distributions).",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over",
      "startOffset" : 49,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over",
      "startOffset" : 49,
      "endOffset" : 59
    }, {
      "referenceID" : 40,
      "context" : "Another solution is to introduce the MDP setting [6, 9, 42], where state represents user’s preference and state transition depicts the dynamic nature of user’s preference over",
      "startOffset" : 49,
      "endOffset" : 59
    }, {
      "referenceID" : 37,
      "context" : "In [39], a user’s dynamic preference (state) is learned from her browsing history and feedback.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "The contextual bandit method is introduced to achieve the trade-off between exploitation and exploration with strategies such as ε-greedy [30], EXP3 [2], and UCB1 [1].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "In guaranteed delivery setting, ads that grouped into campaigns are charged on a pay-per-campaign basis for the pre-specified number of deliveries [23].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 30,
      "context" : "A multi-agent reinforcement learning approach [32] is proposed to derive cooperative policies for the publisher, where impression allocation problem is formulated as an auction problem, and publishers can submit",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : "The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : "The ad selection task is typically modeled as multi-armed bandit (MAB) problem [20, 28, 35, 36], which neglects the fact that bidding actions would continuously occur before the budget running out.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "For example, a model-based RL framework is proposed in RTB setting [3], where the state value is approximated by neural network to address the scalability problem of large auction amounts and the limited budget.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "In [12], a multi-agent bidding model is proposed to jointly consider all the advertisers’ biddings in the system, and a clustering approach is introduced to deal with a large number of advertisers.",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2020,
    "abstractText" : "Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users’ information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users’ real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.",
    "creator" : "LaTeX with acmart 2018/01/24 v1.49 Typesetting articles for the Association for Computing Machinery and hyperref 2017/03/14 v6.85a Hypertext links for LaTeX"
  }
}