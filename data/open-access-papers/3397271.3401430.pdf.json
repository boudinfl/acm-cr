{
  "name" : "3397271.3401430.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval",
    "authors" : [ "Dehong Gao", "Linbo Jin", "Ben Chen", "Minghui Qiu", "Peng Li", "Yi Wei", "Yi Hu", "Hao Wang" ],
    "emails" : [ "longran.wh}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS CONCEPTS • Information System • Information Retrieval • Specialized Information Retrieval • Multimedia and Multimodal Retrieval\nKEYWORDS FashionBERT, Text and Image matching, Cross-modal retrieval\nACM Reference format:\nDehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu and Hao Wang. 2020. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’20), July 25-30, Virtual Event, China. ACM, New York, NY, USA, 10 pages. hps://doi.org/10.1145/3397271.3401430"
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the last decade, a great number of multimedia data (including image, video, audio, and text) have been emerged on the internet. To search important information from these multimodal data efficiently, multimedia retrieval is becoming an essential technique and widely researched by world-wide researchers. Recently, it has been witnessed a soar increase of the research interest in cross-modal retrieval, which takes one type of data as the query and retrieves relevant data of another type. The pivot of cross-modal retrieval is to learn a meaningful cross-modal matching [40].\nThere exists a long research line in cross-modal matching, especially in text and image matching. The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41]. Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46]. Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34]. The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2]. However, these studies are centered on text and image matching of the general domain. In this paper, we focus on the text and image matching of the fashion industry1, which is mainly referred to clothing, footwear, accessories, makeup and etc.\n1 https://en.wikipedia.org/wiki/Fashion\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR’20, July 25-30, 2020, Virtual Event, China. © 2020 Association of Computing Machinery. ACM ISBN 978-1-4503-8016-4/20/07 $15.00. DOI: https://doi.org/10.1145/3397271.3401430\nThe main challenge of these pioneer matching approaches is how to extract the semantic information from images, and integrate this information into the BERT model. All current approaches detect RoIs2 (i.e., Region of Interest [13]) from images as seen in Figure 1(a) and treat these RoIs as “image tokens”. But this RoI method does not work well in the fashion domain since relatively-rare RoIs can be detected from fashion images. As seen in Figure 1(b), we show the detected RoIs of Fashion-Gen images of different categories, where the minimum number of detected RoIs is set to one from an image. We found on average 19.8 RoIs can be detected from one MSCOCO3 image, but only 6.4 can be detected from one Fashion-Gen4 image. This is because in general a fashion image contains only one or two objects (e.g., a coat and/or a pant) with a flat background. We can set the minimum RoI number to detect, but under this setting lots of detected RoIs are repeated since they only focus on the same object(s) as seen in Figure 1(e). These repeated RoIs will produce similar features and contribute little to the later modeling. Meanwhile, we find some RoIs from fashion images are useless for text and image matching, for example, RoIs about the body parts (head, hair, hands etc.) of the models in fashion images as seen in Figure 1(f). These RoIs are irrelated to the fashion products and cannot build connection with the descriptions. On the contrary, most of the fashion texts describe the fine-grained information about the products (e.g., “crew neck”, “off-shoulder”, “high collar”). Occasionally, some of descriptions contain abstract styles, e.g., “artsy” and “bohemian” as seen in Figure 1(d). The RoIs in fashion images can indicate main fashion object(s), but fail to distinguish these fine-grained attributes or styles. Thus, it is more difficult for fashion text and\n2 https://github.com/peteanderson80/bottom-up-attention 3 http://cocodataset.org\nimage matching with such “object-level” RoIs and fine-grained descriptions.\nIn this paper, we propose FashionBERT to solve the above problems. Inspired by the selfie idea [38], we first introduce the patch method to extract image tokens. Each fashion image is split to small patches with the same pixels and we assume these patches as image tokens. The patches show more rawer pixel information, and thus contain more detained information compared with object-level RoIs. Besides, the split patches are non-repeated and ordered in nature, which are well suitable as the sequence inputs of the BERT model. The training procedure of FashionBERT is a standard multitask learning procedure (i.e., Masked Language Modeling, Masked Patch Modeling and Text&Image Alignment, which will be depicted in the later section). We propose an adaptive algorithm to balance the learning of each task. The adaptive algorithm treats the determination of loss weights of each task as a new optimal problem and will estimate the loss weights in each batch step.\nWe evaluate FashionBERT with two tasks, Text&Image alignment classification and cross-modal retrieval (including Image-to-Text and Text-to-Image retrieval). Experiments are conducted on the public fashion product dataset (Fashion-Gen). The results show that FashionBERT significantly outperforms the SOTA and other pioneer approaches. We also apply FashionBERT in our E-commercial website. The main contributions of this paper are summarized as follows: 1) We show the difficulties of text and image matching in the fashion domain and propose FashionBERT to address these issues. 2) We present the patch method to extract image tokens, and the adaptive algorithm to balance the multitask learning of\n4 https://fashion-gen.com/\nFashionBERT. The patch method and the adaptive algorithm are task-agnostic, which can be directly applied in others tasks. 3) We extensively experiment FashionBERT on the public dataset. Experiments show the powerful ability of FashionBERT in text and image matching of the fashion domain. 4) FashionBERT currently has been applied in practice. We present the concrete application of FashionBERT in cross-modal retrieval. Meanwhile, we analyze both matching performances and inference efficiencies in detail."
    }, {
      "heading" : "2 Methodology",
      "text" : "In this section, we will briefly revisit the BERT language model and then describe how we extract the image features and how FashionBERT jointly models the image and text data."
    }, {
      "heading" : "2.1 BERT",
      "text" : "The BERT model introduced by [8] is an attention-based bidirectional language model. Taking tokens (i.e., word pieces) as inputs, BERT processes the embeddings of tokens with a multi-\nlayer Transformer encoder [39]. When pre-trained on a large language corpus, BERT has proven to be very effective for transfer learning in variants of natural language processing tasks. The original BERT model focuses on encoding of the singlemodality text data. In the cross-modal scenario, the extended BERT model takes multi-modality data as input and allows them to interact within the Transformer blocks."
    }, {
      "heading" : "2.2 FashionBERT",
      "text" : "The overview of FashionBERT is illustrated in Figure 2. It is composed of four parts, text representation, image representation, matching backbone and FashionBERT training with adaptive loss. Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42]. The same BERT vocabulary is adopted in our FashionBERT model. We use the standard BERT pre-process method to process the input text. Finally, the sum of the word-piece embedding, position embedding and segmentation embedding is regarded as the text representation. The segmentation (i.e., “T” and “I” in Figure 2) is used to differentiate text and image inputs.\nImage Representation: Different from the RoI method, we cut each image into patches with the same pixels as illustrated in Figure 2. We regard each patch as an “image token”. For each patch, the outputs of the patch network are regarded as the patch features. It is possible to select any pre-trained image model, (e.g., InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network. These patches are ordered in nature. The spatial positions of the patches are used in the position embedding. The sum of the patch features, the position embedding and segmentation embedding are regarded as patch representations. Matching Backbone: The concatenation of the text token sequence and image patch sequence consists of the FashionBERT inputs. Similar to BERT, the special token [CLS] and separate token [SEP] are added in the first position and between the text token sequence and the image patch sequence, respectively.\nThe pre-trained standard BERT is adopted as the matching backbone network of FashionBERT. The information of text tokens and image patches thus interact freely in multiple selfattention layers. FashionBERT outputs the final representations of each token or patch. FashionBERT Training with Adaptive Loss: We exploit three tasks to train FashionBERT.\nMasked Language Modeling (MLM): This task is very similar to the MLM task utilized in BERT pre-training. We apply the Whole Word Masking (WWM) strategy to mask out all the text tokens corresponding to a word at once [6]. For example, in Figure 2, “long sleeve hoodie in black” is masked as “long sleeve [MSK] [MSK] in black”, rather than “long sleeve [MSK] #ie in [MSK]” occasionally. The input tokens are masked out with the probability of 15%. Given a text token sequence \uD835\uDC61 = {\uD835\uDC61 , \uD835\uDC61 , … , \uD835\uDC61 }, the masked-out sequence is denoted by \uD835\uDC61\\ = {\uD835\uDC61 , \uD835\uDC61 , … , [\uD835\uDC40\uD835\uDC46\uD835\uDC3E] , … , \uD835\uDC61 }, which denotes token i is masked out. The operator “\\” means removing. The last-layer hidden outputs of the masked-out tokens are fed into a classifier over the standard BERT vocabularies. Finally, the MLM task is to minimize the cross-entropy loss, written as\n~ \\( ) log ( , )MLM t D i il E P t t   (1) where \uD835\uDF03 is the FashionBERT parameters and \uD835\uDC37 is the whole training set. \uD835\uDC43(\uD835\uDC61 |\uD835\uDC61\\ , \uD835\uDF03) denotes the probability of the maskedout token \uD835\uDC61 predicted by FashionBERT, given surrounding tokens \uD835\uDC61\\ .\nMasked Patch Modeling (MPM): Similar to MLM, we mask out certain patches in a patch sequence in the MPM task. Given an image patch sequence \uD835\uDC5D = {\uD835\uDC5D , \uD835\uDC5D , … , \uD835\uDC5D }, we randomly mask out patches with the probability of 10%, denoting as \uD835\uDC5D\\ = {\uD835\uDC5D , \uD835\uDC5D , … , [\uD835\uDC40\uD835\uDC46\uD835\uDC3E] , … , \uD835\uDC5D }. The patch features of masked-out patches are set to zero. MPM is to predict the distribution over the masked-out patch features. The MPM training is supervised by minimizing the KL-divergence between the distributions of patch features.\n~ \\ ( ) ( .( , ) | .( )) p DMPM KL i i i l E Distr p p Distr p   (2) Text and Image Alignment (TIA): In the TIA task, the hidden output of the special token [CLS] is fed into a binary classifier to indicate whether the text and image data are matched. For one\npositive example in the train dataset, the text and image are extracted from one same fashion product, while for one negative sample, the text and image are randomly selected from different fashion products. TIA objects to optimize the binary crossentropy loss.\n, ~ ˆ( ) [ log ( | , , )\nˆ(1 ) log(1 ( | , , ))]\nTIA t p Dl E y P y t p\ny P y t p\n               (3)\nwhere y and \uD835\uDC66 denote the true and predicted labels, respectively. In sum, FashionBERT attempts to optimize the aggregated loss function as seen in Equation (4), which is well acknowledged as an multitask learning problem.\n1\n( ) ( ) L\ni i i L l     (4) \uD835\uDC3F is the task number and in FashionBERT and \uD835\uDC3F equals to three. ω are the loss weights to balance the learning of each task. We treat the determination of the loss weight \uD835\uDF14 as a new optimal problem:\n22\n1 , 1\n1\n1 1 arg min\n2 2\n. . 1 0\nL L\ni i i j i i j\nL\ni i i\nl\ns t and\n  \n \n \n\n   \n  \n \n (5)\nIn Equation (5), on one hand we aim to minimum the total weighted loss, and on the other hand we expect FashionBERT fairly treats the learning of all tasks. Considering the KKT conditions (Karush-Kuhn-Tucher Conditions) [3], we can obtain the solution to \uD835\uDF14 as\n2 1\n2 1\n1\n( )\n( )\ni i L\nii\nL l\nL l \n \n \n \n (6)\nThe proof to this solution is in the appendix. We illustrate the training procedure of FashionBERT in Algorithm.1."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we describe our experimental settings and show the main results.\nAlgorithm 1 FashionBERT Training with Adaptive Loss Input: the aggregated loss functions ℒ(\uD835\uDF03) = ∑ \uD835\uDC59 (\uD835\uDF03) and training dataset D Parameter: the model parameters \uD835\uDF03 Output: \uD835\uDF03 1: Let \uD835\uDF14 = 1 \uD835\uDC3F⁄ . 2: for each batch of train data do 3: Feed the train batch into FashionBERT to get all \uD835\uDC59 . 4: Obtain weight losses according to Equation (6) 5: Aggregate loss function according to Equation (4) 6: Update FashionBERT by optimizing ℒ(\uD835\uDF03) with ADAM \uD835\uDF03 = \uD835\uDF03 − \uD835\uDF02 \uD835\uDF15ℒ(\uD835\uDF03) \uD835\uDF15\uD835\uDF03⁄ 7: end for 8: return model parameters \uD835\uDF03"
    }, {
      "heading" : "3.1 Experimental Settings",
      "text" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions. In our experiments, we adopt the Fashion-Gen dataset, which contains 67,666 fashion products. The Fashion products are associated with text descriptions provided by professional stylists, which are high-quality data for our FashionBERT pre-training. Each product is photographed from 1 to 6 different angles. We collect 293,008 image (256*256 pixels) and description pairs, among which 260,480 pairs are used for training, and 32,528 for testing. FashionBERT is pre-trained on the training dataset. We directly evaluate it on the testing dataset. Evaluation Tasks and Metrics: We introduce two tasks (i.e., Text and Image Matching, and Cross-modal Retrieval) to test the FashionBERT performances. For Text and Image Matching evaluation, all the test data are adopted. Given a text and image pair, the output of the TIA classifier is regarded as the matching similarity. We use Accuracy to assess the FashionBERT performance on this matching task.\nFor Cross-modal Retrieval evaluation, 1,000 unique description queries and 1,000 unique image queries are randomly selected from the test data. Given one description (or image) query, the ground-truth image (or description) from the same product and randomly-sampled 100 images (or description) from other products consist of the candidate rank set. Then we acquire the evaluation datasets for text-to-image and image-totext retrieval. Same with [23], we score each description and image pair in the test dataset and then sort the candidate rank set. We use three metrics (i.e. Rank@K (K=1, 5, 10)) to evaluate FashionBERT on Cross-modal Retrieval. Rank@K is the percentage of ground-truth matchings appearing in the top-K ranked list.\nImplementation Details: We reuse the pre-trained parameters from the 12-layer BERT-base. Each block has 768 hidden units and 12 self-attention heads. The text representation follows the same processing of BERT. The maximum sequence length is set to 512, among which the patch sequence is set to 64 (8*8 patches) and the maximum text sequence length is set to 448 (=512-64, including the special tokens). For each patch, ResNeXt101 [43] is first adopted to extract the patch features and the dimensions of patch features are 2048.\nFashionBERT is implemented with Tensorflow5 and trained on 8*Tesla V100 GPUs with early stopping to avoid overfitting (it usually had run about 10 epochs when stopping). In each training batch, 64 shuffled <Text, Image> pairs are utilized. Adam optimizer is applied with the learning rating of 2e-5, \uD835\uDEFD = 0.95, \uD835\uDEFD = 0.999, weight decay of 1e-4, learning rate warmed up at the first 5,000 steps, and then linear decay."
    }, {
      "heading" : "3.2 Evaluation of the SOTA and",
      "text" : "Pionner Approaches\nIn this section, we conduct the experiments to response two questions:  Does our model perform well comparing with the baseline\napproaches? We implement the following baseline approaches. VSE [11]: VSE directly projects image features and text features into visual semantic embedding space in an end-toend manner, which is regarded as our first baseline approach. VSE++ [10]: VSE++ extends the VSE approach with modification of the rank-based loss function which pays more attention to the hard negatives. SCAN [18]: SCAN performs the cross-modal match on the image RoIs and text tokens. Meanwhile, the aention mechanism [39] is leveraged to enhance matching ability.\n5 https://www.tensorflow.org/\nPFAN [41]: The position information of each RoI is incorporated in the PFAN model. By this way, the matching can better understand the position information in both texts and images.  Does our model perform well comparing with the state-ofthe-art (SOTA) pre-trained approaches? We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34]. ViLBERT-ZeroShot [23]: In ViLBERT, the authors fine-tune and evaluate ViLBERT with cross-modal retrieval. They release the fine-tuned cross-modal retrieval model as well. Thus, in this experiment, we evaluate the Fashion-Gen testing data with the released ViLBERT model. We follow the same RoI extraction method in ViLBERT. For each image, we keep 10 to 36 high-scoring regions with Faster R-CNN [13] pre-trained on the Visual Genome dataset [27]. ViLBERT-Finetune: In this experiment, based on the pretrained ViLBERT, we fine-tune a new cross-modal retrieval model with the Fashion-Gen training data. VLBERT-Finetune [34]: The pre-trained VLBERT model is not evaluated with cross-modal retrieval. We thus fine-tune a new cross-modal retrieval model with the pre-trained VLBERT. In VLBERT, we follow its RoI extraction setting, where the minimum number of RoIs is set to 10, and at most 100 RoIs with detection scores higher than 0.5 are selected for each image. All these approaches have the same settings, e.g., hidden layers, embedding size, patch features. Each experiment runs three times, and the average performances are shown in Table 1. we observe that FashionBERT with patch and adaptive loss achieves the significant improvement on Rank@K metrices (statistically significant difference over the other baselines with \uD835\uDC5D<0.1). This shows the excellent ability of FashionBERT in fashion text and image matching. In the ViLBERT-ZeroShot experiment, though ViLBERT is pre-trained with the general domain dataset, it does not perform well in matching of the fashion domain, which hints that there may exist a large gap between the fashion domain and the general domain. Thus, the pre-training model with general domain dataset contributes little to the matching of the fashion domain. This also explains that after finetuning with the Fashion-Gen dataset, a soar increase is achieved in the ViLBERT-Finetune experiment. Furthermore, it is found that FashionBERT benefits more from the patch method than from the RoI method compared with the RoI-based ViLBERT/VLBERT experiments and our Patch-based FashionBERT. As mentioned in section 1, more useless and repeated RoIs are detected from fashion images. The selfsupervised learning of FashionBERT was expected to mask out some of these RoIs and to predict them with surrounding ones. However, surrounding tokens may either provide the irrelevant information (the useless RoIs, e.g., the head, hands of the models) or almost the same information (the repeated RoIs which generate the similar patch features as well). It is hard for the\n6 https://github.com/jiasenlu/vilbert_beta 7 https://github.com/jackroos/VL-BERT\nmodel to learn useful information from these noise regions. On the contrary, the patches provide non-repeated and reasonablyrelated information, which is more suitable for self-supervised learning and enhances the performance of the fashion text and image matching."
    }, {
      "heading" : "3.3 Ablation Studies",
      "text" : "In this section, we conduct ablation experiments in order to incrementally exam the influences of adaptive loss, pre-trained image models and model size of BERT. Effect of Adaptive Loss: We first investigate the contribution of the task-agnostic adaptive loss algorithm. We test FashionBERT with and without adaptive loss. When without adaptive loss, \uD835\uDF14 is set to 1 \uD835\uDC3F⁄ .\nThe evaluation results are showed in Figure 3, which illustrates the adaptive loss weight produces positive influence on the FashionBERT performance. As shown in Equation (6), \uD835\uDF14 is in direct ratio to \uD835\uDC59 . This means that when the task \uD835\uDC56 gets a larger loss, the adaptive loss weight \uD835\uDF14 will be larger than the others. In consequence, FashionBERT will pay more attention to the learning of the task \uD835\uDC56. We show the adaptive loss weights during the training batches in Figure 4. It is found that at the\nbeginning FashionBERT pays more attention to MLM and TIA since these two tasks are newly introduced in BERT. Later on, the weight of TIA and MPM decay. At the same time, we find that when FashionBERT well matches the fashion texts and images (as seen ACC_TIA in Figure 4), it shifts its attention on the MPM and MLM tasks. These two tasks are relatively harder than the TIA task. This may also hint that there is still room for further improvements if more difficult matching tasks can be introduced in, for example, the token-level and patch-level alignment. Effect of pre-trained image models: We compare different pre-trained image models when extracting patch features. In this section, we compare ResNeXt-101 with InceptionV3. For the sake of fair comparison, we use their pre-trained parameters in Inception V3 and ResNeXt-101, respectively. The dimensions of the patch features are set to 2048 and the same hyperparameters are used in FashionBERT.\nThe evaluation results are illustrated in Table 2, where FashionBERT with ResNeXt-101 shows clearly better results than that with Inception V3. Compared with Inception V3, ResNeXt101 contains a series of residual network blocks. This residual structure brings more raw-pixel information into the Transformer encoders, which helps the modeling. We will explore more pre-trained tasks and extract more representative information. Effect of Model Size: In order to test the influence of the model size, we vary the number of the transformer encoder layers. FashionBERT is experimented with 4-layer, 6-layer and 12-layer transformer encoders. The 4-layer FashionBERT means that we load the first four layers of the pre-trained BERT. The other hyperparameters are consistent among all the experiments.\nThe evaluation results are shown in Table 3, which demonstrates that FashionBERT benefits from the deeper BERT encoders over all the metrices except Rank@1 (Rank@1 tends to be more sensitive than the other two). Due to limited resources, we did not experiment with the pre-trained BERT-Large. It is possible to achieve further improvements with the 24-layer BERT model."
    }, {
      "heading" : "3.4 Industry Applications",
      "text" : "FashionBERT is a general text and image encoder in the fashion domain, which can be widely applied in varieties of text and image matching tasks. The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform\nretrieval and ranking with nearest-neighbor search [15, 45]. The main framework of cross-modal retrieval is shown in Figure 5.\nIn practice cross-modal retrieval, FashionBERT is pretrained and fine-tuned with private datasets from scratch. The pre-train <Text, Image> pairs are collected from fashion products in our Alibaba.com8 website, where the titles of products act as the text information. The fine-tune dataset of cross-modal retrieval is extracted from logs of the search engine. From search logs, the queries and their clicked products first compose of the click dataset. In consequence, <Query, Title, Image> triples are chosen as the fine-tune dataset, where “Title” and “Image” are from the same clicked products. Totally, ten million <Title, Image> pairs are collected for pre-training. Two million <Query, Title, Image> triples are collected for fine-tuning, of which ninety percent are used in training and the rest are used in testing. Like the downstream task of Visual Question Answer (VQA) in VLBERT [34], “Q”, “T” and “I” are utilized in the segmentation to distinguish the three input types of “Query”, “Title” and “Image” in fine-tuning. We show the fine-tune model of cross-modal retrieval in Figure 6.\nThe previous experiments already prove the advantages of FashionBERT in the public dataset. In this set of experiments, we focus on the comparison of cross-modal retrieval and singlemodal retrieval. To certain degree, the fine-tune task can be regarded as the click prediction in information retrieval. Thus, we employ the accuracy and AUC metrics to evaluate the performance.\n8 www.alibaba.com - the global wholesale online trading market\noriBERT: This approach is the online baseline approach. The image information is not incorporated either in pre-training or in fine-tuning. In pre-training, we adopt the original pre-trained BERT model [8]. In fine-tuning, <Query, Title> pairs from <Query, Title, Image> triples compose of the training and testing datasets. Thus, this approach can be regarded as the singlemodel retrieval approach.\nBERT+IMG: In this experiment, image information is not interacted with text information in BERT. Rather than feeding image features into BERT, we concatenate them with the BERT outputs of the final layer for click prediction in fine-tuning.\nFurthermore, the inference speed is pivotal for online deployment. The computational and memory complexity of selfattention in BERT is Ο(\uD835\uDC3E \uD835\uDC37) , highly related to the input sequence length \uD835\uDC3E (\uD835\uDC3E = \uD835\uDC5A + \uD835\uDC5B in our FashionBERT) and the hidden dimension \uD835\uDC37[39]. The performance can be improved with a shorter sequence length. To speed up the online inference, we attempt the Variable Sequence Length (VSL) strategy, which directly concatenates the text and patch sequences, rather than first pads either of them to the max length.\nThe evaluation results are shown in Table 4. First, both the BERT+IMG and FashionBERT approaches benefit a lot from image features compared with oriBERT. Meanwhile, image features further enhance the performances after fully interacting with text features in FashionBERT. Second, though better performances are obtained with six-layer BERT or FashionBERT, the inference latencies vastly excel the online requirements. To trade off inference speeds and matching performances, model reduction has to be introduced in and only the first two FashionBERT layer in our final model. Meanwhile, we observe that the VSL strategy is quite effective in accelerating the speed and has very little influence on performances.\nOnline inference is still one of the most challenging issues for BERT-like models in deployment. More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26]. We are now attempting to incorporate these approaches in FashionBERT to further accelerate the online performance."
    }, {
      "heading" : "4 Related Work",
      "text" : ""
    }, {
      "heading" : "4.1 Pre-training",
      "text" : "The pre-training technique recently has been widely adopted in Machine Learning, which allows the learning model to leverage information from other related tasks.\nThe pre-training technique becomes popular first in CV. Krizhevsky et al. propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7]. Later on, the researchers found that these CNN blocks in AlexNet pretrained on ImageNet or the other large-scale image corpus can be treated as general feature extractors and perform well in various of downstream tasks [9]. Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43]. These pre-trained models are widely adopted in the\nCV tasks, like object detection [12] and semantic segmentation [22]. The applications of the pre-training technique in NLP is behind in CV. The early studies on pre-training in NLP can be traced back to word embedding, such as word2vec, GloVe [30]. Transformer is proposed to leverage the self-attention mechanism to draw global dependencies between inputs and outputs [39]. After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks. Very recently, the selfsupervised learning draws increasing attention of researchers, which allow the model to pre-train with large-scale unlabeled data and learn a more general representation across tasks."
    }, {
      "heading" : "4.2 Text and Image Matching",
      "text" : "There is a long research history on text and image matching. These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].\nHardoon et al., overview the applications of Canonical Correlation Analysis (CCA), where CCA is introduced to project text and image features into a shared vector space by maximizing the cross relation [14]. Researchers then propose variants of CCA-based approaches [25, 32, 44]. Visual-semantic embedding (VSE) present by Frome et al., learns semantic relationships between labels and explicitly maps images into the semantic embedding space [11]. Faghri et al., extend VSE and propose VSE++ by introducing the hard-negative mining technique [10]. SCAN performs the cross-modal match on the image RoIs and text tokens. Meanwhile, the attention mechanism is leveraged to enhance matching ability [18]. The position information of each RoI is incorporated in the PFAN model [41]. By this way, the matching can better understand the position information in both texts and images. Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks. VideoBERT [35] lever-ages off-the-shelf networks for action recognition from video clips. The clusters of video clips are regarded as visual words. VideoBERT learns the high-level video representation by using BERT as the backbone network. ViLBERT [23] focuses on the representation learning of the general domain. It extracts RoIs from images and regards these\nRoIs as image tokens. Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.\nIn this paper, we mainly follow this research line. We argue that this RoI method used in these BERT-based approaches does not work well in the fashion domain since the detected RoIs from fashion images are not fine-grained enough for fashion text and image matching."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we focus on the text and image matching in crossmodal retrieval of the fashion domain. We propose FashionBERT to address the matching issues in the fashion domain. FashionBERT splits images into patches. The images patches and the text tokens are as the inputs of the BERT backbone. To trade off the learning of each task, we present the adaptive loss algorithm which automatically determines the loss weights. Two tasks are incorporated to evaluate FashionBERT and extensive experiments are conducted on the Fashion-Gen dataset. The main conclusions are 1) the patch method shows its advantages in matching fashion texts and images, compared with the objectlevel RoI method; 2) through the adaptive loss, FashionBERT shifts its attention on different tasks during the training procedure.\nCompared with the matching of the general domain, there is still room for further improvements in the fashion domain. In the future, 1) To better understand the semantic of the fashion images, we attempt to construct more fine-grained training task (for example, token-level and patch-level alignment) to force FashionBERT to learn more detail information. 2) We attempt to visualize the FashionBERT matching secrets. This would help to understand how FashionBERT work inside and make further improvement. 3) We are attempting the model reduction, knowledge distillation approaches to further speed up the online inference.\nAPPENDIX PROOF. The adaptive loss weights learning can be written as\n22\n1 . 1\n1\n1 1 arg min\n2 2\n. . 1 0\nL L\ni i i j i i j\nL\ni i i\nl\ns t and\n   \n \n  \n\n   \n  \n \n (1)\nWe first omit the non-negative constraint and apply the Lagrange multipliers and get the Lagrange:\n2\n1 . 1\n1\n1 1 ( , )\n2 2\n(1 )\nL L\ni i i j i i j\nL\ni i\nL a l   \n \n \n\n    \n \n \n (2)\nThe solution is obtained by\n( , ) 0L a   (3.1) ( , ) 0L a   (3.2)\nFrom Equation (3.1), we get\n2\n, 1 , 1\n2\n0\n1\nL L\ni i i j i j i j\ni i\nl\nL l\n   \n\n \n    \n  \n\n  (4)\nFrom Equation (3.2), we get\n1\n1 L\ni i    (5) By taking Equation (4) into Equation (5), the solution can be\nachieved by 2 1\n2 1\n1\n( )\n( )\ni i L\nii\nL l\nL l \n \n \n \n (6)\nwhen ∇\uD835\uDC59 ∈ [0,1) , \uD835\uDF14∗ is non-negative and satisfy the nonnegative constrain in Equation (1)."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "e authors would like to thank Alibaba PAI team for providing experimental environments and their constructive comments. is work was partially supported by the China Postdoctoral Science Foundation (No. 2019M652038). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."
    } ],
    "references" : [ {
      "title" : "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2018
    }, {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In Proceedings of International Conference on Computer Vision",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Describing clothing by semantic attributes",
      "author" : [ "Huizhong Chen", "Andrew Gallagher", "Bernd Girod" ],
      "venue" : "In Proceedings of European Conference on Computer Vision,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "AdaBERT: Task- Adaptive BERT Compression with Differentiable Neural Architecture Search",
      "author" : [ "Daoyuan Chen", "Yaliang Li", "Minghui Qiu", "Zhen Wang", "Bofang Li", "Bolin Ding", "Hongbo Deng", "Jun Huang", "Wei Lin", "Jingren Zhou" ],
      "venue" : "arXiv preprint. arXiv:2001.04246",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Pre-Training with Whole Word Masking for Chinese BERT",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu" ],
      "venue" : "arXiv. preprint arXiv:1906.08101",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2019
    }, {
      "title" : "ImageNet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : "arXiv. preprint arXiv:1810.04805",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2018
    }, {
      "title" : "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
      "author" : [ "Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
      "author" : [ "Fartash Faghri", "David J. Fleet", "Jamie Ryan Kiros", "Sanja Fidler" ],
      "venue" : "arXiv preprint arXiv:1707.05612. Industry (SIRIP) Papers I SIGIR",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    }, {
      "title" : "DeViSE: A Deep Visual- Semantic Embedding Model",
      "author" : [ "Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc’Aurelio Ranzato", "Tomas Mikolov" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Canonical Correlation Analysis: An overview with Application to Learning Methods",
      "author" : [ "David R. Hardoon", "Sandor Szedmak", "John Shawe-Taylor" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Product Quantization for Nearest Neighbor Search",
      "author" : [ "Herve Jégou", "Matthijs Douze", "Cordelia Schmid" ],
      "venue" : "In IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "TinyBERT: Distilling BERT for Natural Language Understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu" ],
      "venue" : "arXiv preprint arXiv:1909.10351",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2019
    }, {
      "title" : "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut" ],
      "venue" : "arXiv preprint arXiv:1909.11942",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1942
    }, {
      "title" : "Stacked Cross Attention for Image-Text Matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He" ],
      "venue" : "In Proceedings of European Conference on Computer Vision",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2018
    }, {
      "title" : "Unicoder-VL: A Universal Encoder for Vision and Language by Crossmodal Pre-training",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Ming Gong", "Daxin Jiang", "Ming Zhou" ],
      "venue" : "In Proceedings of Association for the Advancement of Artificial Intelligence",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2019
    }, {
      "title" : "Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set",
      "author" : [ "Si Liu", "Zheng Song", "Guangcan Liu", "Changsheng Xu", "Hanqing Lu", "Shuicheng Yan" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations",
      "author" : [ "Ziwei Liu", "Ping Luo", "Shi Qiu", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Fully Convolutional Networks for Semantic Segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2019
    }, {
      "title" : "Hipster wars: Discovering elements of fashion styles",
      "author" : [ "M Hadi Kiapour", "Kota Yamaguchi", "Alexander C Berg", "Tamara L Berg" ],
      "venue" : "In Proceedings of European Conference on Computer",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Discriminative Learning and Recognition of Image Set Classes Using Canonical Correlations",
      "author" : [ "Tae-Kyun Kim", "Josef Kitter", "Roberto Cipolla" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li" ],
      "venue" : "arXiv preprint arXiv:1602.07332",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Shuffle and learn: unsupervised learning using temporal order verification",
      "author" : [ "Ishan Misra", "C Lawrence Zitnick", "Martial Hebert" ],
      "venue" : "In Proceedings of European Conference on Computer Vision,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning" ],
      "venue" : "In Proceedings of conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2018
    }, {
      "title" : "Deep canonical correlation analysis with progressive and hypergraph learning for cross-modal retrieval",
      "author" : [ "Jie Shao", "Leiquan Wang", "Zhicheng Zhao Fei Su", "Anni Cai" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai" ],
      "venue" : "arXiv. preprint arXiv:1908.08530",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1908
    }, {
      "title" : "Videobert: A joint model for video and language representation learning",
      "author" : [ "Chen Sun", "Austin Myers", "Carl Vondrick", "Kevin Murphy", "Cordelia Schmid" ],
      "venue" : "arXiv preprint arXiv:1904.01766",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2019
    }, {
      "title" : "Rethinking the Inception Architecture for Computer Vision. arXiv preprint arXiv:1512.00567",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Efficient object category recognition using classemes",
      "author" : [ "Lorenzo Torresani", "Martin Szummer", "Andrew Fitzgibbon" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2010
    }, {
      "title" : "Selfie: Selfsupervised Pretraining for Image Embedding",
      "author" : [ "Trieu H. Trinh", "Minh-Thang Luong", "Quoc V. Le" ],
      "venue" : "arXiv preprint arXiv:1906.02940",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2019
    }, {
      "title" : "Composing Text and Image for Image Retrieval – An Empirical Odyssey",
      "author" : [ "Nam Vo", "Lu Jiang", "Chen Sun", "Kevin Murphy", "Li-Jia Li", "Li Fei-Fei", "James Hays" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2019
    }, {
      "title" : "Position Focused Attention Network for Image-Text Matching",
      "author" : [ "Yaxiong Wang", "Hao Yang", "Xueming Qian", "Lin Ma", "Jing Lu", "Biao Li", "Xin Fan" ],
      "venue" : "In Proceedings of International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2019
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi" ],
      "venue" : "arXiv preprint arXiv:1609.08144",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Aggregated Residual Transformations for Deep Neural Networks. arXiv preprint arXiv:1611.05431",
      "author" : [ "Saining Xie", "Ross Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2016
    }, {
      "title" : "Deep Correlation for Matching Images and Text",
      "author" : [ "Fei Yan", "Krystian Mikolajczyk" ],
      "venue" : "In Proceedings of IEEE Conference of Computer Vision and Pattern Recognition",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Efficient Indexing of Billion-Scale Datasets of Deep Descriptors",
      "author" : [ "Artem Babenko Yandex", "Victor Lempitsky" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "The pivot of cross-modal retrieval is to learn a meaningful cross-modal matching [40].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 234,
      "endOffset" : 246
    }, {
      "referenceID" : 23,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 234,
      "endOffset" : 246
    }, {
      "referenceID" : 40,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 234,
      "endOffset" : 246
    }, {
      "referenceID" : 9,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 275,
      "endOffset" : 291
    }, {
      "referenceID" : 10,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 275,
      "endOffset" : 291
    }, {
      "referenceID" : 16,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 275,
      "endOffset" : 291
    }, {
      "referenceID" : 37,
      "context" : "The early approaches usually project visual and textual modal representations into a shared embedded subspace for the cross-modal similarity computation or fuse them to learn the matching scores, for example, the CCA-based approaches [14, 25, 44] and the VSEbased approaches [10, 11, 18, 41].",
      "startOffset" : 275,
      "endOffset" : 291
    }, {
      "referenceID" : 0,
      "context" : "Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "Very recently, the pre-training technique has been successfully applied in Computer Vision (CV) [1, 2] and Nature Language Processing (NLP) [8, 46].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 31,
      "context" : "Several researchers are inspired to adopt the pre-trained BERT model as the backbone network to learn the cross-modal information representation [19, 34].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 36,
      "context" : "The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "The proposed approaches have achieved promising performances on several down-stream tasks, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 35,
      "context" : "Inspired by the selfie idea [38], we first introduce the patch method to extract image tokens.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "The BERT model introduced by [8] is an attention-based bidirectional language model.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : "Text Representation: Similar to [8], the input text is first tokenized into a token sequence according to WordPieces [42].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 33,
      "context" : ", InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 39,
      "context" : ", InceptionV3 [36] and ResNeXt-101 [43]) as the backbone of the patch network.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "We apply the Whole Word Masking (WWM) strategy to mask out all the text tokens corresponding to a word at once [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Considering the KKT conditions (Karush-Kuhn-Tucher Conditions) [3], we can obtain",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "Datasets: Although there exist several fashion datasets [4, 20, 21, 24, 37], the majority of these datasets only contain a limited number of images or lack necessary fashion descriptions.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Same with [23], we score each description and image pair in the test dataset and then sort the candidate rank set.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 39,
      "context" : "For each patch, ResNeXt101 [43] is first adopted to extract the patch features and the dimensions of patch features are 2048.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "VSE [11]: VSE directly projects image features and text features into visual semantic embedding space in an end-toend manner, which is regarded as our first baseline approach.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "VSE++ [10]: VSE++ extends the VSE approach with modification of the rank-based loss function which pays more attention to the hard negatives.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : "SCAN [18]: SCAN performs the cross-modal match on the image RoIs and text tokens.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 37,
      "context" : "PFAN [41]: The position information of each RoI is incorporated in the PFAN model.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "We compare our model with the pre-trained cross-modal models, ViLBERT6 [23] and VLBERT7 [34].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "ViLBERT-ZeroShot [23]: In ViLBERT, the authors fine-tune and evaluate ViLBERT with cross-modal retrieval.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "For each image, we keep 10 to 36 high-scoring regions with Faster R-CNN [13] pre-trained on the Visual Genome dataset [27].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "VLBERT-Finetune [34]: The pre-trained VLBERT model is not evaluated with cross-modal retrieval.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform retrieval and ranking with nearest-neighbor search [15, 45].",
      "startOffset" : 213,
      "endOffset" : 221
    }, {
      "referenceID" : 41,
      "context" : "The vanilla application is end-to-end cross-modal retrieval in practice, where we vectorize the search queries and the products with FashionBERT and then perform retrieval and ranking with nearest-neighbor search [15, 45].",
      "startOffset" : 213,
      "endOffset" : 221
    }, {
      "referenceID" : 31,
      "context" : "Like the downstream task of Visual Question Answer (VQA) in VLBERT [34], “Q”, “T” and “I” are utilized in the segmentation to distinguish the three input types of “Query”, “Title” and “Image” in fine-tuning.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "In pre-training, we adopt the original pre-trained BERT model [8].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "More recently, some tiny varieties of BERT are proposed to address this issue, such as tinyBERT [16], ALBERT [17], AdaBERT [5], Reformer [26].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "propose AlexNet in 2012 [28], with which they win the 2012 ILSVR image classification competition [7].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "Later on, the researchers found that these CNN blocks in AlexNet pretrained on ImageNet or the other large-scale image corpus can be treated as general feature extractors and perform well in various of downstream tasks [9].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 30,
      "context" : "Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : "Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 39,
      "context" : "Since then the researchers propose more effective CNN-based models and pre-train them on massive dataset, such as VGG [33], Google Inception [36], ResNet [43].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "These pre-trained models are widely adopted in the CV tasks, like object detection [12] and semantic segmentation [22].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "These pre-trained models are widely adopted in the CV tasks, like object detection [12] and semantic segmentation [22].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "The early studies on pre-training in NLP can be traced back to word embedding, such as word2vec, GloVe [30].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 28,
      "context" : "After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "After that, a series of extended studies are presented for example, GPT [31], BERT [8], XLNet [46], among which BERT is one of the most popular models for its performances in variants of NLP tasks.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : "These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "These researches have greatly promoted the developments of the cross-modal applications, such as cross-modal retrieval [40], image captioning [1] and visual question answering [2].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : ", overview the applications of Canonical Correlation Analysis (CCA), where CCA is introduced to project text and image features into a shared vector space by maximizing the cross relation [14].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "Researchers then propose variants of CCA-based approaches [25, 32, 44].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "Researchers then propose variants of CCA-based approaches [25, 32, 44].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 40,
      "context" : "Researchers then propose variants of CCA-based approaches [25, 32, 44].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : ", learns semantic relationships between labels and explicitly maps images into the semantic embedding space [11].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : ", extend VSE and propose VSE++ by introducing the hard-negative mining technique [10].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "Meanwhile, the attention mechanism is leveraged to enhance matching ability [18].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 37,
      "context" : "The position information of each RoI is incorporated in the PFAN model [41].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "Recently, with the success of pre-training and self-supervised learning [29, 43], researchers attempt to apply BERT in cross-modal tasks.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "VideoBERT [35] lever-ages off-the-shelf networks for action recognition from video clips.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "ViLBERT [23] focuses on the representation learning of the general domain.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 31,
      "context" : "Unicoder-VL [19] and VL-BERT [34] follow the same RoI method, but select a single cross-modal Transformer structure to allow text and image information interacting earlier.",
      "startOffset" : 29,
      "endOffset" : 33
    } ],
    "year" : 2020,
    "abstractText" : "In this paper, we address the text and image matching in crossmodal retrieval of the fashion industry. Different from the matching in the general domain, the fashion matching is required to pay much more aention to the fine-grained information in the fashion images and texts. Pioneer approaches detect the region of interests (i.e., RoIs) from images and use the RoI embeddings as image representations. In general, RoIs tend to represent the “object-level” information in the fashion images, while fashion texts are prone to describe more detailed information, e.g. styles, aributes. RoIs are thus not fine-grained enough for fashion text and image matching. To this end, we propose FashionBERT, which leverages patches as image features. With the pre-trained BERT model as the backbone network, FashionBERT learns high level representations of texts and images. Meanwhile, we propose an adaptive loss to trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text and image matching and cross-modal retrieval) are incorporated to evaluate FashionBERT. On the public dataset, experiments demonstrate FashionBERT achieves significant improvements in performances than the baseline and state-ofthe-art approaches. In practice, FashionBERT is applied in a concrete cross-modal retrieval application. We provide the detailed matching performance and inference efficiency analysis.",
    "creator" : null
  }
}